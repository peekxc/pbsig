---
title: Spectral relaxations of persistent rank invariants
subtitle: With a focus on _parameterized_ settings
author: Matt Piekenbrock$\mathrm{}^\dagger$   \&   Jose Perea$\mathrm{}^\ddagger$
format: 
  revealjs:
    default-timing: 60
    css: 
      - katex.min.css
      - styles.css
    html-math-method: 
      method: katex 
      url: "/"
    smaller: true
    # footer: "Spectral relaxations of persistent rank invariants"
    theme: simple 
    institute: 
      - $\dagger$ Khoury College of Computer Sciences, Northeastern University
      - $\ddagger$. Department of Mathematics and Khoury College of Computer Sciences, Northeastern University
    spotlight:
      useAsPointer: false
      size: 55
      toggleSpotlightOnMouseDown: false
      spotlightOnKeyPressAndHold: 16 # shift : 
      presentingCursor: "default"
    overview: true
    margin: 0.075
    title-slide-attributes:
      data-background-image: images/NE.jpeg
      data-background-size: contain
      data-background-opacity: "0.25"
    # csl: csl.csl
  pdf: default
revealjs-plugins:
  - spotlight
html: 
  html-math-method: katex
  standalone: true
filters:
  # - pandoc-katex.lua
  - roughnotation
# keycodes: https://gcctech.org/csc/javascript/javascript_keycodes.htm
bibliography: references.bib
---


## Vectorizing diagrams {visibility="hidden"}

There are many mappings from $\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) 

::: {.fragment .fade-in style="text-align: left"}

- Persistence Landscapes [@bubenik2020persistence]

:::


::: {.fragment .fade-in-then-out style="text-align: center"}

![](images/pers_landscape_def.png){width=40% fig-align="center"}

$$ \lambda(k, t) = \sup \{ h \geq 0 \mid \mathrm{rank}(H_p^{i-h} \to H_p^{i+h}) \geq k \} $$

:::


## Vectorizing diagrams {visibility="hidden"}

There are many mappings from $\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) 

- Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay]

![](images/pers_landscape_app.png){width=40% fig-align="center"}

$$ \lambda(k, t) = \sup \{ h \geq 0 \mid \mathrm{rank}(H_p^{i-h} \to H_p^{i+h}) \geq k \} $$


## Vectorizing diagrams {visibility="hidden"}

There are many mappings from $\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) 

<ul> 

::: { style="color: rgb(127,127,127);"}

<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>

:::

::: {.fragment .fade-in style="text-align: left"}

<li> Persistence Images [@adams2017persistence] </li> 

:::

<ul> 


::: {.fragment .fade-in-then-out style="text-align: center"}

![](images/pers_image_def.png){height=50% fig-align="center"}

$$ \rho(f, \phi) = \sum\limits_{(i,j) \in \mathrm{dgm}} f(i,j) \phi(\lvert j - i \rvert)$$

:::

## Vectorizing diagrams {visibility="hidden"}

There are many mappings from $\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) 

<ul> 

::: { style="color: rgb(127,127,127);"}

<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>

:::

<li> Persistence Images [@adams2017persistence] + Learning applications [@som2020pi]
</li> 

<ul> 

![](images/pers_image_app.png){height=50% fig-align="center"}

$$ \rho(f, \phi) = \sum\limits_{(i,j) \in \mathrm{dgm}} f(i,j) \phi(\lvert j - i \rvert)$$



## Vectorizing diagrams{visibility="hidden"}

There are many mappings from $\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) 

<ul> 

::: { style="color: rgb(127,127,127);"}

<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>

:::

::: { style="color: rgb(127,127,127);"}

<li> Persistence Images [@adams2017persistence] + Learning applications [@som2020pi] </li>

:::

::: {.fragment .fade-in}

<li> A few others...$^1$ </li> 

![](images/vec1.png){width=80% height=100% fig-align="center"}

:::

</ul>

:::{.aside}

See [@bubenik2020persistence] for an overview. 

:::

## Many goals in common...{visibility="hidden"}

<hr/> 

:::: {.columns}

::: {.column width=40% layout-align="left" style="margin-left: 2em; margin-top: 1em;"}

::: {.fragment fragment-index=1}

- Vectorize persistence information

:::

::: {.fragment fragment-index=2}

- Optimize persistence invariants 

:::

::: {.fragment fragment-index=3}

- Leverage the stability of persistence

:::


::: {.fragment fragment-index=4}

- Connect to other areas of mathematics

:::
 
:::

::: {.column width=40% layout-align="left"}

:::{.r-stack}

![](images/pers_image.png){.fragment fragment-index=1 width="300" height="300"}

![](images/pers_landscape_app.png){.fragment fragment-index=2 width="300" height="300"}

![](animations/stability.gif){.fragment fragment-index=3 width="400" height="300"}

![](images/lsst.png){.fragment fragment-index=4 width="375" height="375"}

:::

:::

:::: 

::: {.fragment style="text-align: center" style="font-size: 40px;"}

<div style="text-align: center; font-size: 35px;" >

__Can we achieve these goals without computing diagrams?__

</div>


:::

<!-- ::: {.fragment style="text-align: center"}

$^\ast$ _avoid the reduction the algorithm_

:::

::: {.fragment style="text-align: center"}

$$\mathrm{dgm}(K) \leftrightarrow R = \partial V $$

::: -->

<!-- :::{.aside}

Image from: https://epfl-lts2.github.io/gspbox-html/doc/graphs/

::: -->

<br> 

## This Talk

:::{.fragment .fade-in style="text-align: center"}

We introduce <span style="color:orange"> _spectral-relaxation_ </span> of the rank invariants $\beta_p^{\ast}$ and $\mu_p^\ast$ that:

<div style="margin-left:2.5em;">

1. Smoothly interpolates$^1$ the _persistent rank_ function
2. Admits $(1{\textstyle -}\epsilon)$ approximation for any $\epsilon > 0$ in essentially $O(n^2)$ time 
3. Is computable in a "matrix-free" fashion in $O(n)$ memory 
4. Has variety of applications, e.g. featurization, optimization, metric learning

</div>

:::

:::{.fragment .fade-in style="text-align: left" layout="[[48,52]]" layout-valign="top"}

![](animations/ph_transform.gif){width=18% height=100% fig-align="left"}

![](animations/trace_summary.gif){width=18% height=100% fig-align="left"}

:::

:::{.aside}
\(1\) The Schatten-1 norms of the operators driving the relaxation are differentiable over the positive semi-definite cone 
:::

## The rank invariants {style="text-align: center;"}

::::{.columns}

:::{.column}

![](images/dgm_pbn.png){width=400 height=400 fig-align="center"}

<div style="text-align: center;">

$$ \beta_p^{i,j}(K)$$

</div>

:::

:::{.column}

![](images/dgm_mu.png){width=400 height=400 fig-align="center"}

<div style="text-align: center;">

$\mu_p^R(K)$

</div>

:::

::::

## Application: optimizing filtrations

![](images/codensity_dgm_ex.png){width=68% height=100% fig-align="center"}

:::{.fragment}

$$ \alpha^\ast = \argmax_{\alpha \in \mathbb{R}} \; \mathrm{card}\big(\, \left.\mathrm{dgm}(K_\bullet, \, f_\alpha) \right|_{R} \, \big) $$

:::

## Why the rank invariant {visibility="hidden"}


:::{.fragment}

There is a duality between diagrams its associated rank function:

$$ \mathrm{dgm}_p(\, K_\bullet, \, f \, ) \triangleq \{ \, ( \, i, j \,) \in \Delta_+ :  \mu_p^{i,j} \neq 0 \, \} \; \cup \; \Delta $$

$$\text{where: } \quad \mu_p^{i,j} = \left(\beta_p^{i,j{\small -}1} - \beta_p^{i,j} \right) - \left(\beta_p^{i{\small -}1,j{\small -}1} - \beta_p^{i{\small -}1,j} \right) \quad $$

:::

:::{.fragment}

_Fundamental Lemma of Persistent Homology_ shows diagrams characterize their ranks
$$\beta_p^{k,l} = \sum\limits_{i \leq k} \sum\limits_{j > l} \mu_p^{i,j}$$

:::

:::{.incremental}

- _Persistence measures_ [@chazal2016structure] extend (1,2) naturally when $\mathbb{F} = \mathbb{R}$ 
- Stability in context of multidimensional persistence [@cerri2013betti] 
- Generalizations of rank invariant via Möbius inversion [@mccleary2022edit] and via zigzag persistence[@dey2021computing]

:::


## Why not just use diagrams?

<!-- Extending the reduction algorithm to parameterized settings is highly non-trivial -->

:::: {.columns}

:::{.fragment}

::: {.column width="45%" layout-align="right" style="margin-left: 1em;"}
![](animations/dgms_vineyards.gif){width=90%}
:::

::: {.column width="45%" height="1em" layout-align="left"}
![](animations/complex_plain.gif){width=90%}
:::

:::

::::

:::{style="text-align: center;"}

__Pro:__ Diagrams are stable, well-studied, and information rich.

:::


::: {.notes}
Reduction algorithm is a restricted form of gaussian elimination. 
:::


## Why not just use diagrams?

:::: {.columns}

::: {.column width="33%"}
![](animations/dgms_vineyards.gif){width=90%}
:::

::: {.column width="33%" height="1em"}
![](animations/complex.gif){width=90%}
:::

::: {.column width="33%"}
![](animations/spy_matrices.gif){width=90%}
:::

::::

:::{style="text-align: center;"}

__Con:__ Extending the $R = \partial V$ to parameterized settings is non-trivial

:::

<div style="text-align: center;">

Maintaining the $R = \partial V$ decomposition "across time" $\implies$ huge memory bottleneck

</div>

:::{.aside}
For details, see @piekenbrock2021move or @bauer2022keeping (also @lesnick2015interactive)
:::

::: {.notes}
Reduction algorithm is a restricted form of gaussian elimination. 
:::

## Beyond $\mathrm{dgm}$'s: Revisiting the rank computation
 
$$ \beta_p^{i,j} : \mathrm{rank}(H_p(K_i) \to H_p(K_j))$$
	
<!-- <hr style="margin: 0; padding: 0;">  -->

:::{.incremental style="list-style-type: none;align=center;"}

::: {.fragment .fade-in-then-semi-out style="text-align: left"}
$\;\quad\quad\quad\beta_p^{i,j} = \mathrm{dim} \big( \;\mathrm{Ker}(\partial_p(K_i))\; / \;\mathrm{Im}(\partial_{p+1}(K_j)) \; \big )$
:::

<!-- <li style="text-align: left" class="fragment fade-in-then-semi-out">
$\quad\quad\quad\quad\beta_p^{i,j} = \mathrm{dim} \big( \;\mathrm{Ker}(\partial_p(K_i))\; / \;\mathrm{Im}(\partial_{p+1}(K_j)) \; \big )$
</li> -->
::: {.fragment .fade-in-then-semi-out style="text-align: left"}
<!-- <li style="text-align: left" class="fragment fade-in-then-semi-out"> -->
$\;\quad\quad\quad\hphantom{\beta_p^{i,j} }= \mathrm{dim}\big(\; \mathrm{Ker}(\partial_p(K_i)) \; / \; (\mathrm{Ker}(\partial_p(K_i)) \cap \mathrm{Im}(\partial_{p+1}(K_j))) \; \big )$
:::

::: {.fragment .fade-in-then-semi-out style="text-align: left"}
$\;\quad\quad\quad\hphantom{\beta_p^{i,j}}=\color{blue}{\mathrm{dim}\big(\;\mathrm{Ker}(\partial_p(K_i)) \; \big)} \; \color{black}{-} \; \color{red}{\mathrm{dim}\big( \; \mathrm{Ker}(\partial_p(K_i)) \cap \mathrm{Im}(\partial_{p+1}(K_j))\;\; \big)}$
:::

::: 

<!-- <br>  -->
::: {.fragment .fade-in-then-semi-out}
Rank-nullity yields the <span style="color: blue">left term</span>: 
$$
\mathrm{dim}\big(\mathrm{Ker}(\partial_p(K_i))\big) = \lvert C_p(K_i) \rvert - \mathrm{dim}(\mathrm{Im}(\partial_p(K_i)))
$$
:::

:::{.fragment .fade-in-then-semi-out}
Computing the <span style="color: red">right term</span> more nuanced: 
:::

:::{.incremental style="list-style-type: none; align=center; text-align: left; margin-left: 2.5em; margin: 0; padding: 0;"}
- Pseudo-inverse$^1$, projectors$^2$, Neumann's inequality$^3$, etc.
- PID algorithm$^4$, Reduction algorithm$^5$, Persistent Laplacian$^6$
:::

:::{.aside}
@anderson1969series, @ben1967geometry, @ben2015projectors, @zomorodian2004computing, @edelsbrunner2000topological, @memoli2022persistent
:::

## Key technical observation

:::{.fragment style="text-align: center;"}

Structure theorem for persistence modules can be used to show: 

$$ 
(i,j) \in \mathrm{dgm}(K_\bullet)
$$
 
:::

:::{.fragment style="margin: 0; padding: 0;"}

$$
\Leftrightarrow \mathrm{rank}(R^{i,j}) - \mathrm{rank}(R^{i\texttt{+}1,j}) + \mathrm{rank}(R^{i\texttt{+}1,j\text{-}1}) - \mathrm{rank}(R^{i,j\text{-}1}) \neq 0
$$

![](images/rank_ll.png){width=575 height=100% fig-align="center" style="margin: 0; padding: 0;"}

:::

:::{.fragment}

$$
\Rightarrow \mathrm{rank}(R^{i,j}) = \mathrm{rank}(\partial^{i,j}) 
$$

:::

## Key technical observation


:::{.fragment}

$$
\mathrm{rank}(R^{i,j}) = \mathrm{rank}(\partial^{i,j}) 
$$

:::

:::{.fragment }

![](images/rv_ll.png){width=950 height=100% fig-align="center" style="margin: 0; padding: 0;"}

:::

:::{.fragment style="text-align: center"}

__Take-a-way:__ Can deduce the $\mathrm{dgm}$ from ranks of "lower-left" submatrices of $\partial_p(K_\bullet)$

:::

## Key technical observation

$$ 
\begin{equation}
\mathrm{rank}(R^{i,j}) = \mathrm{rank}(\partial^{i,j})  
\end{equation}
$$
 
<hr>

:::{.fragment}

$(1)$ often used to show correctness of reduction, but far more general, as it implies:

:::

:::{.fragment}

<div style="padding-left: 1em; border: 1px solid black; margin: 2em; ">
__Corollary [@bauer2022keeping]__: Any algorithm that preserves the ranks of the submatrices $\partial^{i,j}$ for all $i,j \in \{ 1, \dots, n \}$ is a valid barcode algorithm.
</div>

:::

:::{.fragment}
$$ 
\begin{equation}
(1) \Rightarrow \beta_p^{i,j} = \lvert C_p(K_i) \rvert - \mathrm{rank}(\partial_p^{1,i}) - \mathrm{rank}(\partial_{p+1 }^{1,j}) + \mathrm{rank}(\partial_{p+1}^{i + 1, j} ) 
\end{equation}
$$

:::


:::{.fragment}

$$ 
\begin{equation}
(2) \Rightarrow \mu_p^{R} = \mathrm{rank}(\partial_{p+1}^{j + 1, k})  - \mathrm{rank}(\partial_{p+1}^{i + 1, k})  - \mathrm{rank}(\partial_{p+1}^{j + 1, l}) + \mathrm{rank}(\partial_{p+1}^{i + 1, l})  
\end{equation}
$$

:::

:::{.aside}

@edelsbrunner2000topological noted (1) in passing showing correctness of reduction; @dey2022computational explicitly prove (2); (3) was used by @chen2011output. (2) & (3) are connected to relative homology.

:::

## Overview

<ul>
  <li>Introduction</li>
  <ul>
    <li>Rank duality</li>
    <li>Bypassing diagrams</li>
    <li>Technical observations</li>
  </ul>
  <li><p>Spectral rank relaxation</p></li>
  <ul>
    <li>Parameterizing $C(K, \mathbb{R})$</li>
    <li>Spectral functions</li>
    <li>Variational interpretations + examples</li>
  </ul>
  <li><p>Computation</p>
  <ul>
    <li>Lanczos Iteration</li>
    <li>Stochastic trace estimation</li>
    <li>Scalability</li>
  </ul>
</li>
</ul>


## The Implications


:::{style="text-size: 14px; text-align: center; margin-top: 1em;"}

From now on, we work strictly with field coefficients in $\mathbb{R}$

:::

<br/> 

$$
	{\color{green} \mu_p^{R}} = 
	{\color{red} \mathrm{rank}}\begin{bmatrix} {\color{blue} \partial_{p+1}^{j + 1, k}} & 0 \\
	0 & {\color{blue} \partial_{p+1}^{i + 1, l} }
	\end{bmatrix}
	- 
	{\color{red} \mathrm{rank}}\begin{bmatrix} {\color{blue} \partial_{p+1}^{i + 1, k}} & 0 \\
	0 & {\color{blue} \partial_{p+1}^{j + 1, l} }
	\end{bmatrix}
$$

<br/>

:::{style="text-size: 14px; text-align: center;"}

:::{.fragment}

There are advantages to preferring _this_ expression for $\mu_p^R$

:::

<ol>
:::{.fragment}
  <li> <span style="color: blue;"> Inner terms </span> are _unfactored_ </li>
:::
:::{.fragment}
  <li> Variational perspectives on <span style="color: red;">rank function </span> well-studied ($\mathbb{R}$)</li>
:::
:::{.fragment}
  <li> Theory of <span style="color: green;">_persistent measures_$^{\ast}$</span> readily applicable </li>
:::
</ol>

:::

::: aside 

Chazal, Frédéric, Vin De Silva, Marc Glisse, and Steve Oudot. 2016. The Structure and Stability of Persistence Modules.

:::

## Parameterized filtrations

Suppose we have an $\alpha$-parameterized filtration $(K, f_\alpha)$ where $f_\alpha : K \to \mathbb{R}_+$ satisfies:

$$
f_\alpha(\tau) \leq f_\alpha(\sigma) \quad \text{ if } \tau \subseteq \sigma \quad \forall \tau,\sigma \in K \text{ and } \alpha \in \mathbb{R}
$$

:::{layout-ncol=2}

![](animations/codensity_family.gif){width=48% height=100% fig-align="right"}

![](animations/complex_plain.gif){width=48% height=100% fig-align="left"}

:::

## __Relax \#1__: Parameterized _boundary matrices_

:::{.fragment style="text-align: center;"}

Parameterize $C_p(K; \mathbb{R})$ with $\mathcal{S} \circ f_\alpha : K \to \mathbb{R}_+$ where $\mathcal{S}: \mathbb{R} \to [0,1]$  


:::

:::{.fragment style="text-align: center;"}

![](images/smoothstep_3.jpeg){width=88% height=100% fig-align="center"}

:::


:::{.fragment style="text-align: center; border: 1;"}

$$ 
\boxed{
\partial_p^{i,j}(\alpha) = D_p(\mathcal{S}_i \circ f_\alpha) \circ \partial_p(K) \circ D_{p+1}(\mathcal{S}_j \circ f_\alpha) 
}
$$ 

:::

:::{.fragment style="text-align: center;"}

__Note__: $P^T \partial_p^{i,j}(\alpha) P$ has rank $= \mathrm{rank}(R_p^{i,j}(\alpha))$ for all $\alpha \in \mathbb{R}$. 

:::

:::{.aside style="text-align: center;"}
Replacing $S \mapsto \mathcal{S}$ ensures continuity of $\partial_p^{i,j}(\alpha)$
:::


## Rank Invariances when $\mathbb{F} = \mathbb{R}$ {visibility="hidden"}

::: {.fragment .fade-in style="text-align: left"}

&emsp;&emsp;&emsp;&emsp;

$\hspace{10em} \mathrm{rank}(A) \triangleq \mathrm{dim}(\mathrm{Im}(A))$

::: 

::: {.fragment .fade-in style="text-align: left"}

$\hspace{10em}  \hphantom{\mathrm{rank}(A)} \equiv \mathrm{rank}(A^T) \quad \quad  \quad \text{(adjoint)}$

:::

::: {.fragment .fade-in style="text-align: left"}

$\hspace{10em}  \hphantom{\mathrm{rank}(A)} \equiv \mathrm{rank}(A^T A) \quad \quad \; \text{(inner product)}$

:::

::: {.fragment .fade-in style="text-align: left"}

$\hspace{10em}  \hphantom{\mathrm{rank}(A)} \equiv \mathrm{rank}(A A^T) \quad \quad \; \text{(outer product)}$

:::

::: {.fragment .fade-in style="text-align: left"}

$\hspace{10em}  \hphantom{\mathrm{rank}(A)} \equiv \mathrm{rank}(S^{-1}AS) \quad \;  \text{(change of basis)}$

:::

::: {.fragment .fade-in style="text-align: left"}

$\hspace{10em}  \hphantom{\mathrm{rank}(A)} \equiv \mathrm{rank}(P^T A P) \quad \; \text{(permutation)}$

:::

::: {.fragment .fade-in style="text-align: left"}

$\hspace{10em}  \hphantom{\mathrm{rank}(A)} \equiv \dots  \quad \quad \quad \quad  \quad \quad  \! \! \text{(many others)}$

:::

<br> 

::: {.fragment .fade-in style="text-align: left"}

<div style="text-align: center; font-size: 35px;" >

__Q: Can we exploit some of these to speed up the computation?__

</div>

:::

## Spectral functions {visibility="hidden"}

:::{style="text-align: center"} 

__Relaxation \#2__: Approximate $\mathrm{rank}$ with _spectral functions_ [@bhatia2013matrix]

:::


:::{style="list-style-type: none; align=center;"}

<div class="columns">

<div class="column">

::: {.fragment .fade-in-then-semi-out fragment-index=1 style="text-align: left"}

$\quad\quad\quad\quad \mathrm{rank}(X) = \sum \, \mathrm{sgn}_+(\sigma_i)$

:::

::: {.fragment .fade-in-then-semi-out fragment-index=2 style="text-align: left"}

$\quad\quad\quad\quad \hphantom{\mathrm{rank}(X)}\approx \sum\limits_{i=1}^n \, \phi(\sigma_i, \tau) \phantom{\int\limits_{-\infty}^x}$

:::

::: {.fragment .fade-in-then-semi-out fragment-index=3 style="text-align: left"}

$\quad\quad\quad\quad \hphantom{\mathrm{rank}(X)}=\lVert \Phi_\tau(X) \rVert_\ast$

:::

</div>

<div class="column">

::: {.fragment .fade-in-then-semi-out fragment-index=1 style="text-align: right"}

where $\quad\quad$ $X = U \mathrm{Diag}(\mathbf{\sigma})V^T$

:::

::: {.fragment .fade-in-then-semi-out fragment-index=2 style="text-align: right"}

where $\quad \phi(x, \tau) \triangleq \int\limits_{-\infty}^x\hat{\delta}(z, \tau) dz$

:::

::: {.fragment .fade-in-then-semi-out fragment-index=3 style="text-align: right"}

where $\quad \Phi_\tau(X) \triangleq \sum_{i=1}^n \phi(\sigma_i, \tau) u_i v_i^T$

:::

</div>

</div>

::: {.fragment .fade-in-then-semi-outstyle="text-align: center"}

$\Phi_\tau(X)$ is a _Löwner operator_ when $\phi$ is _operator monotone_ [@jiang2018unified]

$$ A \succeq B \implies \Phi_\tau(A) \succeq \Phi_\tau(B) $$

:::

:::{.fragment style="text-align: center"}

Closed-form proximal operators exist when $\Phi_\tau$ convex [@beck2017first]

Often used in nonexpansive mappings [@bauschke2011convex]

:::

:::

## Spectral functions

:::{.fragment .fade-in-then-semi-out fragment-index=1 style="text-align: center"} 

__Relaxation \#2__: Approximate $\mathrm{rank}$ with _spectral functions_ [@bhatia2013matrix]

:::


:::{style="list-style-type: none; align=center;"}

::: {.fragment .fade-in-then-semi-out fragment-index=2 style="text-align: center"}

$$\mathrm{rank}(X) = \sum \, \mathrm{sgn}_+(\sigma_i) \approx \sum \, \phi(\sigma_i, \tau), \quad \quad \phi(x, \tau) \triangleq \int\limits_{-\infty}^x\hat{\delta}(z, \tau) dz$$

:::

::: {.fragment .fade-in-then-semi-out fragment-index=2 style="text-align: left"}

$$ \text{ where } \quad \hat{\delta}(x, \tau) = \frac{1}{\nu(\tau)} p\left(\frac{x}{\nu(\tau)}\right), \quad \tau > 0, \quad \nu \text{ inc. }$$

:::

::: {.fragment .fade-in-then-semi-out fragment-index=4 style="text-align: center"}

$\Phi_\tau(X)$ is a _Löwner operator_ when $\phi$ is _operator monotone_ [@jiang2018unified]

$$ A \succeq B \implies \phi_\tau(A) \succeq \phi_\tau(B) $$

:::

:::{.fragment style="text-align: center"}

Closed-form proximal operators exist when $\phi_\tau$ convex + minor conditions$^1$ 

:::

:::

::: aside 

\(1\) See @beck2017first and @bauschke2011convex for existence and optimality conditions. 

:::


## Löwner Operators

For any smoothed Dirac measure^[Any $\hat{\delta}$ of the form $\nu(1/\tau) p (z \cdot \nu (1/\tau))$ where $p$ is a density function and $\nu$ positive and increasing is sufficient.] $\hat{\delta}$ and differentiable _operator monotone_ function $\phi: \mathbb{R}_+ \times \mathbb{R}_{++} \to \mathbb{R}_+$, [@bi2013approximation] show that:

<!-- <div class="columns" style="margin-left: 2.5em; "> -->

<div style="list-style-type: none !important;">

<div class="columns">

<div class="column" style="width: 30%">

:::{.fragment .fade-in fragment-index=1 .no_bullet}

($\tau$-approximate) $\vphantom{\hat{\delta}}$

:::

:::{.fragment .fade-in fragment-index=2}

(Monotone) $\vphantom{\lVert \phi_{\tau}(X) \rVert_\ast}$

:::

:::{.fragment .fade-in fragment-index=3}

(Smooth) $\vphantom{\mathbb{R}_1^{n \times m^{\ast^{\ast}}}}$

:::

:::{.fragment .fade-in fragment-index=4}

(Explicit) $\vphantom{\partial \lVert \Phi_\tau(\cdot) \rVert_\ast}$ 

:::

</div>

<div class="column" width="70%" layout-align="right">

:::{.fragment .fade-in fragment-index=1}

$0 \leq \mathrm{rank}(X) - \lVert \Phi_\tau(X) \rVert_\ast \leq c(\hat{\delta})$

:::

:::{.fragment .fade-in fragment-index=2}

$\lVert \Phi_{\tau}(X) \rVert_\ast \geq \lVert \Phi_{\tau'}(X) \rVert_\ast$ for any $\tau \leq \tau'$

:::

:::{.fragment .fade-in fragment-index=3}

Semismooth^[Here _semismooth_ refers to the existence of directional derivatives] on $\mathbb{R}^{n \times m}$ $\vphantom{\mathbb{R}_1^{n \times m^{\ast^{\ast}}}}$, differentiable on $\mathbf{S}_+^m$

:::

:::{.fragment .fade-in fragment-index=4}

Differential $\partial \lVert \Phi_\tau(\cdot) \rVert_\ast$ has closed-form soln.

:::

</div>

</div>

:::{.fragment .fade-in fragment-index=5}

Function/operator pairs ( $\Phi_\tau$, $\Phi_\tau$ ) particular specializations of _matrix functions_:

$$\Phi_\tau(X) = U \Phi_\tau(\Sigma) V^T$$

Commonly used in many application areas, e.g. compressed sensing [@li2014new]

:::

</div>


## Interpretation: Regularization

::: {.fragment fragment-index=1}

Ill-posed linear systems $Ax = b$ are often solved by "regularized" least-squares: 

$$
x_\tau^\ast = \argmin\limits_{x \in \mathbb{R}^n} \lVert Ax - b\rVert^2 + \tau \lVert x \rVert_1 
$$

:::

::: {.fragment fragment-index=2}

The minimizer is given in closed-form by the regularized pseudo-inverse:

$$
x_\tau^\ast = (A^T A + \tau I)^{-1} A^T b
$$
:::

::: {.fragment fragment-index=3}

![](images/lasso.png){width=50% fig-align="center"}

:::

::: aside

Image from: https://thaddeus-segura.com/lasso-ridge/

::: 

## Interpretation: Regularization

<br/>

::: {.fragment fragment-index=1}


<div style="text-align: center;">
Under the appropriate parameters$^1$ for $\nu$ and $p$, $\phi$ takes the form:

</div>

$$
\phi(x, \tau) = \frac{2}{\tau}\int\limits_{0}^z z \cdot  \big((z/\sqrt{\tau})^2+1\big)^{-2} dz = \frac{x^2}{x^2 + \tau}
$$

:::


::: {.fragment fragment-index=2}

<div style="text-align: center;">

The corresponding Löwner operator and its Schatten $1$-norm is given$^2$ by:

</div>

$$
\Phi_\tau(X) = (X^T X + \tau \, I_n)^{-1} X^T X, \quad \quad \lVert \Phi_\tau(X) \rVert_\ast = \sum\limits_{i = 1}^n \frac{\sigma_i(X)^2}{\sigma_i(X)^2 + \tau}
$$

:::

::: {.fragment fragment-index=3}

<div style="text-align: center;">

This the <span style="color: purple;"> _Tikhonov regularization_ </span> in standard form used in $\ell_1$-regression (LASSO)

</div>

:::

::: aside 

\(1\) This $\phi$ corresponds to setting $\nu(\tau) = \sqrt{\tau}$ and $p(x) = 2x (x^2 + 1)^{-2}$; \(2\) See Theorem 2 in @zhao2012approximation. 

::: 



## Application \#1: Filtration optimization

:::{layout="[[25,50,25]]" layout-valign="bottom"}

<div class="column" layout-align="right">
![](animations/dgms_vineyards.gif){width=60%}
</div>

<div class="column" layout-align="center">
![](images/dgm_opt.png){height=100% fig-align="center"}
</div>

<div class="column" layout-align="left">
![](animations/complex_plain.gif){width=60%}
</div>

::::

$$ 
\alpha^\ast = \argmax_{\alpha \in \mathbb{R}} \; \mathrm{card}\big(\, \left.\mathrm{dgm}(K_\bullet, \, f_\alpha) \right|_{R} \, \big) 
$$

## Application \#1: Filtration optimization

:::{layout="[[60,30]]" layout-valign="bottom"}

<div class="column" layout-align="right">
![](images/codensity_mult.png)
</div>

<div class="column" layout-align="center">
![](images/optimal_codensity_complex.png){width=35% height=100%}
</div>

:::

$$ 
\alpha^\ast = \argmax_{\alpha \in \mathbb{R}} \; \mathrm{card}\big(\, \left.\mathrm{dgm}(K_\bullet, \, f_\alpha) \right|_{R} \, \big) 
$$


## Application \#1: Filtration optimization

![](images/codensity_ex1.png){width=60%, fig-align="center"}

<br/>

$$ 
	\mu_p^{R} = 
	\mathrm{rank}\begin{bmatrix} \partial_{p+1}^{j + 1, k} & 0 \\
	0 & \partial_{p+1}^{i + 1, l}
	\end{bmatrix}
	- 
	\mathrm{rank}\begin{bmatrix} \partial_{p+1}^{i + 1, k} & 0 \\
	0 & \partial_{p+1}^{j + 1, l}
	\end{bmatrix}
$$

## Application \#1: Filtration optimization

![](images/codensity_ex2.png){width=60%, fig-align="center"}

<br/>

$$ 
\mu_p^{R} = 
\mathrm{tr}\begin{bmatrix} \lVert \partial_{p+1}^{j + 1, k} \rVert_\ast & 0 \\
0 & \lVert \partial_{p+1}^{i + 1, l} \rVert_{\ast}
\end{bmatrix}
- 
\mathrm{tr}\begin{bmatrix} \lVert \partial_{p+1}^{i + 1, k} \rVert_\ast & 0 \\
0 & \lVert  \partial_{p+1}^{j + 1, l} \rVert_\ast
\end{bmatrix}
$$

## Application \#1: Filtration optimization

![](images/codensity_ex3.png){width=60%, fig-align="center"}

<br/>

$$ 
\hat{\mu}_p^{R} = 
\mathrm{tr}\begin{bmatrix} \Phi_\tau(\partial_{p+1}^{j + 1, k}) & 0 \\
0 & \Phi_\tau(\partial_{p+1}^{i + 1, l})
\end{bmatrix}
- 
\mathrm{tr}\begin{bmatrix} \Phi_\tau(\partial_{p+1}^{i + 1, k}) & 0 \\
0 & \Phi_\tau(\partial_{p+1}^{j + 1, l})
\end{bmatrix}
$$

:::{.fragment }

$$\boxed{\text{There exists a positive }\tau^\ast > 0 \text{ such that } \mu_p^R = \lceil \hat{\mu}_p^R \rceil \text{ for all } \tau \in (0, \tau^\ast]}$$

:::


## Application \#1: Filtration optimization {visibility="hidden"}

![](images/combinatorial_explosion.png){width=60%, fig-align="center"}[^1]


[^1]: Xu, Weiyu, and Babak Hassibi. "Precise Stability Phase Transitions for $\ell_1 $ Minimization: A Unified Geometric Framework." IEEE transactions on information theory (2011)


## Application \#1: Filtration optimization

![](images/codensity_ex4.png){width=60%, fig-align="center"}

<br/>

$$ \mu_p^{R} = \mathrm{tr}\begin{bmatrix} \Phi_\tau(\partial_{p+1}^{j + 1, k}) & 0 \\
0 & \Phi_\tau(\partial_{p+1}^{i + 1, l})
\end{bmatrix}
- 
\mathrm{tr}\begin{bmatrix} \Phi_\tau(\partial_{p+1}^{i + 1, k}) & 0 \\
0 & \Phi_\tau(\partial_{p+1}^{j + 1, l})
\end{bmatrix}
$$

## Application \#1: Filtration optimization

![](images/codensity_ex5.png){width=60%, fig-align="center"}

<br/>

$$ \mu_p^{R} = \mathrm{tr}\begin{bmatrix} \Phi_\tau(\partial_{p+1}^{j + 1, k}) & 0 \\
0 & \Phi_\tau(\partial_{p+1}^{i + 1, l})
\end{bmatrix}
- 
\mathrm{tr}\begin{bmatrix} \Phi_\tau(\partial_{p+1}^{i + 1, k}) & 0 \\
0 & \Phi_\tau(\partial_{p+1}^{j + 1, l})
\end{bmatrix}
$$

:::{.fragment style="text-align: center"}

Similar to the Iterative Soft-Thresholding Algorithm (ISTA) [@beck2017first]

:::

## Combinatorial Laplacian 

__Relax #3:__ Replace $\partial \mapsto L$ with _combinatorial Laplacians_ [@horak2013spectra]:

$$ \Delta_p = \underbrace{\partial_{p+1} \partial_{p+1}^T}_{L_p^{\mathrm{up}}}  + \underbrace{\partial_{p}^T \partial_{p}}_{L_p^{\mathrm{dn}}} $$

:::{.fragment}

$f_\alpha$ is 1-to-1 correspondence with inner products on cochain groups $C^p(K, \mathbb{R})$ 

$$L_p^{i,j}(\alpha) \Leftrightarrow \langle \; f,\, g \; \rangle_{\alpha} \; \text{ on } \;  C^{p+1}(K, \mathbb{R})$$

::: 

:::{.fragment}

Benefits: Symmetric, positive semi-definite, have "nice" linear and quadratic forms:
$$
L_p^{\text{up}}(\tau, \tau')= \begin{cases}
		 \mathrm{deg}_f(\tau) \cdot f^{+/2}(\tau) & \text{ if } \tau = \tau' \\ 
%		\mathrm{deg}(\tau_i) & \text{ if } i = j \\ 
		s_{\tau, \tau'} \cdot  f^{+/2}(\tau) \cdot f(\sigma) \cdot f^{+/2}(\tau') & \text{ if } \tau \overset{\sigma}{\sim} \tau' \\
		0 & \text{ otherwise} 
	\end{cases}
$$

$\implies$ can represent operator in "matrix-free" fashion
:::

## Interpretation: Diffusion

::: {.fragment fragment-index=1 style="text-align: center"}

Diffusion processes on graphs often modeled as time-varying $v^{(t)} \in \mathbb{R}^n$ via:

$$ v'^{(t)} = -L v^{(t)} \quad \Leftrightarrow \quad L \cdot u(x,t) = - \frac{\partial u(x, t)}{\partial t} $$

::: 

::: {.fragment fragment-index=2 layout-valign="center" layout-ncol=2 layout="[[30, 70]]" style="text-align: center"}

![](images/heat_diffusion.png){width=250 fig-align="right"}

![](images/heat_diffusion.jpg){width=550 fig-align="left"}

:::

::: {.fragment fragment-index=3 style="text-align: center"}

Value of $v(t)$ at time $t$ given by the _Laplacian exponential diffusion kernel_:

$$v^{(t)} = \mathrm{exp}(-t L) v^{(0)}$$

<!-- $$H_t = U \mathrm{exp}(-t \Lambda) U' = \sum\limits_{i=1}^n e^{-t \lambda_i} \, u_i \, u_i^T$$ -->

:::

:::{.aside style="margin-top: 2em !important;"}

Images from @crane2017heat and @sharma2011topologically

::: 


## Interpretation: Diffusion

<br/>

::: {.fragment fragment-index=1}

<div style="text-align: center;">

Under the appropriate parameters for $\nu$ and $\rho$^[This $\phi$ corresponds to setting $\nu(\tau) = \tau$ and $p(x) = \mathrm{exp}(-x)$ for $x > 0$ and $p(x) = 0$ otherwise], $\phi$ takes the form:

</div>

$$
\phi(x, \tau) = 1 - \mathrm{exp}(- x / \tau)
$$

:::


::: {.fragment fragment-index=3}

<div style="text-align: center;">

The corresponding Löwner operator and its Schatten $1$-norm is given by (for $t = \tau^{-1}$):

</div>

$$
\Phi_\tau(X) \simeq U \mathrm{exp}(-t \Lambda) U^T, \quad \lVert \Phi_\tau(X) \rVert_\ast \simeq \sum\limits_{i = 1}^n \mathrm{exp}(-t \cdot \lambda_i)
$$

:::

::: {.fragment fragment-index=4}

<div style="text-align: center;">

This is the <span style="color: red;"> _Heat kernel_ </span> and its Schatten-1 norm is the <span style="color: red;"> _heat kernel trace_ </span>

</div>

:::

## Application \#2: Directional Transform 

<hr/>

:::{.fragment style="text-align: center"}

Consider filtering a fixed $K$ embedded in $R^d$ by a 1-parameter directions in $S^{d-1}$

$$
K_\bullet = K(v)_i = \{\, x \in X \mid \langle x, v \rangle \leq i  \,\}
$$

:::

:::{.fragment style="text-align: center"}

![](animations/dt_single.gif){width=350 height=100% fig-align="center"}

:::

<!-- ![](images/dt.png){width=400 height=100% fig-align="center"} -->

## Application \#2: Directional Transform 

<hr/>

$$
K_\bullet = K(v)_i = \{\, x \in X \mid \langle x, v \rangle \leq i  \,\}
$$

![](animations/ph_transform.gif){width=550 height=100% fig-align="center"}


:::{.fragment}

$$\{ \; \mathrm{dgm}(v) : v \in S^{d-1} \; \} \Leftrightarrow \text{Persistent Homology Transform (PHT)}$$ 

:::

:::{.fragment style="text-align: center;"}

Turner et al.$^1$ show PHT(X) is injective, sparking an inverse theory for persistence!

:::

::: aside 

\(1\): @turner2014persistent

:::

## Application \#2: Directional Transform 

<hr/>

:::{.fragment .fade-in-then-semi-out}

Injectivity of the PHT $\implies$ can impose metric $\mathcal{D}$ over <span style="color: orange;"> _shape space_ </span> by integrating $d_B$:

$$ \mathcal{D}(X, Y) \triangleq \sum_{p=0}^d \int_{S^{d-1}} \operatorname{d}_B\left(\mathrm{dgm}_p(X, v) \right), \left( \mathrm{dgm}_p(Y, v) \right) dv $$

:::

<!-- :::{.fragment layout=[[50,50]]}

![](images/turtle1.png){width=650 height=100% fig-align="right"}

![](images/turtle2.png){width=650 height=100% fig-align="left"}

::: -->

:::{.fragment .fade-in-then-semi-out}
To make $\mathcal{D}(\cdot, \cdot)$ blind to rotations, Turner^[@turner2014persistent] minimize $\mathcal{D}$ over rotations $\{R_i\}_{i=1}^m$:

$$ d_{\mathrm{PHT}}(X, Y) = \inf_{i = 1, \dots, m} \mathcal{D}(X, R_i(Y)) $$

:::


:::{.fragment .fade-in-then-semi-out}

When $m = \lvert V \rvert$, computing $d_{\mathrm{PHT}}(X, Y)$ requires: 

<ol>
  <li> Computing $\mathrm{dgm}_p(\cdot, v)$ for $\{X,Y\}$ over sufficiently dense $\mathcal{V} \subset S^{d-1}$ ( ${\color{orange} O(m \cdot N^3)}$ )</li>
  <li> Minimizing $\mathcal{D}$ over all $m$ rotations ( ${\color{red}\approx O(m^2 \cdot N^{1.5} \log N)}$ )$^1$ </li>
</ol>

:::

::: aside 
\(1\) Assumes $d_B \sim O(n^{1.5} \log n)$, following [@kerber2017geometry]. 
:::

## Application \#2: Directional Transform 


:::{layout=[[26,62]] style="text-align: center;"}

![](animations/dt_single.gif){height=100% fig-align="right"}

![](animations/trace_summary.gif){height=100% fig-align="left"}

:::


When $m = \lvert V \rvert$, computing $d_{\mathrm{DT}}(X, Y)$  $\xcancel{d_{\mathrm{PHT}}(X, Y)}$  requires: 

<ol>
  <li> Computing $\mathrm{tr}(\phi_\tau(\cdot))$ for $\{X,Y\}$ over sufficiently dense $\mathcal{V} \subset S^{d-1}$ ($\approx {\color{blue} O(m \cdot N^2)}$)</li>
  <li> Phase-aligning two _periodic_ signals via FFT ( ${\color{green} O(m \log m)}$ ) </li>
</ol>


## Application \#2: Directional Transform 



![](images/shape_signatures.png){width=850 height=100% fig-align="center"}

<!-- :::{layout=[[50,50]]}

![](images/turtle1.png){width=650 height=100% fig-align="right"}

![](images/bone1.png){width=650 height=100% fig-align="left"}

::: -->



## Overview 

<ul>
  <li>Introduction</li>
  <ul>
    <li>Rank duality</li>
    <li>Bypassing diagrams</li>
    <li>Technical observations</li>
  </ul>
  <li><p>Spectral rank relaxation</p></li>
  <ul>
    <li>Parameterizing $C(K, \mathbb{R})$</li>
    <li>Spectral functions</li>
    <li>Variational interpretations + examples</li>
  </ul>
  <li><p>Computation</p>
  <ul>
    <li>Lanczos Iteration</li>
    <li>Stochastic trace estimation</li>
    <li>Scalability</li>
  </ul>
</li>
</ul>

## Computation in quadratic time 

<hr/> 

<div class="incremental" style="list-style-type: none; align=left;">

:::{.fragment .fade-in-then-semi-out style="text-align: left"}

Computing $A = U \Lambda U^T$ for any $A \in \mathbf{S}_+^n$ bounded by $\Theta(n^3)$ time and $\Theta(n^2)$ space^[Assumes the standard matrix multiplication model for simplicity (i.e.  excludes the Strassen-family)]

:::

::: {.fragment .fade-in-then-semi-out style="text-align: left"}

<u>However</u>, if $v \mapsto Av \approx O(n)$, then $\Lambda(A)$ obtainable in <span style="color: red;"> $O(n^2)$ time </span> and <span style="color: red;">$O(n)$ space</span>!

:::

::: {.fragment .fade-in-then-semi-out style="text-align: left"}

__Idea__: For some random $v \in \mathbb{R}^n$, expand successive powers of $A$:

$$ 
\begin{align}
K_j &= [ v \mid Av \mid A^2 v \mid \dots \mid A^{j-1}v] && \\
Q_j &= [ q_1, q_2, \dots, q_j] \gets \mathrm{qr}(K_j) && \\
T_j &= Q_j^T A Q_j &&
\end{align}
$$

:::

::: {.fragment .fade-in-then-semi-out style="text-align: left"}

It turns out that every $A \in \mathbf{S}$ expanded this way admits a _three-term recurrence_ 

$$ A q_j = \beta_{j-1} q_{j-1} + \alpha_j q_j + \beta_j q_{j+1} $$

:::

::: {.fragment .fade-in-then-semi-out style="text-align: left"}

<div style="text-align: center; font-size: 34px;"> 

This is the renowned *__Lanczos method__* for Krylov subspace expansion

</div>

<img src="images/lanczos_top_10.png" style="position: fixed; top: 50%; left: 50%; transform: translate(-50%, -65%); height: 60vh !important; width: 100%;"> </img>


:::

</div>

## Lanczos iteration

![](animations/lanczos_krylov.gif){width=75% height=100% fig-align="center"}

<div style="padding-left: 1em; border: 1px solid black; margin: 0em; ">
__Theorem [@simon1984analysis]__: Given a symmetric rank-$r$ matrix $A \in \mathbb{R}^{n \times n}$ whose matrix-vector operator $A \mapsto A x$ requires $O(\eta)$ time and $O(\nu)$ space, the Lanczos iteration computes $\Lambda(A) = \{ \lambda_1, \lambda_2, \dots, \lambda_r \}$ in $O(\max\{\eta, n\}\cdot r)$ time and $O(\max\{\nu, n\})$ space _when executed in exact arithmetic_. 
</div>

## Randomized trace approximation 

<!-- Lanczos method is unstable + exact arithmetic is impractical (this _cannot_ be ignored)^[These numerical errors not only affect convergence] -->

<hr/>


:::{.fragment .fade-in-then-semi-out style="text-align: center"}

Let $A = \mathbb{R}^{n \times n}$. If $v \in \mathbb{R}^n$ a $\mathrm{r.v.}$ with $\mathbb{E}[vv^T] = I$, then: 

:::

:::{.fragment .fade-in-then-semi-out style="text-align: left;"}

$$ \mathtt{tr}(A) = \mathtt{tr}(A \mathbb{E}[v v^T]) \quad\quad \text{(identity)} $$

:::

:::{.fragment .fade-in-then-semi-out style="text-align: left;"}

$$ \hphantom{\mathtt{tr}(A)} = \mathbb{E}[\mathtt{tr}(Avv^T)] \quad\quad \text{(linearity)} $$

:::

:::{.fragment .fade-in-then-semi-out style="text-align: left;"}

$$ \hphantom{\mathtt{tr}(A)} = \mathbb{E}[\mathtt{tr}(v^T A v)] \quad\quad \text{(cylic)}$$

:::

:::{.fragment .fade-in-then-semi-out style="text-align: left;"}

$$ \hphantom{\mathtt{tr}(A)} = \mathbb{E}[v^T A v] \quad\quad \text{(symmetry)}$$

:::


:::{.fragment .fade-in-then-semi-out style="text-align: center"}
$$
\implies \mathtt{tr}(A) \approx \frac{1}{n_v}\sum\limits_{i=1}^{n_v} v_i^\top A v_i, \quad \text{ for } v_i \sim \mathcal{N}(\mu=0, \sigma = 1)
$$
:::



## Randomized trace approximation 

<hr/> 

<div style="padding-left: 1em; border: 1px solid black; margin: 0em; ">
__Theorem [@ubaru2017fast]__: For any $A \in S_+^n$, if $n_v \geq (6/\epsilon^2) \log(2/\eta)$ unit-norm $v \in \mathbb{R}^n$ are drawn uniformly from $\{-1, +1\}^n$, then for any $\epsilon, \eta \in (0,1)$:
$$ \mathrm{Pr}\bigg(\lvert \mathrm{tr}_{n_v}(A) - \mathrm{tr}(A) \rvert \leq \epsilon \cdot \mathrm{tr}(A) \bigg) \geq 1 - \eta$$

<!-- and $f: [a,b] \to \mathbb{R}$ analytic in the interval $[a,b]$, -->

</div>

<!-- :::{.fragment .fade-in-then-semi-out style="text-align: center"}

$\Phi_\tau(X)$ are _trace-class_, satisfy $\mathtt{tr}(\Phi_\tau(X)) = \sum\limits_{i=1}^n \phi(\lambda_i, \tau)$

::: -->

::::{.fragment .fade-in-then-semi-out style="text-align: center"}

$\implies$ Generalizes to _spectral functions_ $\Phi_\tau(X)$ via __stochastic Lanczos quadrature__$^1$

$$ \mathrm{tr}(\Phi_\tau(X)) \approx \frac{n}{n_v} \bigg( e_1^T \, \Phi_{\tau}(T) \, e_1 \bigg ), \quad \quad T = \mathrm{Lanczos}(X) $$

:::

:::{.fragment .fade-in-then-semi-out style="text-align: center"}

$$\implies \hat{\mu}_p^{R} \sim O(n \cdot s l^2)^{1} \text{ time and } O(n) \text{ space }$$ 

Where $n \sim \lvert K^p \rvert$, and both $l, s \sim O(1)$ are small constants$^2$ 

:::

::: aside 

\(1\) See @ubaru2017fast. \(2\) We found setting the lanczos polynomial degree $l \leq 20$ and number of Monte-carlo iterations $s \leq 200$ was sufficient for most applications.

:::

## Scalability

![](images/imate_trace_bench.png){width=750 height=100% fig-align="center"}

Figure taken from the new `imate` package documentation ([`[gh]/ameli/imate`](https://ameli.github.io/imate/index.html))


## Application \#3: Computing $\mathrm{dgm}$'s


:::{layout="[[50,50]]" layout-valign="bottom"}

![](images/divide_conquer_dgm.png){width=400 height=100% fig-align="right"}

![](images/bisection_tree.png){width=400 height=100% fig-align="left"}

:::

:::{.fragment}

<div style="padding-left: 1em; border: 1px solid black; margin: 0em; ">
__Theorem [@chen2011output]__: For a simplicial complex $K$ of size $\lvert K \rvert = n$, computing the $\Gamma$-persistent pairs requires $O(C_{(1-\delta)\Gamma}\mathrm{R}(n) \log n)$ time and $O(n + \mathrm{R}(n))$ space, where $R(n)$ ($R_S(n)$) is the time (space) complexity of computing the rank of a $n \times n$ boundary matrix.
</div>

:::


## Conclusion 

Spectral relaxation of rank invariant using _matrix functions_ 

- Suitable for parameterized families of filtrations
- Differentiable + amenable for optimization 
- Stable to perturbations in $f_\alpha$ when $\tau > 0$ 
- Excellent compute properties. Implementation ongoing. 
- Better optimizer implementation also ongoing. 

<!-- Looking for collaborators + ideas! In particular: -->

<!-- :::{.incremental}

- Optimizing parameterized filtrations
- Differentiating n-parameter families of filtrations
- Encoding features with Laplacian spectra
- Sparse minimization problems (compressive sensing)
- Understanding connections to other areas of math

::: -->

## Acknowledgements & Advertising

- Explicit proof of (unfactored) $\beta_p^{\ast}$ in CT book by Tamal Dey & Yusu Wang 
- Expression + PH algorithm involving $\mu_p^\ast$ by Chao Chen & Michael Kerber 
- Insightful developments by (Bauer|Masood|Giunti|Houry|Kerber|Rathod)
- SLQ due to @ubaru2017fast; code ported from [imate](https://ameli.github.io/imate/)

<hr/>

<div style="text-align: center; font-size: 40px;"> 

To see the code develop and track its progress, head to: 

[[gh]/peekxc](https://github.com/peekxc) or [mattpiekenbrock.com](https://mattpiekenbrock.com/)

</div>

<hr/>

<div style="text-align: center; font-size: 80px;"> 

Thanks for listening! 

</div>

## References

::: {#refs}
:::

<script>
  window.WebFontConfig = {
    custom: {
      families: ['KaTeX_AMS'],
    },
  };
</script>

## Other applications (time permitting)

![](images/dw_chi_comp.png)


## Lanczos 

Spectra of Laplacian operators well-studied: 

- Iterative Krylov methods / Lanczos dominate solving sparse systems$^2$
- Many laplacian preconditioning methods known [@jambulapati2021ultrasparse]
- Nearly optimal algorithms known for SDD [@stathopoulos2007nearly]


## Computation 

:::{.fragment}

- Permutation invariance $\implies$ can optimize memory access of $\mathtt{SpMat}$ operation

:::

:::{.fragment}

- Any complex data structure suffices, e.g. tries$^2$, combinadics, etc...

::: 

:::{.aside}

See [@komzsik2003lanczos, @parlett1995we] for an overview of the Lanczos. See [@boissonnat2014simplex] for representing complexes.

:::


## Computation 


Preliminary experiments suggest the scalability is promising 

![](images/watts_strogatz_perf.png)

- $\approx \, \leq 25$ Lanczos vectors to approximate full spectrum at $ > 0$
- $\implies O(n)$ memory to obtain $\lVert \cdot \rVert_\ast$ in $O(n^2)$ time (with small constants!)
- Larger values $$ or lower numerical tolerances $\implies$ essentially linear time compute
- Previousy computed eigenvectors can be re-used for "warm restarts"



## Experiment \#1: Directional Transform  

![](images/shape_signatures.png){width=80% height=100% fig-align="center"}

:::{.notes}
Luis mentioned modding out rotations and translations adn scale to compare shapes. We can handle rotations via phase alignment. 

Sarah mentioned handling orientation.

:::


## Permutation Invariance {visibility="hidden"}

Consider the setting where $f_\alpha : \mathbb{R} \to \mathbb{R}^N$ is an $\alpha$-parameterized filter function: 

$$ \mu_p^R(\, f_\alpha \, ) = \{ \mu_p^R(K_\bullet^\alpha) : \alpha \in \mathbb{R} \}$$

Difficult to compute $R_\alpha = \partial_\alpha V_\alpha$ for all $\alpha$ as $K_\bullet = (K, f_\alpha)$ is changing constantly...

<!-- #\mathrm{rank}(\partial_p(K_\bullet( \, f_\alpha \,))) -->

<!-- On the other hand... -->
$$ \mathrm{rank}(\partial_p(K_\bullet)) \equiv \mathrm{rank}(P^T \partial_p(K) P) $$
$$ \mathrm{rank}(\partial_p(K_\bullet)) \equiv \mathrm{rank}(W \mathrm{sgn}(\partial_p(K)) W) $$

Thus we may decouple $f_\alpha$ and $K$ in the computation: 

$$
\begin{align*}
 \mu_p^{R}(K,f_\alpha) &\triangleq \mathrm{rank}\big(\,\hat{\partial}_{q}^{j + \delta, k}\,\big) - \; \dots \; + \mathrm{rank}\big(\, \hat{\partial}_{q}^{i + \delta, l}\,\big)  \\
&\equiv \mathrm{rank}\big(\,V_p^j \circ \partial_{q} \circ W_q^k \,\big) - \; \dots \; + \mathrm{rank}\big(\,V_p^{i+\delta} \circ \partial_{q} \circ W_q^l \,\big)
 \end{align*}
 $$

where the entries of $V$, $W$ change continuously w/ $\alpha$, while $\partial_q$ remains _fixed_...

## Spectral functions {visibility="hidden"}

Nuclear norm $\lVert X \rVert_\ast = \lVert \mathbf{\sigma} \rVert_1$ often used in sparse minimization problems like _compressive sensing_ due to its convexity in the unit-ball $\{A \in \mathbb{R}^{n \times m} : \lVert A \rVert_2 \leq 1 \}$

:::{layout="[[50,50]]" layout-valign="bottom"}

![](images/l0_l1.png){width=300 height=100% fig-align="right"}

![](images/convex_envelope.png){width=320 height=100% fig-align="left"}

:::

<div style="text-align: center;"> 

__Left:__ The $\ell_0$ and $\ell_1$ norms on the interval $[-1,1]$

__Right:__ $g$ forms the convex envelope of $f$ in the interval $[a,b]$

</div> 

## Spectral functions {visibility="hidden"}

Unfortunately, $\lVert \cdot \rVert_\ast$ often a poor substitute for rank

![](images/rank_relax.png){width=70% height=100% fig-align="center"}

__Left:__ The $\ell_0$ and $\ell_1$ norms on the interval $[-1,1]$
__Right:__ 



## Experiment \#2: Intrinsic signatures

<div style="text-align: center;"> 

__Dataset:__ 3D meshes of animals in different poses [@chazal2009gromov]

</div>

![](images/gh_data_pose.png){width=425 height=100% fig-align="center"}

<div style="text-align: center;"> 

__Challenge:__ Recognize intrinsic shape categories (via a distance metric)

</div>

## Experiment \#2: Intrinsic signatures

The _Gromov-Hausdorff_ distance yields a metric on the set of compact metric spaces $\mathcal{X}$

$$
d_{GH}(d_X, d_Y) = \sup_{x \in X, y \in Y} \lvert d_X(x, \psi(y)) - d_Y(y, \phi(x))\rvert 
$$

![](images/camel_gh.png){width=600 height=100% fig-align="center"}

Using intrinsic metric makes $d_{\mathrm{GH}}$ blind to e.g. shapes represented in different _poses_

:::{.fragment}

<div style="text-align: center;"> 

Unfortunately, the GH distance is NP-hard to compute [@memoli2012some]

</div>

:::

## Experiment \#2: Intrinsic signatures

<div style="text-align: center;"> 

It's known $d_B$ ($d_W$) on Rips filtrations $\mathcal{R}(X, d_X)$ lower bound GH (GW) distance

</div> 

$$ 
d_B(\mathrm{dgm}_p(\mathcal{R}(X, d_X)), \mathrm{dgm}_p(\mathcal{R}(Y, d_Y))) \; \leq \; d_{GH}((X, d_X), (Y, d_Y))
$$

:::{layout-ncol=1}

![](images/camel_gh_rips_comparison.png){width=875 height=100% fig-align="center"}

:::

<div style="text-align: center;"> 

Motivates use of persistence in metric settings for e.g. shape classification!

</div> 

## Experiment \#2: Intrinsic signatures

<div style="text-align: center;"> 

__Issue:__ Diagrams are far from injective, cannot distinguish e.g. stretched shapes

</div> 

![](images/dgm_noninjective.png){width=575 height=100% fig-align="center"}

<div style="text-align: center;"> 

The lower bound on $d_{\mathrm{GH}}$ could be totally useless!

</div> 


## Experiment \#2: Intrinsic signatures

:::{.fragment}
Lower bounds extend to Rips filtrations _augmented_ with real-valued functions $f, g$: 

$$
\mathcal{R}(f) \triangleq \mathcal{R}(X, d_X, f) = \{\mathcal{R}_\alpha(X_\alpha)\}_{\alpha > 0}, \quad X_\alpha \triangleq f^{-1}((-\infty, \alpha)) \subseteq X
$$

:::

:::{.fragment}

The diagrams from $\mathcal{R}(\lambda \cdot f_X)$ represent _stable signatures_ for each $\lambda > 0$:

$$
d_B(\mathcal{R}(\lambda \cdot f_X), \mathcal{R}(\lambda \cdot f_Y)) \leq \max(1, \lambda L) \cdot d_{\mathrm{GH}}(X, Y)
$$

:::

<!-- <div style="text-align: center;">  -->

<!-- </div>  -->

<!-- <hr>  -->

:::{.fragment}

Chazal showed these bounds extend to metrics on _augmented_ metric spaces:

$$
\mathcal{X}_1 = \{ (X, d_X, f_X) \mid (X, d_X, f_X) \in \mathcal{X}, f_X: X \to \mathbb{R} \text{ continuous }\}
$$

These signatures also extend to measure metric spaces, see [@chazal2009gromov]

::: 

:::{.fragment}

<br>  

<span style="color: red;"> NOTE:   </span> 
Size of $L$ depends on the choice of $f$ + each $\lambda$ produces a new signature! 

:::

## Experiment \#2: Intrinsic signatures

<div style="text-align: center;"> 

__Ex:__ The _eccentricity_ function $e_X^1(x) = \max_{x' \in X} d_X(x,x')$ has $L = 2$ 

</div> 

![](images/dgm_noninjective2.png){width=775 height=100% fig-align="center"}

<div style="text-align: center;"> 

Augmenting via a fraction of $e_X^1$ modifies the diagrams of the ellipsoid significantly
<!-- , while the ones for the sphere hardly change due to the fact that the eccentricity is constant -->

</div> 


## Experiment \#2: Intrinsic signatures

Lower bounds extend to Rips filtrations _augmented_ with real-valued functions $f, g$: 

$$
d_B(\mathcal{R}(\lambda \cdot f_X), \mathcal{R}(\lambda \cdot f_Y)) \leq \max(1, \lambda L) \cdot d_{\mathrm{GH}}(X, Y)
$$

:::{layout-ncol=4}

![](images/camel1_rips.png){width=450 height=100% fig-align="left"}

![](images/camel_dgm_rips.png){width=450 height=100% fig-align="left"}

![](images/camel1_ecc.png){width=450 height=100% fig-align="left"}

![](images/camel_dgm_ecc.png){width=450 height=100% fig-align="left"}

:::

<div style="text-align: center;"> 

Larger values of $\lambda$ yield worse bounds, but can lead to simpler diagrams

</div>

<!-- Extra structure combines stability of persistence with flexibility of metrics -->


## Experiment \#2: Intrinsic signatures


<div style="text-align: center;"> 

Each choice of $\lambda > 0$ yields a _stable signature_ via $\mathcal{R}(\lambda \cdot f_X)$

</div> 

<div style="text-align: center;"> 

Which value of $\lambda$ to choose?

</div>

![](images/camel1_interp1.png){width=950 height=100% fig-align="center"}



## Experiment \#2: Intrinsic signatures


<div style="text-align: center;"> 

Each choice of $\lambda > 0$ yields a _stable signature_ via $\mathcal{R}(\lambda \cdot f_X)$

</div> 

<div style="text-align: center;"> 

Which value of $\lambda$ to choose?

</div>

![](images/camel1_interp2.png){width=950 height=100% fig-align="center"}

<div style="text-align: center;"> 

We sample from $\Delta_+$ randomly, retaining signatures with sufficient topological activity

</div>

## Experiment \#2: Intrinsic signatures
...and compared the computed spectral signatures under the relative distance metric: 

<!-- \partial_p^\ast = U \Sigma V^T -->
$$
\Lambda(\mu_p^R) = \{\sigma_1, \sigma_2, \dots, \sigma_n \}, \quad \quad \chi(\mathbf{\sigma}, \mathbf{\tilde{\sigma}}) = \sum\limits_{i=1}^n \frac{\lvert \sigma_i - \tilde{\sigma}_i \rvert}{\sqrt{\sigma_i + \tilde{\sigma}_i}}
$$

![](images/dw_chi_comp.png){width=750 height=100% fig-align="center"}


<!-- ## Experiment \#3: Filtration optimization -->

<!-- ![](images/smoothed_mu.png){width=55% height=100%} -->





<!-- __Summary:__ We can obtain $\mu_p^R(K, f_\alpha)$ for varying $\alpha$ by using thresholded versions of $f_\alpha$ as scalar-products  -->

## Overview 