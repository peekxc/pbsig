---
title: Spectral relaxations of Persistence Invariants
subtitle: Relaxing the rank invariant for parameterized settings
author: Matt Piekenbrock$\mathrm{}^\dagger$   \&   Jose Perea$\mathrm{}^\ddagger$
format: 
  revealjs:
    smaller: true
    footer: "Spectral relaxations of persistent rank invariants"
    theme: simple 
    institute: 
      - $\dagger$ Khoury College of Computer Sciences, Northeastern University
      - $\ddagger$. Department of Mathematics and Khoury College of Computer Sciences, Northeastern University
    spotlight:
      useAsPointer: false
      size: 35
      toggleSpotlightOnMouseDown: false
      spotlightOnKeyPressAndHold: 16 # shift : 
      presentingCursor: "default"
    overview: true
    margin: 0.075
revealjs-plugins:
  - spotlight
html: 
  html-math-method: katex
filters:
  # - pandoc-katex.lua
  - roughnotation
# keycodes: https://gcctech.org/csc/javascript/javascript_keycodes.htm
# bibliography: references.bib
---

## Relaxing the Rank invariant  

There are many mappings from $\mathrm{dgm}$'s to function spaces 
  

<!-- :::{.incremental style="list-style-type: none; margin-left:2.5em; align=center;"} -->

::: {.fragment .fade-in-then-semi-out style="text-align: left"}

:::{.columns}

:::{.column width="40%" style="list-style-type: none; margin-left:2.5em; align=center;"}

- Persistence Landscapes 

::: 

::: {.column width="40%"}

![](pers_landscape.png)

:::

:::

:::

## Relaxing the Rank invariant  

There are many mappings from $\mathrm{dgm}$'s to function spaces 
  

::: {.fragment .fade-in-then-semi-out style="text-align: left"}

:::{.columns}

:::{.column width="40%" style="list-style-type: none; margin-left:2.5em; align=center;"}

- Persistence Landscapes 
- Persistence Images

::: 

::: {.column width="40%"}

![](pers_image.png)

:::

:::

:::


## Relaxing the Rank invariant  

There are many mappings from $\mathrm{dgm}$'s to function spaces 
  

::: {.fragment .fade-in-then-semi-out style="text-align: left"}

:::{.columns}

:::{.column width="40%" style="list-style-type: none; margin-left:2.5em; align=center;"}

- Persistence Landscapes 
- Persistence Images
- Many others...

::: 

::: {.column width="40%"}

![](pers_landscape.png)
![](pers_image.png)

:::

:::

:::


## Relaxing the Rank invariant  


There are many mappings from $\mathrm{dgm}$'s to function spaces 
  
:::{.columns}

:::{.column width="40%" style="list-style-type: none; margin-left:2.5em; align=center;"}

- Persistence Landscapes 
- Persistence Images
- Many others...

::: 

<!-- ::: {.r-stack}
![](pers_landscape.png){width=20%} 
![](pers_image.png){width=20%}
::: -->


:::


:::{.fragment .fade-in-then-semi-out style="text-align: left"}

Less work done on _the rank invariant_ (exceptions: [][][])

$$ \beta_p^{i,j} : \mathrm{rank}(H_p(K_i) \to H_p(K_j))$$

:::

:::{.incremental}
:::{.fragment .fade-in style="text-align: left"}
We introduce a _spectral-relaxation_ of $\beta_p^{i,j}$ that:

<div style="margin-left:2.5em;">

1. is smooth + differentiable on $\mathbf{S}_+$ and semismooth on $\mathbb{R}^{n \times n}$
2. $(1{\textstyle -}\epsilon)$ approximates of $\beta_p^{i,j}$ for any $\epsilon > 0$
3. Derives from non-harmonic spectra of Laplacian operators
4. Has several compute advantages over current approaches

</div>

:::
:::
<!-- - Continuous relaxation 
- Captures persistence properties
- Doesn't require $\mathrm{dgm}$ to compute -->

<!-- ![](../betti_add.png) -->

## Application: parameterized families 
Consider a 1-parameter family of simplicial complexes: 

$$\{ \; \mu_p^R(K_\alpha) \triangleq \mathrm{card}\big(\, \left.\mathrm{dgm}(K_\alpha) \right|_{R} \, \big) : \alpha \in \mathcal{A} \; \}$$


## Why not just use diagrams?

__Pro:__ Diagrams are stable, well-studied, and information rich.

__Con:__ Only competitive algorithm to produce a diagram is _reduction_ algorithm:

$$ \partial(K_\bullet) \mapsto R = \partial V $$

Extending the reduction algorithm to parameterized settings is highly non-trivial

:::: {.columns}


::: {.column width="33%"}
![](vineyard.gif){width=90%}
:::

::: {.column width="33%"}
![](complex_vine.gif){width=90%}
:::

::: {.column width="33%"}
![](spy_matrices.gif){width=90%}
:::

::::

Maintaining the $R = \partial V$ decomposition "across time" $\implies$ huge memory bottleneck

::: {.notes}
Reduction algorithm is a restricted form of gaussian elimination. 
:::


## Why not just use diagrams?

__Pro:__ Diagrams are stable, well-studied, and information rich.

__Con:__ Only competitive algorithm to produce a diagram is _reduction_ algorithm:

$$ \partial(K_\bullet) \mapsto R = \partial V $$

Extending the reduction algorithm to parameterized settings is highly non-trivial

:::: {.columns}


::: {.column width="33%"}
![](vineyard.gif){width=90%}
:::

::: {.column width="33%"}
![](complex_vine.gif){width=90%}
:::

::: {.column width="33%"}
![](spy_matrices.gif){width=90%}
:::

::::

Maintaining the $R = \partial V$ decomposition "across time" $\implies$ huge memory bottleneck

::: {.notes}
Reduction algorithm is a restricted form of gaussian elimination. 
:::

## Why the rank invariant?

There is a duality between diagrams its associated rank function:

$$ \mathrm{dgm}_p(f) \triangleq \{ \, ( \, i, j \,) \in \Delta_+ :  \mu_p^{i,j} \neq 0 \, \} \; \cup \; \Delta $$

$$\text{where: } \quad \mu_p^{i,j} = \left(\beta_p^{i,j{\small -}1} - \beta_p^{i,j} \right) - \left(\beta_p^{i{\small -}1,j{\small -}1} - \beta_p^{i{\small -}1,j} \right) \quad $$

Dually, the _"Fundamental Lemma of Persistent Homology"_ says that:
$$\beta_p^{k,l} = \sum\limits_{i \leq k} \sum\limits_{j > l} \mu_p^{i,j}$$

$\Longrightarrow$ Diagrams completely characterize their rank functions

$\Longleftrightarrow$ and vice versa

Much recent work on this front recently, in the context of multidimensional persistence[], Mobius inversions[], etc


## Revisiting the PH rank computation

<hr> 
$$ \beta_p^{i,j} : \mathrm{rank}(H_p(K_i) \to H_p(K_j))$$
	
<hr> 

:::{.incremental style="list-style-type: none;align=center;"}

::: {.fragment .fade-in-then-semi-out style="text-align: left"}
$\quad\quad\quad\quad\beta_p^{i,j} = \mathrm{dim} \big( \;\mathrm{Ker}(\partial_p(K_i))\; / \;\mathrm{Im}(\partial_{p+1}(K_j)) \; \big )$
:::

<!-- <li style="text-align: left" class="fragment fade-in-then-semi-out">
$\quad\quad\quad\quad\beta_p^{i,j} = \mathrm{dim} \big( \;\mathrm{Ker}(\partial_p(K_i))\; / \;\mathrm{Im}(\partial_{p+1}(K_j)) \; \big )$
</li> -->
::: {.fragment .fade-in-then-semi-out style="text-align: left"}
<!-- <li style="text-align: left" class="fragment fade-in-then-semi-out"> -->
$\quad\quad\quad\quad\hphantom{\beta_p^{i,j} }= \mathrm{dim}\big(\; \mathrm{Ker}(\partial_p(K_i)) \; / \; (\mathrm{Ker}(\partial_p(K_i)) \cap \mathrm{Im}(\partial_{p+1}(K_j))) \; \big )$
:::

::: {.fragment .fade-in-then-semi-out style="text-align: left"}
$\color{blue}{\quad\quad\quad\quad\hphantom{\beta_p^{i,j} }=\mathrm{dim}\big(\;\mathrm{Ker}(\partial_p(K_i)) \; \big)}$
$-$
$\color{red}{\mathrm{dim}\big( \; \mathrm{Ker}(\partial_p(K_i)) \cap \mathrm{Im}(\partial_{p+1}(K_j))\;\; \big)}$
:::

::: 

<!-- <br>  -->
::: {.fragment .fade-in-then-semi-out}
Rank-nullity yields the <span style="color: blue">left term</span>: 
$$
\mathrm{dim}\big(\mathrm{Ker}(\partial_p(K_i))\big) = \lvert C_p(K_i) \rvert - \mathrm{dim}(\mathrm{Im}(\partial_p(K_i)))
$$
:::

::: {.fragment .fade-in-then-semi-out}
Computing the <span style="color: red">right term</span> more nuanced: 
:::

:::{.incremental style="list-style-type: none; align=center; text-align: left; margin-left: 2.5em;"}
- Gaussian elimination []
- Pseudo-inverse []
- Orthogonal Projectors []
:::

<!-- <p style="text-align: left; margin-tb: 2em;">
<!-- [$\beta_p^{i,j} = \lvert C_p(K_i) \rvert - \mathrm{rank}(\partial_p(K_i)) -$]{.rn rn-type=circle rn-color=red} -->
<!-- [$\mathrm{dim}\big(\mathrm{Ker}(\partial_p(K_i)) \cap \mathrm{Im}(\partial_{p+1}(K_j))$]{.rn rn-type=strike-through rn-color=red} -->


## Key Observation

The structure theorem[^1] asserts 1-parameter persistence modules decompose in an _essentially unique_ way into indecomposables, which can be used to show: 

$$ R[i,j] \neq 0 \Leftrightarrow \mathrm{rank}(R^{i,j}) - \mathrm{rank}(R^{i\texttt{+}1,j}) + \mathrm{rank}(R^{i\texttt{+}1,j\text{-}1}) - \mathrm{rank}(R^{i,j\text{-}1}) \neq 0 $$
$$ 
\begin{equation}
\implies \mathrm{rank}(R^{i,j}) = \mathrm{rank}(\partial^{i,j})  
\end{equation}
$$
 
Often used to show correctness of reduction, but far more general, as it implies:

$$ \beta_p^{i,j}(K_\bullet) = \lvert C_p(K_i) \rvert - \mathrm{rank}(\partial_p^{1,i}) - \mathrm{rank}(\partial_{p+1 }^{1,j}) + \mathrm{rank}(\partial_{p+1}^{i + 1, j} ) $$

$$ \mu_p^{R}(K_\bullet) = \mathrm{rank}(\partial_{p+1}^{j + 1, k})  - \mathrm{rank}(\partial_{p+1}^{i + 1, k})  - \mathrm{rank}(\partial_{p+1}^{j + 1, l}) + \mathrm{rank}(\partial_{p+1}^{i + 1, l})  $$

<div style="margin-top: 0.01em; margin-bottom: 0.01em; border: 1px solid black; ">
__Corollary (Bauer)__[^1]: Any algorithm that preserves the ranks of the submatrices $\partial^{i,j}$ for all $i,j \in \{ 1, \dots, n \}$ is a valid barcode algorithm.
</div>

[^1]: Zomorodian \& Carlsson, "Computing persistent homology," Symposium on Computational Geometry, 2004.

::: {.notes}
shows 1-parameter persistence modules decompose in essentially unique way
The result about lower-left matrices having their rank preserved during reduction was mentioned in passing by Edelsbrunner et al. The fact that one can exploit this to express the rank invariant using only unfactored submatrices was first proven explicitly to my knowledge by Wang and Dey in their book. 
The corollary by Bauer et al. is now being studied in the context of reducing the sparsity of the matrices used in the reduction algorithm. 
:::

## Overview

- Introduction 
- Background: Duality between rank and diagrams 
- 

## A continuous & smooth relaxation 




## Rank Invariances 

$\mathrm{rank}$ is invariant to many things:

$$
\begin{align}
  \mathrm{rank}(A) &\triangleq \mathrm{dim}(\mathrm{Im}(A)) & \\
  &\equiv \mathrm{rank}(A^T) & \text{(adjoint)} \\
  &\equiv \mathrm{rank}(A^T A) & \text{(inner product)} \\
  &\equiv \mathrm{rank}(A A^T) & \text{(outer product)} \\
  &\equiv \mathrm{rank}(S^{-1}AS) & \text{(change of basis)} \\
  &\equiv \mathrm{rank}(O^T A O) & \text{(rotations)} \\
  &\equiv \mathrm{rank}(P^T A P) & \text{(permutation)} \\
  &\equiv \dots & \text{etc.}
\end{align}
$$

_Q_: Can we exploit some of these? 

## Rank test 2 

<!-- https://revealjs.com/auto-animate/ -->


<section data-auto-animate>

<h2> $$\mathrm{rank}(A) \triangleq \mathrm{dim}(\mathrm{Im}(A))$$ </h2>

</section>
<section data-auto-animate>
  <h2> $$\mathrm{rank}(A) \triangleq \mathrm{dim}(\mathrm{Im}(A))$$ </h2>
  <h2> $$\mathrm{rank}(A) \triangleq \mathrm{rank}(\mathrm{Im}(A))$$ </h2>
</section>

## Permutation Invariance 

Consider the setting where $f_\alpha : \mathbb{R} \to \mathbb{R}^N$ is an $\alpha$-parameterized filter function: 

$$ \mu_p^R(\, f_\alpha \, ) = \{ \mu_p^R(K_\bullet^\alpha) : \alpha \in \mathbb{R} \}$$

Difficult to compute $R_\alpha = \partial_\alpha V_\alpha$ for all $\alpha$ as $K_\bullet = (K, f_\alpha)$ is changing constantly...

<!-- #\mathrm{rank}(\partial_p(K_\bullet( \, f_\alpha \,))) -->

<!-- On the other hand... -->
$$ \mathrm{rank}(\partial_p(K_\bullet)) \equiv \mathrm{rank}(P^T \partial_p(K) P) $$
$$ \mathrm{rank}(\partial_p(K_\bullet)) \equiv \mathrm{rank}(W \mathrm{sgn}(\partial_p(K)) W) $$

Thus we may decouple $f_\alpha$ and $K$ in the computation: 

$$
\begin{align*}
 \mu_p^{R}(K,f_\alpha) &\triangleq \mathrm{rank}\big(\,\hat{\partial}_{q}^{j + \delta, k}\,\big) - \; \dots \; + \mathrm{rank}\big(\, \hat{\partial}_{q}^{i + \delta, l}\,\big)  \\
&\equiv \mathrm{rank}\big(\,V_p^j \circ \partial_{q} \circ W_q^k \,\big) - \; \dots \; + \mathrm{rank}\big(\,V_p^{i+\delta} \circ \partial_{q} \circ W_q^l \,\big)
 \end{align*}
 $$

where the entries of $V$, $W$ change continuously w/ $\alpha$, while $\partial_q$ remains _fixed_...

## Combinatorial Laplacian 

Recall $\mathrm{rank}(A) = \mathrm{rank}(A^T A) = \mathrm{rank}(A A^T)$

For boundary matrices, these matrices translate into up- and down- _Laplacians_:

$$ \Delta_p = \partial_{p+1} \partial_{p+1}^T  + \partial_{p}^T \partial_{p+1} $$

Thus: 
$$\begin{align}
\mathrm{rank}\big(\,V_p^j \circ \partial_{q} \circ W_q^k \,\big) \\
= \mathrm{rank}(L_{\ast, p}^{j,k}) + \dots - \mathrm{rank}(L_{\ast, p}^{j,k})
\end{align}
$$

"weight function" is 1-to-1 with scalar product on cochain groups $C^p(K, \mathbb{R})$ 

__Summary:__ We can obtain $\mu_p^R(K, f_\alpha)$ for varying $\alpha$ by using thresholded versions of $f_\alpha$ as scalar-products 



<script>
  window.WebFontConfig = {
    custom: {
      families: ['KaTeX_AMS'],
    },
  };
</script>