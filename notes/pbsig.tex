\documentclass[12pt]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subfiles}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{scalefnt}

\usepackage{tcolorbox}

\numberwithin{equation}{section}

%\usepackage{titlesec}

\usepackage{algorithm}
\PassOptionsToPackage{noend}{algpseudocode}
\usepackage{algpseudocode}
\usepackage{float}% http://ctan.org/pkg/float

\RequirePackage{fix-cm}

\usepackage{nicematrix}

\usepackage{mdframed}
\mdfdefinestyle{bframe}{%
    outerlinewidth=1pt,
    innertopmargin=0,
    innerbottommargin=5pt,
    innerrightmargin=8pt,
    innerleftmargin=8pt,
    backgroundcolor=white
   }
   
\usepackage{empheq}
\usepackage{xcolor}
%\subfile{whiteboxes}
\definecolor{shadecolor}{cmyk}{0,0,0,0}
\newsavebox{\mysaveboxM} % M for math
\newsavebox{\mysaveboxT} % T for text

\newcommand*\boxAppOne[2][Application \#1: Vectorizing persistence information]{%
  \sbox{\mysaveboxM}{#2}%
  \sbox{\mysaveboxT}{\fcolorbox{black}{white}{#1}}%
  \sbox{\mysaveboxM}{%
    \parbox[t][\ht\mysaveboxM+.5\ht\mysaveboxT+.5\dp\mysaveboxT][b]{\wd\mysaveboxM}{#2}%
  }%
  \sbox{\mysaveboxM}{%
    \fcolorbox{black}{shadecolor}{%
      \makebox[\linewidth-1em]{\usebox{\mysaveboxM}}%
    }%
  }%
  \usebox{\mysaveboxM}%
  \makebox[15pt][r]{%
    \makebox[\wd\mysaveboxM][l]{%
      \raisebox{\ht\mysaveboxM-0.5\ht\mysaveboxT+0.5\dp\mysaveboxT-0.5\fboxrule}{\usebox{\mysaveboxT}}%
    }%
  }%
}

\newcommand*\boxAppTwo[2][Application \#2: Differentiating persistence information]{%
  \sbox{\mysaveboxM}{#2}%
  \sbox{\mysaveboxT}{\fcolorbox{black}{white}{#1}}%
  \sbox{\mysaveboxM}{%
    \parbox[t][\ht\mysaveboxM+.5\ht\mysaveboxT+.5\dp\mysaveboxT][b]{\wd\mysaveboxM}{#2}%
  }%
  \sbox{\mysaveboxM}{%
    \fcolorbox{black}{shadecolor}{%
      \makebox[\linewidth-1em]{\usebox{\mysaveboxM}}%
    }%
  }%
  \usebox{\mysaveboxM}%
  \makebox[15pt][r]{%
    \makebox[\wd\mysaveboxM][l]{%
      \raisebox{\ht\mysaveboxM-0.5\ht\mysaveboxT+0.5\dp\mysaveboxT-0.5\fboxrule}{\usebox{\mysaveboxT}}%
    }%
  }%
}
  
  
\usepackage{chngcntr}


\counterwithin*{equation}{section}
\usepackage{scalerel}
%\newcommand\plus{\scaleobj{0.5}{+}}

% \titleformat{<command>}[<shape>]{<format>}{<label>}{<sep>}{<before-code>}[<after-code>]
\usepackage[small]{titlesec}
%\titlelabel{\thetitle.\;}
%\titleformat{\subsection}[runin]{\normalfont\bfseries}{}{1pt}{}[:]

%\titleformat*{\section}{\large\bfseries}
%\titleformat*{\subsection}{\normal\bfseries}
%\titleformat*{\subsubsection}{}
%\titleformat*{\paragraph}{\large\bfseries}
%\titleformat*{\subparagraph}{\large\bfseries}


\newcommand{\+}{%
	\raisebox{0.18ex}{\scaleobj{0.55}{+}}
%  \raisebox{\dimexpr(\fontcharht\font`X-\height+\depth)/2\relax}{\scaleobj{0.5}{+}}%
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}

\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}

%\newlength\myindent 
%\setlength\myindent{6em} 
%\newcommand\bindent{
%  \begingroup 
%  \setlength{\itemindent}{\myindent} 
%  \addtolength{\algorithmicindent}{\myindent} 
%}
%\newcommand\eindent{\endgroup} % closes a group

\usepackage{dsfont}

\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{example}{Example}[section]

\newcommand{\inv}{^{\raisebox{.2ex}{$\scriptscriptstyle-1$}}}
\newcommand\sbullet[1][.5]{\mathbin{\vcenter{\hbox{\scalebox{#1}{$\bullet$}}}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\bigzero}{\mbox{\normalfont\Large\bfseries 0}}
\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}

% Smooth Betti curves in dynamic settings \\ using persistent spectral theory
\title{\vspace{-2.0em} 
Matrix-free spectral relaxations of \\ persistence rank invariants
\vspace{-0.5em}}

\author{Matt Piekenbrock}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Using a duality result between persistence diagrams and persistence measures, we introduce a family of continuous relaxations of the persistent rank invariant for persistence modules indexed over the real line. 
Like the rank invariant, the relaxation satisfies inclusion-exclusion, is derived from simplicial boundary operators, and may be used to recover the persistence diagram. 
Unlike the rank invariant, the approximation enjoys a number of stability and continuity properties typically reserved for persistence diagrams, such as global Lipschitz continuity and differentiability over the positive semi-definite cone. 
%Key to achieving a $(1-\epsilon)$-approximation is the use of spectral L{\"o}wner decompositions 
%Our proposed $(1-\epsilon)$-approximation relies on uses nonconvex spectral functions  composition with the spectral decomposition relax boundary operators with L{\"o}wner operators. 
%equipping the space of cochains over $\mathbb{R}$ with an inner product. 
Fundamental to the family we propose is their characterization as spectral functions.
These operators manifest as combinatorial Laplacians, which encode rich geometric information in their non-harmonic spectra, providing several avenues for geometric data analysis. 
%Surprisingly, we find the relaxation may be efficiently standard persistence computation and it may be iteratively approximated in a "matrix-free" fashion. 
As exemplary applications, we study its utility in performing common tasks in topological data analysis and machine learning, such as hyper-parameter optimization and shape classification.

\end{abstract}

\section{Introduction}\label{sec:intro}
% History of persistence: shape descriptor, homology inference, stability => homology is useful for application domains
Persistent homology~\cite{} is the most widely deployed tool for data analysis and learning applications within the topological data analysis (TDA) community. 
Persistence-related pipelines typically follow a well-established pattern: given input data set $X$, construct a filtration $(K, f)$ from $X$ such that useful topological or geometric information may be profitably gleaned from its \emph{persistence diagram}~\cite{}.
%$$ \mathrm{dgm}(K, f) = \{ (i,j) \in \mathbb{R}^2 : i < j \}$$ 
Historically, persistence diagrams were first used as shape descriptors due to their effectiveness in tackling the \emph{homology inference problem}~\cite{}: by pairing simplices using homomorphisms between homology groups, diagrams demarcate topological features succinctly.
%given a finite data $X$ sampled from a topological space $\mathcal{X}$, can one infer the homology of $\mathcal{X}$ from $X$ with high confidence?
%One of the key properties of persistence is its multi-layered representation: just as homology categorifies the Euler characteristic via alternating sum of its Betti numbers, persistent homology categorifies persistent rank invariant through a similar alternating sum. 
The surprising and essential quality of persistence is that this pairing exists, is unique, and is stable under additive perturbations~\cite{cohen2005stability}.
Persistence has established the de facto connection between homology and the application frontier: whether for shape recognition, computer vision, metric learning, dimensionality reduction, or time series analysis, researchers have found ways of exploiting persistence diagrams.

Though theoretically sound, diagrams suffer from several practical issues: they are sensitive to strong outliers, far from injective, and difficult both to compute \emph{and} to compare. 
%Performing even basic statistical operations, such as averaging, has proven difficult under the standard matching metrics~\cite{}. 
Tackling some of these issues, practitioners have developed ways of equipping diagrams with additional structure by way of maps to function spaces (e.g. Hilbert spaces)---examples include persistence images~\cite{}, persistence landscapes~\cite{}, and template functions~\cite{}. 
These diagram vectorizations have proven useful for learning applications due to their stability and configurable (pseudo-)metric structure~\cite{}.
Along a separate thread of research addressing the issue of injectivity, Turner et al. propose the \emph{persistent homology transform} (PHT), which is a shape statistic that associates an embedded data set $X \subset \mathbb{R}^d$ with a collection of diagrams: 
\begin{align*}\label{eq:pht1}
	\mathrm{PHT}(X): S^{d-1} &\to \mathcal{D}^d \\
	v &\mapsto \left( \, \mathrm{dgm}_0(X, v), \mathrm{dgm}_1(X, v), \dots, \mathrm{dgm}_{d-1}(X, v) \, \right) \numberthis
\end{align*}
%rather than compute one diagram, compute many, each diagram representing a different perspective. 
The main result of~\cite{} shows this collection of diagrams is sufficient to reconstruct $X$, sparking both an inverse theory for persistence and a mathematical foundation for metric learning.  
%however, their scalability is limited as diagrams are prerequisite to their computation.
The scalability issue remains exacerbated, however, as extending the standard persistence computation to parameterized settings has proven non-trivial~\cite{}. %moves
Indeed, achieving rotation invariance in~\eqref{eq:pht1} alone involves a quadratic number of diagram comparisons. 
 
 % decrease usage of vectorization / increase generality 
 % enable learning applications including ones addressed by ones from previous paragraph
We seek to shift the computational paradigm while retaining the application potential: rather than first constructing diagrams and then endowing them with additional structure, we devise a spectral method that performs both steps, simultaneously and approximately. 
%Our strategy is natural in that it extracts persistence information ~\cite{}.
% include words: without constructing diagrams 
Using the duality between rank functions and diagrams, we not only avoid explicitly computing diagrams, but we in fact avoid using the reduction algorithm from~\cite{edelsbrunner2022computational} entirely.  
Our strategy is motivated by both a measure-theoretic perspective on $\mathbb{R}$-indexed persistence modules~\cite{chazal2016structure} and by a technical observation that suggests several computational advantages to working with the rank invariant directly (see section~\ref{sec:betti_derivation} for details). 
%?between the persistent Betti numbers and combinatorial Laplacian operators
As the vectorization we propose continuously interpolates between the rank invariant and a certain spectral operator, we elucidate a connection between persistence and other areas of applied mathematics, such as Tikhonov regularization, compressive sensing, and iterative subspace  methods.
Moreover, inspired by a relationship established between the persistent Betti numbers and combinatorial Laplacian operators~\cite{}, we show our vectorization able to harvest the rich geometric information such operators encode for tasks like shape classification and sparse optimization.  
%Suprsingly, among the many benefits of our approach is its iterative nature: 

%the multiplicity function is computable solely from (matrix) rank computations~\cite{zomorodian2004computing}. 

% TODO: save this, but put it somewhere else
%Indeed, one can show that~\eqref{eq:multiplicity} implies an algorithm can recover the diagram by computing at most $2n-1$ rank computations via a divide-and-conquer like approach on the index-persistence plane~\cite{chen2011output}. The duality between persistence diagrams and their corresponding rank functions suggests an alternative computational paradigm---distinct from the reduction family of algorithms---with which to approach persistence-related computations entirely. 


%most persistence-related research has concentrated on properties of diagram---notable exceptions including~\cite{cerri2013betti, chen2011output}---
%whereas 
%Mobius inversion reference
%We contend there are several properties of the rank invariant that justify its use and study, particularly in parameterized settings. 
%Moreover, such rank computations are \emph{localized} to subcomplexes of the filtration. 
%\subfile{intro_feature}
%\subfile{overview} % Contributions, Main result overview, etc. 
%\subsection{Related work}\label{sec:related_work}
%Zomorodian et al.~\cite{zomorodian2004computing} outline an algorithm to compute a basis for $Z_p(K_i) \cap B_p(K_j)$ via a sequence of boundary matrix reductions; their subsequent Theorem 5.1 reduces the complexity of computing PH groups with coefficients in any PID to that of computing homology groups. 
%However, the standard homology computations require $O(N^2)$ space and $O(N^3)$ time to compute, implying either of these approaches to computing the PBN computation exhibits same complexity as the full persistence computation.
% TODO: See seminar "Persistent Homology with Random Graph Laplacians"
% Iterative solution of linear systems in the 20th century
% Sketching Persistence Diagrams
\subsection*{Rank invariant relaxation}\label{sec:rank_invariant_summary}
So as to not leave the reader in suspense, we first outline the proposed relaxation informally. The rest of the paper discusses the theoretical and practical details. At a high level, the relaxation we propose constructs a vector-valued mapping over a subset $\mathcal{A} \subset \mathbb{R}^d$: 
$$ (X, \mathcal{R}, \epsilon) \mapsto \mathbb{R}^{\lvert \mathcal{R} \rvert}$$
where $X$ is the input a data set, $\epsilon$ a relaxation parameter, and $\mathcal{R}$ a \emph{sieve} which sifts through $\mathcal{A}$, summarizing the topological and geometric behavior observed by $(K, f_\alpha)$ for all $\alpha \in \mathcal{A}$ in the process.  The steps to produce this mapping are as follows:
\begin{enumerate}
	\item Let $K$ denote a fixed simplicial complex constructed from the data set $X$. Select a continuously-parameterized family of filtrations:
	$$(K, f_\alpha) = \{ \, f_\alpha : K \to \mathbb{R} \mid f_\alpha(\tau) \leq f_\alpha(\sigma) \;\, \forall \; \tau \subseteq \sigma \in K, \alpha \in \mathcal{A} \, \}$$
	Exemplary choices of $f_\alpha$ include filtrations geometrically realized from methods that themselves are have parameters (``hyper-parameters''), such as density filtrations~\cite{} or Rips filtrations over dynamic metric spaces~\cite{}.  
	\item Choose a \emph{sieve} $\mathcal{R} \subset \Delta_+$ that is decomposable to a disjoint set of rectangles:
	$$ \mathcal{R} = R_1 \cup R_2 \cup \dots \cup R_m,  \quad \quad \Delta_+ \triangleq \{ \, (i,j) \in \bar{\mathbb{R}}^2  \mid i < j \, \}$$ 
	This choice typically requires a priori knowledge and is application-dependent. In section~\ref{sec:applications} we give evidence random sampling may be sufficient for vectorization or exploratory purposes when $\mathcal{R}$ is unknown.
	
%	\item Choose rectangle $R = [i,j] \times [k,l] \subset \Delta_+$ in the upper-half plane $\Delta_+ = \{\}$
	\item Choose a homology dimension $p\geq 0$ and approximation parameter $\epsilon > 0$ representing how closely the relaxation should model the quantity: 
	\begin{equation}\label{eq:mu_alpha}
		\mu_p(\mathcal{R}_\alpha) \stackrel{\epsilon}{\approx} \{ \, \mathrm{card}(\mathcal{R} \, \cap \, \mathrm{dgm}(K, f_\alpha)) \mid \alpha \in \mathcal{A} \, \}
	\end{equation}

	\item Choose a Laplacian operator $\mathcal{L}(K, f_\alpha)$ and regularization scheme $\Phi$ to associate to $\mathcal{R}$: 
$$ \mathcal{L}: \mathcal{R} \times \Phi \; \to \; C_1(\mathcal{A}, \mathbb{R}^{\lvert \mathcal{A}\rvert})  $$
%	$$ L_{ij}^{\mathrm{up}}(K, f_\alpha) = \mathcal{D}_{i}^{1/2}( \alpha) \circ \partial \circ W_{j}( \alpha ) \circ \partial^T \circ \mathcal{D}_{i}^{1/2}( \alpha) $$
Informally, the spectra of $\mathcal{L}$ encodes the geometry and topology of the intersection pattern of $(K, f_\alpha)$, and $\Phi$ prioritizes how this information is reflected more in the map. 
	\item For each corner point $(i,j) \in \partial(\mathcal{R})$, restrict and project $\mathcal{L}$ onto a Krylov subspace: 
	$$
	\mathcal{K}_n(\mathcal{L}, v) \triangleq \mathrm{span}\{ v, \mathcal{L}v, \mathcal{L}^2 v, \dots, \mathcal{L}^{n-1}v \}
	$$
	for some initially chosen vector $v \in \mathrm{span}(\textbf{1})^\perp$. As we will show in section~\ref{sec:spectral_relax}, the eigenvalues of $T = \mathrm{proj}_{\mathcal{K}} \restr{\mathcal{L}}{\mathcal{K}}$ form the basis of the $\epsilon$-approximation of~\eqref{eq:mu_alpha}. 
	\item (Optional) Depending on the amount of memory available, store the largest $k \geq 0$ eigenvectors of $T$ at each corner point $(i,j) \in \partial(\mathcal{R})$ to accelerate the computation of (5) along similar $\alpha \in \mathcal{A}$ (see section~\ref{sec:lanczos_it} for details).
\end{enumerate}
The remaining steps of the relaxation depend on the application in mind. We capture these two generic application domains as follows: 
\begin{enumerate}[label=(\Alph*)]
	\item Construct an $\epsilon$-approximation of $\mu_p(\mathcal{R}_\alpha)$ over a family $\mathcal{A}$
	\item Differentiate an $\alpha$-parameterized filter $f_\alpha$ for a fixed sieve $\mathcal{R}$ over a family $\mathcal{A}$ % $\nabla f_\alpha (\mathcal{R}) \in \mathbb{R}^{\lvert \mathcal{A}\rvert}$
\end{enumerate}
\noindent
The duality between diagrams and rank functions suggests any application exploiting vectorized persistence information may benefit from our relaxation. 
Exemplary applications of (A) include: characterizing swarm and flocking behavior with Betti curves~\cite{}, topology-guided image denoising~\cite{}, detecting bifurcations in dynamical systems~\cite{}, generating metric invariants for shape classification and metric learning~\cite{},  and so on. 
Moreover, the differentiability of our relaxation enables learning applications to optimize persistence information. Exemplary applications of (B) include filtration optimization, incorporating topological priors into loss functions, and....
\\
\\
%\begin{tcolorbox}[title=Generic application \#1: Vectorizing persistence information]
%\begin{empheq}[box=\boxAppOne]{equation*}
%	\epsilon\text{-approximate } \mu_p(\mathcal{R}_\alpha) \text{ over a family } \mathcal{A}
%\end{empheq}
%\end{tcolorbox}
%\begin{tcolorbox}[title=Generic application \#2: Learning using persistence information]
%	Compute a gradient $\nabla f_\alpha (\mathcal{R}) \in \mathbb{R}^{\lvert \mathcal{A}\rvert}$ with which to optimize $\mu_p(\mathcal{R}_\alpha)$ over $\mathcal{A}$
%\end{tcolorbox}

%The fact that the proposed relaxation satisfies the same inclusion-exclusion formula obeyed by the rank invariant implies that we may recover a $(1-\epsilon)$-approximation of~\eqref{eq:mu_alpha} simply by obtaining $(1-\epsilon)$-approximations of $\mu_p$ at every corner point $(i,j) \in \partial(\mathcal{R})$. We defer discussion of the details of the computation to section~\ref{sec:computational_imp}.
\noindent 
\textbf{Contributions:}
Our primary contribution is an iterative $\epsilon$-approximation method for computing the \emph{persistent rank invariants}---$\mu_p^{\ast}$ and $\beta_p^\ast$---in essentially $O(m)$ memory and $O(mn)$ time, where $m, n$ are the number of $p+1, p$ simplices in the complex, respectively (see section~\ref{sec:computational_imp} for details). 
The approximation is spectral-based and is particularly efficient when executed on perturbed inputs, which we generically refer to as the \emph{parameterized setting}. 
When the accuracy parameter $\epsilon$ is made small enough, both invariants are recovered exactly. 
In deriving the approximation, we obtain families of continuous rank invariants which are Lipshitz continuous, stable under perturbations, and differentiable on the positive semi-definite cone. 
Unlike existing dynamic persistence algorithms, our approach is quite simple, requiring no complicated data structures or maintenance procedures to implement. 
%require explicitly filtered complexes $K$ to reside in memory .
The proposed relaxation is also \emph{matrix-free}, requiring only as much memory as is needed to enumerate simplices in the underlying complex $K$.
%We find the approximation to also be efficient in practice: the complexity reduction does not use Zigzag persistent homology~\cite{milosavljevic2011zigzag} nor does it rely on Strassen-like reductions to the matrix multiplication time $O(n^\omega)$.
Interestingly, our results also imply the existence of an efficient output-sensitive algorithm for computing persistence diagrams (via~\cite{chen2011output}) that requires as its only input  matrix-vector product operator $x \mapsto \partial x$, which we consider to be of independent interest. 


\subsection*{Duality between ranks and diagrams}\label{sec:duality_pbn_dgm}
%In this section, we describe the background theory that motivated the relaxation and contributions listed above, beginning with description of the duality between persistence diagrams and the rank invariant.  

% --- Counting measure multiplicity interpretation --- 
\subfile{counting_measure}

\subsection{Organization}
The paper is organized as follows. 
Section~\ref{sec:background_notation} introduces the notation and background theory on which the rest of the paper depends, including an illuminating derivation of a relatively little-known expression of the persistent rank invariant $\beta_p(K)$ in section~\ref{sec:betti_derivation}, which is a key technical ingredient for much of the work presented here.
Section~\ref{sec:spectral_relax} contains the main results: a family of spectral relaxations of the rank invariants, their properties and interpretations, and their connections to other areas of mathematics. 
Section~\ref{sec:computational_imp} compares and contrasts the computational differences between the proposed relaxation and the traditional persistence computation.
Section~\ref{sec:applications} demonstrates some of the prototypical use cases mentioned in section~\ref{sec:intro}.
Some technical details, illustrative examples, and pseudocode are relegated to the appendix for readability.

% ---- Background & Notation ---- 
\section{Notation \& Background}\label{sec:background_notation}
\subfile{background}

\subsection{Motivating Derivation}\label{sec:betti_derivation}
%In what follows, we briefly outline the computation of the 
To motivate this effort, we begin by deriving a lesser known
%\footnote{To the authors knowledge, only two works have } 
expression of the persistent Betti number, beginning with a bit of notation. 
% For the moment, we omit the subscript $t \in \mathrm{T}$ and focus on a fixed instance of time. 
Let $B_p(K_\bullet) \subseteq Z_p(K_\bullet) \subseteq C_p(K_\bullet)$ denote the $p$-th boundary, cycle, and chain groups of a given filtration $K_\bullet$, respectively, and let $\partial_p : C_p( K_{\bullet}) \to C_{p-1}(K_{\bullet})$ denote the $p$-th boundary operator. 
We use $\partial$ to denote the $N \times N$ filtration boundary matrix with respect to an ordered basis $(\sigma_i)_{1 \leq i \leq N}$ induced by $K_\bullet$, given by:
\begin{equation}\label{eq:boundary_matrix}
	\partial[i,j] = \begin{cases}
		(-1)^{s_{ij}} & \sigma_i \in \partial[\sigma_j], \; \text{where } s_{ij} = \mathrm{sgn}([\sigma_i], \partial [\sigma_j])  \\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
Since $\partial_p$ is completely characterized by the oriented $p$ and $p-1$ simplices of $K_\bullet$,  one may obtain a matrix representative of $\partial_p$ by setting all columns corresponding to simplices of dimension $q \neq p$ to $0$. 
With a small abuse in notation, we will freely use both $\partial$ and $\partial_p$ to refer to both the boundary operators themselves and their matrix representatives.


Given a filtration $K_\bullet$ of size $N = \lvert K_\bullet \rvert$, its $p$-th persistent Betti number $\beta_p^{i,j}$ at index $(i,j) \in \Delta_+^N$, where $\Delta_+^N  := \{ \, (i,j) \in [N] \times [N] : i < j \, \}$, is defined as as the dimension of the quotient group $H_p^{i,j} = Z_p(K_i) / B_p(K_j)$: 
\begin{align*} \label{eq:pbn}
	\centering
	\beta_p^{i,j} &= \mathrm{dim} \left ( Z_p(K_i) / B_p(K_j) \right ) \\
	& = \mathrm{dim} \left( Z_p(K_i) / (Z_p(K_i) \cap B_p(K_j) \right) \\
	& \numberthis = \mathrm{dim} \left( Z_p(K_i) \right) - \mathrm{dim}\left( Z_p(K_i) \cap B_p(K_j) \right ) 
\end{align*}
By definition, the boundary and cycle groups $B_p(K_j)$ and $Z_p(K_i)$ are subspaces of the operators $\partial_p(K_i)$ and $\partial_{p+1}(K_j)$, yielding: 
\begin{equation}\label{eq:pbn_simple}
	\beta_p^{i,j} = \mathrm{dim}\big( \; \mathrm{Ker}(\partial_p(K_i)) \; \big) - \mathrm{dim}\big( \; \mathrm{Ker}(\partial_p(K_i)) \cap \mathrm{Im}(\partial_{p+1}(K_j)) \; \big) 
\end{equation}
%The dimension of the boundary group $B_{p-1}(K_i)$ may be directly inferred from the rank of $\partial_p^{i}$, and the dimension of $C_p(K_i)$ is simply the number of $p$-simplices with filtration values $f(\sigma) \leq i$. 
Now, consider computing $\beta_p^{i,j}$ via~\eqref{eq:pbn_simple} from matrix representatives $\partial_p \in \mathbb{F}^{n \times m}$.
%$\mathrm{dim}\big(\mathrm{Ker}(\partial_p(K_i))\big) = \mathrm{nullity}(\partial_p(K_i))$
Since the nullity of an operator may be reduced to a rank computation, the complexity of first term may be reduced to the complexity of computing the rank of a (sparse) $n \times m$ matrix. 
In contrast, the second term---the persistence part---typically requires finding a basis in intersection of the two subspaces via either column reductions or projection-based techniques. In general,  direct methods that accomplish this require $\Omega(N^3)$ time and $\Omega(N^2)$ memory~\cite{golub2013matrix}.

% Golub and Van Loan 
%Alternatively, both iterative and explicit projector-based methods~\cite{ben2015projectors} may also for the intersection term in~\eqref{eq:pbn}, though these projectors may still be expensive to compute. 
% TODO: elaborate more, give von neumann inequality 
%In what follows, we outline a different approach to computing~\eqref{eq:pbn} that we argue is not only simpler and computationally attractive, but also amenable to acceleration in dynamic settings. 
To illustrate an alternative approach, we will require a key property of persistence. The structure theorem from~\cite{zomorodian2004computing} shows that 1-parameter persistence modules can be decomposed in an \emph{essentially unique} way into indecomposables.
\begin{figure}
\centering
%	\includegraphics[width=0.45\textwidth]{mult_both}
	\includegraphics[width=0.85\textwidth]{betti_add}
	\caption{From left to right, $\beta_p^{i,j}$ counts the number of points (3) in upper left-corner of $\mathrm{dgm}_p(K_\bullet)$, where $i,j \in \Delta_+^N$; the same $\beta_p^{i,j}$  with $i,j \in \Delta_+$; the additivity of $\beta_p^{\ast}$ implies $\mu_p^{R}$ over a box $R=[i,j] \times [k,l]$ is given as the sum of four PBNs; generalization of~\ref{eq:uniq_pivot}---in this case $\mu_p^R = 4 - 1 - 0 + 0 = 3$ counts pivot entries in the reduced matrix $R = \partial V$.}
	%$r_R(i,j) = 3 - 2 + 1 - 2 = 0$ yields whether the entry $R[i,j]$ is non-zero.}
	\label{fig:mult}
\end{figure}
 One consequence of this result is the Pairing Uniqueness Lemma~\cite{edelsbrunner2000topological}, which asserts that if $R = \partial V$ decomposes the boundary matrix $\partial$ to a \emph{reduced} matrix $R \in \mathbb{R}^{m \times n}$ using left-to-right column operations, then:
%whose lowest non-zero entries satisfy $\mathrm{low}_R(i) \neq \mathrm{low}_R(j)$ whenever both $\mathrm{col}_i(R) \neq 0$ and $\mathrm{col}_j(R) \neq 0$, then:  
\begin{equation}\label{eq:uniq_pivot}
R[i,j] \neq 0 \Leftrightarrow \mathrm{rank}(R^{i,j}) - \mathrm{rank}(R^{i\texttt{+}1,j}) + \mathrm{rank}(R^{i\texttt{+}1,j\text{-}1}) - \mathrm{rank}(R^{i,j\text{-}1}) \neq 0 
\end{equation}
%where, for any $1 \leq i < j \leq N$, the quantity $r_A(i,j)$ is defined as:
%\begin{equation}
%	r_R(i,j) = \mathrm{rank}(R^{i,j}) - \mathrm{rank}(R^{i\texttt{+}1,j}) + \mathrm{rank}(R^{i\texttt{+}1,j\text{-}1}) - \mathrm{rank}(R^{i,j\text{-}1})
%\end{equation}
where $R^{i, j}$ denotes the lower-left submatrix defined by the first $j$ columns and the last $m - i + 1$ rows (rows $i$ through $m$, inclusive). 
In other words, the existence of non-zero ``pivot'' entries in $R$ may be inferred entirely from the ranks of certain submatrices of $R$.   
As we will use this fact frequently in this paper, we record it formally with a lemma. 
\begin{lemma}\label{lemma:rank}
Given filtration $K_\bullet$ of size $N = \lvert K \rvert$, let $R = \partial V$ denote the decomposition of the filtered boundary matrix $\partial \in \mathbb{F}^{N \times N}$ given in equation~\eqref{eq:boundary_matrix}. Then, for any pair $(i,j)$ satisfying $1 \leq i < j \leq N$, we have:
	\begin{equation}\label{eq:lower_left_rank}
		\mathrm{rank}(R^{i,j}) = \mathrm{rank}(\partial^{i, j})
	\end{equation}
Equivalently, all lower-left submatrices of $\partial$ have the same rank as their corresponding submatrices in $R$.
\end{lemma} % 
\noindent
%\noindent \textbf{Remark: } Though Lemma~\ref{lemma:rank} is defined over the full boundary matrix $\partial$, there is no loss in generality in its restriction to $p$-dimensional homology exclusively, for any fixed $p \geq 0$. To see this, note that since the reduction algorithm only adds $p$-chains to $p$-chains. Hence, if we set all columns corresponding to simplices of dimension $q \neq p$ to $0$ in the $m \times m$ boundary matrix $\partial$, then $\partial$ recovers the $p$-th boundary operator $\partial_p : C_p(K_\bullet) \to C_{p-1}(K_\bullet)$. 
%In what follows, we will use $\partial_p$ and $R_p$ to refer to matrices of $\partial$ and $R$ whose $q$-chains are set to $0$, for $q \neq p $. 
An explicit proof of this can be found in~\cite{dey2022computational}, though it was also noted in passing by Edelsbrunner~\cite{edelsbrunner2000topological}. It can be shown by combining the Pairing Uniqueness Lemma with the fact that $R$ is obtained from $\partial$ via left-to-right column operations, which preserves the ranks of every such submatrix.
Lemma~\ref{lemma:rank} is remarkable in that although $R$ is not unique, its non-zero pivots are, and these pivots \emph{define} the persistence diagram. 
This seems like a minor observation at first, however it is far more general, as recently noted by Bauer et al.~\cite{bauer2022keeping}:
\begin{proposition}[Bauer et al.~\cite{bauer2022keeping}]
	Any persistence algorithm which preserves the ranks of the submatrices $\partial^{i,j}(K_\bullet)$ for all $i,j \in [N]$ is a valid persistence algorithm. 
\end{proposition}
\noindent A lesser-known fact that exploits Lemma~\ref{lemma:rank}---also pointed out in~\cite{dey2022computational}---is that~\eqref{eq:lower_left_rank} enables the PBN to be written as a sum of ranks of submatrices of $\partial_p$ and $\partial_{p+1}$.
\begin{proposition}[Dey \& Wang~\cite{dey2022computational}]\label{prop:rank_reduction}
Given a fixed $p \geq 0$, a filtration $K_\bullet$ of size $N = \lvert K_\bullet \rvert$, and any pair $(i,j) \in \Delta_+^N$, the persistent Betti number $\beta_p^{i,j}(K_\bullet)$ at $(i,j)$ is given by:
	\begin{equation}\label{eq:betti_four}
	% \mathrm{rank}(I_p^{1,i})
	\beta_p^{i,j}(K_\bullet) = \lvert K_i^p \rvert - \mathrm{rank}(\partial_p^{1,i}) - \mathrm{rank}(\partial_{p\+1 }^{1,j}) + \mathrm{rank}(\partial_{p\+1}^{i \+ 1, j} )
	%\mathrm{rank}(\partial_{p+1}^{i+1,\ast} \otimes\partial_{p+1}^{\ast,j} )
	\end{equation}
%where $I_p^{1,i}$ denotes the first $i$ columns of the $n \times n$ identity matrix.
\end{proposition}
\noindent For completeness, we give our own detailed proof of Proposition~\ref{prop:rank_reduction} in the appendix. 
By combining Proposition~\ref{prop:rank_reduction} with~\eqref{eq:multiplicity}, we recover a submatrix-rank-based $p$-th multiplicity function $\mu_p^{R}(\cdot)$, which to the authors knowledge was first pointed out by Chen \& Kerber~\cite{chen2011output}: 
\begin{proposition}[Chen \& Kerber~\cite{chen2011output}]\label{prop:mu_reduction}
Given a fixed $p \geq 0$, a filtration $K_\bullet = \{K_i\}_{i\in [N]}$ of size $N = \lvert K \rvert$, and a $R = [i,j] \times [k,l]$ whose indices $(i,j,k,l)$ satisfy $0 \leq i < j \leq k < l \leq N$, the $p$-th multiplicity $\mu_p^{R}$ of $K_\bullet$ is given by:
	\begin{equation}\label{eq:mu_four}
	\mu_p^{R}(K_\bullet) = \mathrm{rank}(\partial_{p\+1}^{j \+ 1, k})  - \mathrm{rank}(\partial_{p\+1}^{i \+ 1, k})  - \mathrm{rank}(\partial_{p\+1}^{j \+ 1, l}) + \mathrm{rank}(\partial_{p\+1}^{i \+ 1, l}) 
	%\mathrm{rank}(\partial_{p+1}^{i+1,\ast} \otimes\partial_{p+1}^{\ast,j} )
	\end{equation}
\end{proposition}
\noindent For more geometric intuition of these propositions, see Figure~\ref{fig:mult}. Note the differences between these two quantities: whereas $\beta_p^{i,j}$ can capture points on the diagram with unbounded persistence (``essential'' classes~\cite{edelsbrunner2000topological}), the multiplicity function $\mu_p^{R}$ cannot count these classes unless the filtration is \emph{coned}~\cite{chen2011output}, an operation which doubles the size of the complex.

Compared to the classical reduction methods~\cite{edelsbrunner2022computational, zomorodian2004computing}, the primary advantage of the rank-based expressions from~\eqref{eq:betti_four}-\eqref{eq:mu_four} is that they imply the complexity of obtaining either $\beta_p^{i,j}(K_\bullet)$ or $\mu_p^{R}(K_\bullet)$ may be reduced to the complexity of computing the rank of a set of submatrices of $\partial$.
This fact actually motivated the rank-based persistence algorithm from Chen et al~\cite{chen2011output}.
Moreover, observe that the constitutive terms in these expressions are \emph{unfactored} boundary (sub)matrices---thus, if we can represent the operation $x \mapsto \partial x$ without actually constructing $\partial$ in memory, we may use iterative Krylov or subspace acceleration methods~\cite{golub2013matrix, parlett1994we} for their computation. 
%Moreover, the measure-theoretic counter-parts~\eqref{eq:pbn_cont} and~\eqref{eq:measure} suggests we may re-use  
This line of thought suggests other algebraic properties of the rank function---such as e.g. invariance under permutations and invariance under adjoint multiplication---may simplify these rank-based expression even further.
%By working with persistence modules indexed with real-valued coefficients, we naturally recover the measure-theoretic perspective from~\eqref{eq:measure}.
We dedicate the rest of the paper to exploring these questions. 

\begin{remark}
% Change index set
The notation used thus far employed integer indices $(\,i, j\,) \in \Delta_+^N$ to describe persistent quantities over a filtration $K_\bullet = (K, f)$ of size $\lvert K \rvert = N$, which is equivalent to indexing $K$ with a filter function $f : K \to I$ defined on the index set $I = [N]$.
It is more common in practice to define persistence of a persistent-pair $(\sigma_i, \sigma_j) \in \mathrm{dgm}(K_\bullet)$ via $f(\sigma_j) - f(\tau_i)$, rather than as $j - i$, especially when $f$ is  geometric in nature.
%For example, when $f : K \to \mathbb{R}$ satisfies $f(\sigma') \leq f(\sigma)$ for every $\sigma' \subseteq \sigma$, the subcomplexes $K_i = f^{-1}(-\infty, \hat\imath \,]$ of $K$ represent sublevel sets $f^{-1}(-\infty, \hat\imath \,]$ for every $\hat\imath \in \mathbb{R}$. 
In this setting, each pair $(\sigma_i, \sigma_j) \in \mathrm{dgm}(K_\bullet)$ is typically represented as the point $(\, \hat\imath, \hat\jmath \, )$ where $\hat\imath = f(\sigma_i)$ and $\hat\jmath = f(\sigma_j)$.
% In theory, any function satisfying $f(\tau) \subseteq f(\sigma)$ for every face/coface pair $(\tau, \sigma)$ yields a well defined persistence diagram. 
%Unless it is clear from the context, we alter our notation by re-defining $\beta_p^{\hat\imath,\hat\jmath}$ using pairs $(\,\hat\imath,\hat\jmath\,) \in \Delta_{+}$ from the upper-half plane $\Delta_{+} = \{ \, (x,y) \in \mathbb{R}^2 : y > x \, \} $ for the remainder of the paper.
%Since this is equivalent to reindexing a given filter $f : K \to \mathbb{R}$ over the index set $[N]$, 
%Since persistence diagrams are isomorphic under monotone transformations of the index set, 
Again exploiting the inclusion-exclusion property known for real-valued persistence modules~\cite{chazal2016structure}, we from now on will use the notation $(\,i, j\,)$ to denote pairs $(\,i, j\,) \in \Delta_{+}$ from the upper-half plane $\Delta_{+} = \{ \, (x,y) \in \mathbb{R}^2 : y > x \, \} $.

\end{remark}

%In contrast to the persistence diagram, note that both of counting invariants are neither stable nor smooth as they are integer-valued quantities.
%However, as~\eqref{eq:mu_four} makes clear, the restriction to sub-matrices is akin to computing $R$-parameterized multiplicities over the index-persistence plane $\Delta_+^N$. 
%Moreover, suppose we: 
%\begin{enumerate}
%	\item Restrict ourselves to $H_p(K; \mathbb{R})$
%	\item Index $K_\bullet$ over the (real) upper-half plane $\Delta_+$
%	\item Parameterize the chains $C_p(K_\bullet; \mathbb{R})$ with real-valued entries
%\end{enumerate}


%We exploit various properties of the rank function to 
%Using various properties of the rank function, this accelerate the computation of both invariants. 
%Combined with various properties of the rank function, we exploit this aspect of to accelerate the computation of both invariants in~\eqref{eq:betti_four} and~\eqref{eq:mu_four}. 

%To get some intuition on what the structure and size of these matrices, we include a picture of each of the terms in Equation~\eqref{eq:betti_four}. 
%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.70\textwidth]{four_matrices}
%	\caption{The four matrices whose ranks yield $\beta_p^{i,j}$ in the same order as given in~\eqref{eq:betti_four}. Each solid portion represents (sparse) blocks of non-zero entries, while each white portion is zero. Observe $\partial_{p+1}^{i+1, j} \subseteq \partial_{p+1}^{1,j}$ can be obtained by intersecting the non-zero entries of $\partial_{p+1}^{1,j}$ with the non-zero entries in the complement of $\partial_p^{1,i}$.  }
%\end{figure}
%Boundary matrices are sparse and highly structured: given $K_\bullet$ with $m$ simplices $\sigma_1, \sigma_2, \dots, \sigma_m$ constructed from a $p$-dimensional complex $K$, its full boundary matrix $\partial$ is upper-triangular and has a storage complexity of: 
%\begin{equation}
%	\mathrm{nnz}(\partial) \sim O(m \log m)
%\end{equation} 
%To see this, note that since a $p$-simplex has $2^{p+1} - 1$ faces, we have $p \leq \log(m + 1) - 1$. Moreover, since each column of $\partial_p^\ast$ contains exactly $p+1$ non-zero entries, $\mathrm{nnz}(\partial) \sim O((p+1)m)$.


%\begin{proposition}\label{prop:mu_query}
%For any fixed $p \geq 0$, let $\partial_p$ denote the $p$-dimensional boundary matrices of filtration $K_\bullet$ of size $n = \lvert K^{(p)} \rvert$, $m = \lvert K^{(p\+1)} \rvert$.
%Let $R = [i,j] \times [k,l]$ denote a fixed rectangle in $\mathbb{R}^2$ satisfying $a < b \leq c < d$. The multiplicity $\mu_p^R$ of this box is given by:
%	\begin{equation}\label{eq:mu_four}
%	\mu_p^{R} =  \mathrm{rank}(\partial_{p\+1}^{j} ) - \mathrm{rank}(\partial_{p\+1}^{i \+ 1, j} )
%	%\mathrm{rank}(\partial_{p+1}^{i+1,\ast} \otimes\partial_{p+1}^{\ast,j} )
%	\end{equation}
%where $I_p^{1,i}$ denotes the first $i$ columns of the $n \times n$ identity matrix.
%\end{proposition}
%\begin{corollary}
%	Given a filtration $K_\bullet$ of size $n = \lvert K_{i}^{(p)} \rvert$ and $m = \lvert K_{j}^{(p+1)} \rvert$ and two indices $i \in [n]$, $j \in [m]$, computing $\beta_p^{i,j}$ can be done in time and storage complexity $O(\max \{\, R_\partial(n, i, p), \, R_\partial(m, j, p+1) \,\})$ where $R_\partial(a,b,c)$ is the complexity of computing the rank of a $c$-dimensional $a\times b$ boundary matrix with $b\cdot (c+1)$ non-zero $\mathbb{F}$ entries. 
%	%with $n < m$
%\end{corollary} 

% --- Generic Rank approximation via eigenvalues --- 
\section{Spectral relaxation and its implications}\label{sec:spectral_relax}

% This subsection shows the benefits of rank(P^T A P) = rank(A)
\subsection{Parameterized boundary operators}\label{sec:param_boundary}
%As stated in section~\ref{sec:intro}, we are concerned with the computation of certain topological invariants in \emph{parameterized} settings, i.e. settings where the input data---geometrically realized as simplicial complexes---is thought to be generated from a parameterized family. Such families naturally arise in many applications, e.g. in Rips filtrations parameterized by dynamic metric spaces~\cite{} or in multi-parameter persistence settings~\cite{}.  
%Though the boundary matrices in~\eqref{eq:betti_four} are given in filtration order to preserve the inclusion relations between simplices in $K_\bullet$, the permutation invariance of the rank function suggests $\partial$ need not be ordered at all to be evaluated---so long as each constitutive term has the same non-zero pattern as its filtration-ordered counterpart, their ranks will be identical.
%% Another approach to tracking the PBNs in parameterized setting is to maintain a persistence diagram through the vineyards algorithm~\cite{cohen2006vines}. This, however, requires $O(m^2)$ memory and an $\approx O(m^2)$ preprocessing procedure to detect changes in the filtration order\footnote{The bound $O(m^2)$ assumes the homotopy changes each filtration value monotonically throughout the homotopy. Otherwise the number of order changes is clearly unbounded.} to simulate persistence across a homotopy, the canonical continuous parameterized setting. In contrast, as we show below, PBNs need no such preprocessing procedure and may be computed in effectively $O(m)$ memory, even in parameterized settings. 
%Recall that the boundary operator $\partial_p$ for a filtration pair $(K_{\bullet}, f)$ of size $m = \lvert K \rvert$ is represented by an ordered $(m \times m)$ boundary matrix $\partial_p$ whose columns and rows corresponding to $q$-simplices and $(q-1)$-simplices are zero, for $q \neq p$.
%After orienting $K$ in an arbitrary but consistent way, the entries of $\partial_p$ have the form: 
%\begin{equation}\label{eq:matrix_pchain}
%	\partial_p[k, l] = \begin{cases} 
%	c(\sigma_j)  & \text{if } \sigma_l \in \partial_p(\sigma_k) \\
%	0 & \text{otherwise}
%   \end{cases}
%\end{equation}
%where $c(\sigma_\ast) \in \mathbb{F}$ is an arbitrary constant satisfying $c(\sigma) = -c(\sigma')$ if $\sigma$ and $\sigma'$ are opposite orientations of the same simplex, typically set to $\pm 1$. In what follows, we assume a fixed orientation is given on $K$, and write $\pm c(\sigma)$ to indicate the sign of $c(\sigma)$ depends on the orientation of $\sigma$. Note that, for the reduction algorithm, the ordering of $\partial_p$ must respect the facet poset of $K_\bullet$: if $\tau, \sigma \in K_\bullet^{(p)}$ and $f(\tau) \leq f(\sigma)$, then the chain $\partial_p(\tau)$ must appear before $\partial_p(\sigma)$ in $\partial_p$. %such that the elementary left-to-right matrix operations. 
%such that $\mathrm{range}(f) = \mathbb{R}$, and it is often more informative to work with the pairs $(f(\tau_i), f(\sigma_j))$ stemming from the persistent-pairs $()$ persistence diagram 
% A common scenario is where $K_\bullet$ is obtained via a Rips filtration constructed over a metric space $(X, d_X)$. In this case $f : \mathcal{P}(X) \to \mathbb{R}_+$ is given as the diameter $f(\sigma) = \max_{x,x' \in \sigma} d_X(x,x')$. 
% mention DMSs 
%In many applications it is of interest to study such filter function in parameterized settings, e.g. given some set of parameters $\mathcal{H}$, the goal is to understand a given topological invariant at many  parameters $h \in \mathcal{H}$, i.e. treat $f : \mathcal{P}(X) \times \mathcal{H} \to \mathbb{R}_+$. We include several example in section~\ref{}.
%To see the utility of this fact, consider a filtration $(K_\bullet, f)$ equipped with a \emph{parameterized} filter function $f : \mathcal{A} \times K \to \mathbb{R}$ with respect to some set $\mathcal{A}$.
In typical dynamic persistence settings (e.g.~\cite{cohen2006vines}), the boundary matrix $\partial(K_\bullet)$ must maintain a simplexwise filtration order to preserve the inclusion structure of $(K_\bullet, f_\alpha)$ (w.r.t $f_\alpha$).
In contrast, the rank function is permutation invariant for any $X \in \mathbb{R}^{n \times n}$ and permutation $P$: 
$$ \mathrm{rank}(X) = \mathrm{rank}(P^T X  P) $$
%There are computational advantages to the lesser known rank invariant expressions~\eqref{eq:betti_four} and~\eqref{eq:mu_four} due to the many invariances of the rank function. One such property is permutation invariance: given any $A \in \mathbb{R}^{n \times n}$, it is easy to see that $\mathrm{rank}(A) = \mathrm{rank}(P^T A P)$ for any permutation matrix $P$.  
% In contrast, the permutation invariance of the rank function suggests $\partial$
This suggests rank invariant computations need not maintain this order---as long as each constitutive term has the same non-zero pattern as its filtration-ordered counterpart, their ranks will be identical.

%Permutation invariance is a useful property for computation in parameterized setting. 
We now show how to adapt the rank  the expressions from~\eqref{eq:betti_four} and~\eqref{eq:mu_four} to exploit this permutation invariance, beginning with a definition.
\begin{definition}[Parameterized $\partial_p$]\label{def:time_boundary_matrix}
Let $(K, f_\alpha)$ denote parameterized family of filtrations of a simplicial complex of size $\lvert K^p \rvert = n$. Fix an arbitrary linear extension $(K, \preceq^\ast)$ of the face poset of $K$. Define the $\mathcal{A}$-\emph{parameterized} \emph{boundary matrix} $\partial_p(\alpha)$ of $(K, f_\alpha)$ as the $n \times n$ matrix ordered by $\preceq^\ast$ for all $\alpha \in \mathcal{A}$ whose entries $(k,l)$ satisfy:
\begin{equation}\label{eq:param_boundary_matrix}
	\partial_p(\alpha)[k,l] = \begin{cases}
	% \pm \, S_{i,j}(\sigma_k, \sigma_l) & \text{if } \sigma_k \in \partial_p(\sigma_l) \\
s_{kl} \cdot f_\alpha(\sigma_k) \cdot f_\alpha(\sigma_l) & \text{if } \sigma_k \in \partial_p(\sigma_l)\\
%	s_{kl} \cdot (\bar{S}_{i} \circ f_h)(\sigma_k)^+ \cdot (S_{j} \circ f_h)(\sigma_l) & \text{if } \sigma_k \in \partial_p(\sigma_l)\\
	%, \text{ where } \epsilon = f(\sigma_l) \\
	0 & \text{otherwise}
\end{cases}
\end{equation}
%where $S_{i} : \mathbb{R} \to \{0, 1\}$ is a \emph{step} function satisfying $S_i(x) = 0$ if $x > i$ and $1$ otherwise, $\bar{S}_{i} = 1 - S_i$, $f_h(\sigma) = f(\sigma, h)$, 
where $s_{kl} = \mathrm{sgn}([\sigma_k], \partial [\sigma_l])$ is the sign of the oriented face $[\sigma_k]$ in $\partial[\sigma_l]$.
% and the quantity $x^{+} = x^{-1}$ if $x > 0$ and $0$ otherwise.
\end{definition}
\noindent
Observe that (1) the non-zero entries of the boundary operator from definition~\ref{eq:param_boundary_matrix} vary continuously in $f_\alpha$ and (2) $\partial_p(\alpha)$ decouples into a product of diagonal matrices $D_\ast(f_\alpha)$: 
	\begin{equation}\label{eq:decouple}
		\partial_p(\alpha) = D_p(f_\alpha) \circ \partial_p \circ D_{p+1}(f_\alpha) 
	\end{equation}
	where the non-zero entries of $D_p(f_\alpha)$ and  $D_{p+1}(f_\alpha)$ depend on restrictions of $f_\alpha$ to $K^p$ and $K^{p+1}$, respectively.
We refer to the fixed inner matrix $\partial_p \in \{-1, 0, 1\}^{n \times m}$ as the \emph{sign pattern matrix}.
Exploiting this decoupling, we can rewrite any term of the form from~\eqref{eq:lower_left_rank} as:
\begin{equation}\label{eq:rank_equiv_param}
	\mathrm{rank}(\, \partial_p^{i,j}(K_\bullet) \, ) = \mathrm{rank}(\, \partial_p^{i,j}(\alpha) \, ) \triangleq \mathrm{rank}\!\left( \, D_p(\bar{S}_i \circ f_\alpha) \circ \partial_p(K) \circ D_{p+1}(S_j \circ f_\alpha) \, \right)
\end{equation}
where $\bar{S}, S : \mathbb{R} \to \{0,1\}$ are up and down \emph{step functions}, respectively, given by: 
\begin{equation}\label{eq:step_functions}
	\bar{S}_i(x) = \begin{cases} 1 & \text{if } x > i \\ 0 & \text{otherwise}\end{cases}, \quad \quad S_j(x) = \begin{cases} 1 & \text{if } x \leq j \\ 0 & \text{otherwise}\end{cases}
\end{equation}
Note in~\eqref{eq:rank_equiv_param} we write $\partial_p^\ast(K_\bullet)$ and $\partial_p^\ast(K)$ to make the distinction that the former is explicitly filtered in the traditional simplexwise manner by $f_\ast$, whereas the latter is ordered arbitrarily according to $\preceq^\ast$. 
Since these operators have the same rank, we may replace the constitutive terms in~\eqref{eq:betti_four} and~\eqref{eq:mu_four} appropriately using their parameterized versions from definition~\ref{def:time_boundary_matrix} to obtaining expressions that are permutation invariant. 

%Computationally, note that the element-wise definition of $\hat{\partial}_p^{i,j}$ from~\eqref{eq:param_boundary_matrix} corresponds to the product: 
%\begin{equation}
%	\hat{\partial}_p^{i,j}(h) := V_p^i(h)^+ \circ \partial_p \circ W_{p+1}^j(h)
%\end{equation}
%where $W_p^x(h)$ denote an $\mathcal{H}$-parameterized diagonal ``weight'' matrix $\mathrm{diag}(\{\, g_h(\sigma_i) \, \})_{i=1}^{N}$ obtained by setting $g_h(\sigma) = (S_x \circ f_h)(\sigma)$ if $\mathrm{dim}(\sigma) = p$ for all $\sigma \in K$ and $g_h(\sigma) = 0$ otherwise. The definition of $V_p^x$ synonymous, where $\tilde{g}_h(\sigma) = 1 - (S_x \circ f_h)(\sigma)$ and $A^+$ denotes the pseudo-inverse of $A$. Thus, definition~\ref{def:time_boundary_matrix} corresponds to a \emph{fixed} $p$-th boundary matrix $\partial_p$ whose rows and columns are weighted by $V_p^i(h)$ and $W_{p+1}^j(h)$, respectively. As a result, there is no need to explicitly maintain an ordered basis $(\sigma_i)_{1 \leq i \leq N}$ of simplices as a function of $\mathcal{H}$ to obtain persistence-related information, as is done by the vineyards algorithm~\cite{cohen2006vines}---it is enough to simply preserve the correct sign pattern.

We summarize the observations above with a proposition. To simplify the notation, we write $A^{x} = A^{\ast,x}$ to denote the submatrix including all rows of $A$ and all columns of $A$ up to $x$, and we write $q = p + 1$. 
Without loss in generality, we also assume the orientation of the simplices induced by $(K, \preceq^\ast)$ is inherited from the order on the vertex set $V$. 
%We also write $r(A) = \mathrm{rank}(A)$.
% TODO: ask Jose about this
%We also use $\mathcal{K}_f$ to denote the space of all filtration pairs $(K, f)$.
\begin{proposition}\label{prop:mu_betti_1}
	Let $\delta > 0$ denote the number from~\eqref{eq:pbn_cont} satisfying $i + \delta < j - \delta$. Given $(K, f_\alpha)$ and any rectangle $R = [i,j] \times [k,l] \subset \Delta_{+}$ satisfying $i < j \leq k < l$, define the $\mathcal{A}$-parameterized invariants $\beta_p^{i,j} : \mathcal{A} \times K \to \mathbb{N}$ and $\mu_p^{R} : \mathcal{A} \times K \to \mathbb{N}$ by: 
	\begin{align*}\label{eq:pbn_parameterized}
		\beta_p^{i,j}(\alpha) = \lvert K_i^{p}(\alpha) \rvert - \mathrm{rank}\big(\,\partial_{p}^{i}(\alpha)\,\big) - \mathrm{rank}\big(\,\partial_{q}^{j}(\alpha)\,\big) + \mathrm{rank}\big(\,\partial_{q}^{i \+ \delta, j}(\alpha)\,\big) \numberthis
	\end{align*}
	\vspace{-2.1em}
	\begin{align*}\label{eq:mu_parameterized}
	\mu_p^{R}(\alpha) = \mathrm{rank}\big(\, \partial_{q}^{j \+ \delta, k}(\alpha)\,\big) - \mathrm{rank}\big(\, \partial_{q}^{i \+ \delta, k}(\alpha)\,\big) - \mathrm{rank}\big(\, \partial_{q}^{j \+ \delta, l}(\alpha)\,\big) + \mathrm{rank}\big(\, \partial_{q}^{i \+ \delta, l}(\alpha)\,\big) \numberthis
	\end{align*}
	yield the correct quantities $\mu_p(R) = $
	$\mathrm{card} \left( \, \restr{\mathrm{dgm}_p(f_\alpha)}{R} \, \right)$ and 
	$\beta_p^{i,j} = \mathrm{dim}(H_p^{i,j}(K))$ from~\eqref{eq:measure} and~\eqref{eq:pbn}, respectively, for all $\alpha \in \mathcal{A}$.
\end{proposition}
\noindent For completeness, a proof of Proposition~\ref{prop:mu_betti_1} is given in the appendix. 
%% TODO: proof of sign pattern stuff 

%In the following sections, we will exploit the continuity of the entries in  of~\eqref{eq:param_boundary_matrix} to ease the computation of both counting invariants in parameterized settings.

%In summary, by restricting ourselves to homology defined over real-valued coefficients, we may parameterize the non-zero entries of the $p$-th boundary matrix $\partial_p$ from~\ref{eq:boundary_matrix} directly using $f : K \times \mathcal{H} \to \mathbb{R}$. 
% Moreover, since $\mathrm{rank}(A) = \mathrm{rank}(P^T A P)$ for any permutation matrix $P$, it is clear that expressions of the counting invariants from Propositions~\ref{prop:rank_reduction} and~\ref{prop:mu_reduction} yield equivalent values to the parameterized expressions in~\eqref{eq:pbn_parameterized} and~\eqref{eq:mu_parameterized}. 
%\end{proof}
%The parameterized PBN can be written as: 
%\begin{align*}\label{eq:pbn_parameterized}
%	\beta_p^{i,j} : \mathcal{H} \times \mathcal{P}(V) &\to \mathbb{N} \\
%	%h &\mapsto \lvert K_i^{(p)}(h) \rvert  -  (\mathrm{r} \circ \hat{\partial}_{p}^{i})(h) - (\mathrm{r} \circ \hat{\partial}_{q}^{j})(h) + (\mathrm{r} \circ \hat{\partial}_{q}^{i \+ \epsilon, j})(h) \numberthis
%	h, K &\mapsto \lvert K_i^{p}(h) \rvert - \mathrm{rank}\big(\,\hat{\partial}_{p}^{i}(h)\,\big) - \mathrm{rank}\big(\,\hat{\partial}_{q}^{j}(h)\,\big) + \mathrm{rank}\big(\,\hat{\partial}_{q}^{i \+ \delta, j}(h)\,\big) \numberthis
%\end{align*}
%% deflation?
%. Observe~\eqref{eq:pbn_parameterized} is essentially the same form as~\eqref{eq:betti_four}. 
%By Proposition~\ref{prop:mu_reduction}, we also have a parameterized multiplicity function for any rectangle $R = [i,j] \times [k,l]$ in the upper half-plane $\Delta_{+}$ satisfying $i < j \leq k < l$: 
%\begin{align*}\label{eq:mu_parameterized}
%	\mu_p^{R} : \mathcal{H} \times \mathcal{P}(V) &\to \mathbb{N} \\
%	h, K & \mapsto  \mathrm{rank}\big(\,\hat{\partial}_{q}^{j \+ \delta, k}(h)\,\big) - \mathrm{rank}\big(\,\hat{\partial}_{q}^{i \+ \delta, k}(h)\,\big) - \mathrm{rank}\big(\,\hat{\partial}_{q}^{j \+ \delta, l}(h)\,\big) + \mathrm{rank}\big(\, \hat{\partial}_{q}^{i \+ \delta, l}(h)\,\big) \numberthis
%\end{align*}

%One remark is in order: note that although definition~\ref{def:time_boundary_matrix} specifies $\hat{\partial}_p^{i,j}$ as a full $\binom{n}{p} \times \binom{n}{q}$ matrix, implying a memory complexity of $O(q\cdot n^{q})$ for all $h \in \mathcal{H}$, we remark that there is no need to fully allocate this much memory as the rows/columns corresponding to the set of $p$/$q$ face/coface pairs $(\sigma_k, \sigma_l)$ with $f(\sigma_l) < i$ or $f(\sigma_k) > j$ are entirely $0$. 
%As we will show below, it is enough to have access to the simplices $K_j^{(q)}(h)$. % and $\bar{K}_i^{(p)}(h)$?

% In general, computing dgm_p(X) for p \geq 1 requires O(\lvert X \rvert^{3(p+2)}) [ p = 1 <=> O(n^9), p=2 <=> O(n^12)] (curvature sets paper) and O(\lvert X \rvert^{2(p+2)}) memory.

% Benefits of rank(A) = rank(A^T A) = rank(A A^T)


%\subsection{Special Cases} The spectrum of the graph Laplacian is known to have closed-form expressions for certain structured graphs. In particular, the spectra of the cycle graph $C_n$ and the path graph $P_n$ over $n$ vertices is given by: 
%\begin{equation}
%	\Lambda(C_n) = 1-\cos\left (\frac{2\pi k}{n} \right), \quad \Lambda(P_n) = 1-\cos\left (\frac{\pi k}{n-1} \right)
%\end{equation}
%for all $k = 0, 1, \dots, n-1$. Thus, if we know ahead of times the graph we are working with is one of these special graphs, it follows that may read off its $(i,j)$-th PBN in $O(n)$ time. 

%Moreover, by composing the vertex and edge functions $f_V$ and $f_E$, respectively, with the step functions $S_\ast : \mathbb{R} \to \{0, 1\}$

%graph has a unique Laplacian matrix, this matrix does not in general uniqueIy
%determine a graph: the Laplacian tells us nothing about how many Ioops were
%to be found in the original graph.
% The characteristic polynomial of L(G) is given as the product of the characteristic polynomial of its disjoint subgraphs L(G_1), ..., L(G_k).
% The Spectrum of the Laplacian in Riemannian 	Geometry
% The spectrum does not in general determine the geometry of a manifold. Nevertheless, some geometric information can be extracted from the spectrum. In what follows, we define a spectral invariant to be anything that is completely determined by the spectrum






% TODO: UNDERSTAND THE FOURTH TERM 
%The graph laplacian appears naturally in the computation of $\beta_0^\ast$. Consider the rank expression from~\eqref{eq:betti_four}. Since $\partial_0$ is trivial, the expression reduces to: 
%\begin{equation}
%	\beta_p^{i,j} = \lvert K_i^{(0)}\rvert - \mathrm{rank}(\partial_{1}^{j}) + \mathrm{rank}(\partial_{1}^{i \+ 1, j} )
%\end{equation} 
%Since $\mathrm{rank}(A) = \mathrm{rank}(A A^T) = \mathrm{rank}(A^T A)$, we may equivalently express the second and third terms in terms of their graph Laplacians. Let $G_j = \mathcal{L}(K_j)$ denote the graph of $K_j$...
%as $\mathrm{rank}(L(K_j))$ and $\mathrm{rank}(L(K_j \cap \bar{K}_i))$, where $\bar{K}_i = K_\bullet \setminus K_i$ denotes the complement. Since the rank of any (irreducible) graph Laplacian may be deduced by counting the connected components of the underlying graph, we deduce that the complexity of computing $\beta_0^{i,j}$ reduces to the complexity 
% TODO: compute this for small example to verify Laplacians

%\textbf{Example:} Consider a 2-simplex $K = \Delta_2$ with vertex weights $\mathrm{Im}(f_E) = \{a, b, c\}$ and edge weights $\mathrm{Im}(f_E) = \{d,e,f\}$. Suppose we augment the chain expressions in boundary matrix $\partial_p$ to reflect The matrix representations of 


%For $p \geq 1$, the PBN expression can still be generalized via \emph{up Laplacians}. The higher-dimensional generalizations of the graph Laplacian for simplicial complexes have been studied in a variety of settings, see e.g.~\cite{}. 
%In particular, given $K$ a finite oriented simplicial complex and some $p \geq 0$, the $p$-th \emph{combinatorial Laplacian} of $K$ is the linear operator $\mathcal{L}_p : C_p \to C_p$ given by: 
%\begin{equation}
%	\mathcal{L}_p = \partial_{p\+1} \circ \partial_{p\+1}^T + \partial_{p}^T \circ \partial_{p}
%\end{equation}
%For convenience, we use the notation ${\uparrow}L_p = \partial_{p\+1} \circ \partial_{p\+1}^T$ and ${\downarrow}L_{p} = \partial_{p}^T \circ \partial_{p}$ to denote the so-called \emph{up} and \emph{down} combinatorial $p$-th Laplacians, respectively. 
%With this notation, notice the computation of the PBN can be expressed via the ranks of up Laplacians...

%We now show a few properties that the rank-based formula $\beta_p^{i,j}$ exhibits that are advantageous in parameterized settings. 
%The first such property is a simple parameterized relaxation of PBN:
%\begin{align*}
%	\hat{\beta}_p^{i,j} &= \lvert K_i^{(p)} \rvert - \mathrm{rank}(\partial_{p}^{i}) - \mathrm{rank}(\partial_{q}^{j}) + \mathrm{rank}(\partial_{q}^{i \+ 1, j} ) \\
%%	&= \lvert K_i^{(p)} \rvert - \mathrm{rank}((\partial_{p}^{i})(\partial_{p}^{i})^T) - \mathrm{rank}(\underbrace{(\partial_{q}^{j})(\partial_{q}^{j})^T}_{\uparrow\mathcal{L}_1^i}) + \mathrm{rank}((\partial_{q}^{i \+ 1, j})(\partial_{q}^{i \+ 1, j})^T)
%&= \lvert K_i^{(p)} \rvert - \mathrm{rank}((\partial_{p}^{i})(\partial_{p}^{i})^T) - \mathrm{rank}((\partial_{q}^{j})(\partial_{q}^{j})^T) + \mathrm{rank}( (\partial_{q}^{i \+ 1, j})(\partial_{q}^{i \+ 1, j})^T)\\
%&= \lvert K_i^{(p)} \rvert - \mathrm{rank}(L_p^i) - \mathrm{rank}(L_q^j) + \mathrm{rank}(L_q^{i\+1,j}) \numberthis
%\end{align*}

% \hat{\beta}_p^{i,j} = \sum\limits_{\underset{\lvert \sigma \rvert = p+1}{\sigma \in \mathcal{P}(X)}} \mathrm{sgn}\left( \; \lvert i - f(\sigma) \rvert_{\+} \right)  - \mathrm{rank}(\partial_{p}^{i}) - \mathrm{rank}(\partial_{q}^{j}) + \mathrm{rank}(\partial_{q}^{i \+ 1, j} )

%Clearly the entries of $\partial_p(t)$ now vary smoothly in $t \in T$. Moreover, for fixed $p \geq 0$, we have:
%\begin{enumerate}
%	\item $\mathrm{rank}(\partial_p^t) = \mathrm{dim}(\mathrm{B}_{p-1}(K_t))$ for all $t \in T$ 
%	\item $\lVert \partial_p^t - \partial_p^{t'} \rVert_F \sim O(m_p)$ when $\delta_\mathcal{X}$ is $C$-Lipshitz over $T$ and $\lvert t - t' \rvert$ is small,
%	\item $\lVert \partial_p^t \rVert_{2} \leq \epsilon \sqrt{\kappa} \, (p+1)$ where $\kappa = \max \sum\limits_{t \in T}\sum\limits_{\sigma \in K_t}\mathds{1}(\mathrm{diam}(\sigma) \leq \epsilon)$
%	%\sqrt{\epsilon\,\kappa\,(p+1)}$ where $\kappa = \max \sum\limits_{t \in T}\sum\limits_{\sigma \in K_t}\mathds{1}(\mathrm{diam}(\sigma) \leq \epsilon)$ %$C(n,k) = \binom{n}{k}$
%\end{enumerate}


%In the spirit of~\cite{chen1996class}, 
% Computational results of being able to calculate/approximate rank quickly
%\subfile{generic_rank}
%On one hand, equation~\eqref{eq:pbn_stability} shows that PBN functions \emph{are} stable with respect to the extended matching distance: if the two filter functions $f, f'$ are $\epsilon$-close in the sense that $\lVert f - f' \lVert_\infty \leq \epsilon$, then the PBN functions have distance $d(\beta_f, \beta_{f'}) \leq \epsilon$. 
%On the other hand, computationally, the extending matching distance $d(\beta_f, \beta_{f'})$ requires finding the optimal matching between the points $p \in \Lambda_+$ with non-zero multiplicity, and determining the locations of these points effectively requires obtaining the persistence diagrams of both $f$ and $f'$---both of which are high complexity algorithms. 
%We would like to avoid both. The counting variants studied thus far are natural combinatorial invariants that 
%We tackled the form by fixing specific \emph{regions} of $\Lambda_+$ a-priori; we now focus on on the latter.
%persistence information contained in them, if possible. 

%for the multiplicity function $\mu_p^R$ to be considered stable we would need to show 
%$\lVert \mu_p^R(h,K) - \mu_p^R(h + \epsilon,K) \rVert \leq \epsilon$
%there exists a non-negative constant $C$ such that $\lVert \mu_h^R - \mu_{h + \epsilon}^R \rVert \leq \epsilon \cdot C$, where $\mu^R_{h} = \mu_p^R(K,h)$.
%$1/\epsilon \leq C$
%for $\mu_p(K, h)$ to be stable with respect to the matching distance from~\eqref{eq:pbn_stability} we need to show there exists a non-negative constant $C$ such that $1/\epsilon \leq C$

%Several other authors~\cite{}, have confronted this discontinuity by relaxing $\mathrm{rank}(X)$ with the nuclear norm $\lVert X\rVert_\ast$. 
%%To see the motivation for this, let $X \in \mathbb{R}^{n \times m}$ denote a given real matrix with SVD $X = U \Sigma V^T$, and let $\vec{\sigma} = (\sigma_1, \dots, \sigma_n)$ denote a vector containing the singular values of $X$.  
%%\begin{equation}\label{eq:pseudo_norm}
%%	\mathrm{rank}(X) = \lVert \vec{\sigma} \rVert_0  \approx \lVert \vec{\sigma} \rVert_1 = \lVert X \rVert_\ast 
%%\end{equation}
%In the context of the Rank Minimization Problem (RMP), the nuclear norm is often used as a surrogate for the rank function due to the fact that it forms a convex envelope of the rank function in the space of all $n \times n$ matrices on the unit-ball in the operator norm~\cite{}. 
%Paired with the observation that the rank function can be equivalently expressed as composing the singular value function with the $\ell_0$ \emph{pseudo-norm} (as in~\eqref{eq:pseudo_norm}), replacing rank optimization problems with nuclear norm minimization problems has become an active area of research which has lead to many fruitful results in the areas of compressed sensing and matrix completion. 
 
\subsection{Spectral rank relaxation}\label{sec:spectral_relax}
Extending from the ideas outlined in the previous section, we now substitute the discontinuous rank function with a continuous approximation. Our approach exploits the spectral characterization of the rank function: given a matrix $X \in \mathbb{R}^{n \times m}$ and its singular value decomposition (SVD) $X = U \Sigma V^T$, the \emph{rank} of $X$ is given by the composition:
%of the one-sided sign function with the singular value function:
\begin{equation}\label{eq:rank_def}
	\mathrm{rank}(X) = \sum\limits_{i=1}^{n} \mathrm{sgn}_+(\sigma_i(X)), \quad \quad \mathrm{sgn}_{+}(x) = \begin{cases}
		1 & \text{if } x > 0 \\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
where $\Sigma = \mathrm{diag}(\{\sigma_1, \sigma_2, \dots, \sigma_n \})$ are the singular values  of $X$ and $\mathrm{sgn}_+: \mathbb{R} \to \{0, 1\}$ is the one-sided sign function. 
As the singular values vary continuously under perturbations in $X$, the discontinuity in~\eqref{eq:rank_def} manifests from the one-sided sign function. 
% In the general setting, however, the nuclear norm deviates drastically from the rank function and tends to act as good rank substitute in low-rank regimes. We defer further discussion on this topic to section~\ref{}.  
%One way to counter this discontinuity is to replace the $\mathrm{sgn}_+$ function with an $\epsilon$-parameterized family of continuous $\mathrm{sgn}$ approximation functions $\phi: \mathbb{R}_+ \times \mathbb{R}_{++} \to \mathbb{R}_+$ satisfying:
%\begin{flalign}\label{eq:rank_sgn}
%	(\, \forall \, x > 0 \,)  & & \quad\quad\quad 
%%\begin{equation}
%	\mathrm{rank}(X) \approx \sum\limits_{i=1}^n \phi(\sigma_i(X), \epsilon), \quad \quad &  \lim_{\epsilon \to 0^+} \phi(x, \epsilon) = \mathrm{sgn}_+(x) &&
%\end{flalign}
%\end{equation}

%Following the seminal work done by Chen et al.~\cite{chen1996class}, 
Building off the seminal work of Chen \& Mangasarian~\cite{mangasarian1994class}, Bi et al.~\cite{bi2013approximation} propose replacing the $\mathrm{sgn}_+$ function in~\eqref{eq:rank_def} with integrated smoothed variations of the Dirac delta $\delta$:  
\begin{flalign}\label{eq:phi}
(\, \forall \, z \geq 0, \epsilon > 0  \,)  & & \quad\quad\quad
\phi(x, \epsilon) := \int\limits_{-\infty}^x \hat{\delta}(z, \epsilon) dz, \quad \quad  & 
\hat{\delta}(z, \epsilon) = \nu(1/\epsilon) \cdot p \big( z \, \nu (1/\epsilon) \big ) && 
\end{flalign}
where $p: \mathbb{R}_+ \to \mathbb{R}_+$ is continuous density function and $\nu : \mathbb{R}_+ \to \mathbb{R}_+$ is continuous increasing function satisfying $\nu(0) = 0$ and $\nu(\epsilon) > 0$. 
In contrast to the $\mathrm{sgn}_+$ function, if $p$ is continuous on $\mathbb{R}_+$ then $\phi(\cdot, \epsilon)$ is continuously differentiable on $\mathbb{R}_+$, and if $p$ is bounded above on $\mathbb{R}_+$, then $\phi(\cdot, \epsilon)$ is globally Lipshitz continuous on $\mathbb{R}_+$. 
Moreover, note that $\phi(0, \epsilon) = 0$ and: 
\begin{flalign}\label{eq:phi2}
(\, \forall x \geq 0 \,)  && \lim\limits_{\epsilon \to 0^{+}} \phi(x, \epsilon) = \mathrm{sgn}_+(x) && 
\end{flalign}
for any choice of $(p, \nu)$ satisfying the assumptions above. 
Varying the relaxation parameter $\epsilon > 0$ in~\eqref{eq:phi} yields an $\epsilon$-parameterized family of continuous $\mathrm{sgn}_+$ relaxations $\phi: \mathbb{R}_+ \times \mathbb{R}_{++} \to \mathbb{R}_+$, where $\epsilon > 0$ controls the accuracy of the relaxation. 

Many of the properties of~\eqref{eq:phi} extend naturally to the rank function when substituted appropriately via~\eqref{eq:rank_def}.
In particular, pairing $X = U\Sigma V^T$ with a scalar-valued $\phi$ that is continuously differentiable at every $\sigma \in \Sigma$ yields a corresponding \emph{Lwner operator} $\Phi_\epsilon$: 
% $f$ is continuous differentiable at every eigenvalue of $X$
%This is done and mapping $X$ as follows: %$\sigma \mapsto \phi(\sigma, \epsilon)$
%$X \mapsto U \tilde{\Sigma} V^T = \tilde{X}$ where $\tilde{\Sigma} = \mathrm{diag}( \phi(\sigma_1, \epsilon), \phi(\sigma_2, \epsilon), \dots, \phi(\sigma_n, \epsilon))$, 
\begin{align*}\label{eq:lowner}
 \Phi_\epsilon (X) & = U \, \mathrm{diag}( \phi(\sigma_1, \epsilon), \phi(\sigma_2, \epsilon), \dots, \phi(\sigma_n, \epsilon)) \, V^T \\
 & = \sum\limits_{i=1}^{n} \phi(\sigma_i, \epsilon) u v^T\numberthis
\end{align*}
%\begin{align*}\label{eq:lowner}
% \Phi_\epsilon : \quad \mathbb{R}^{n \times m}  & \; \to \; \mathbb{R}^{n \times m}  \\
%						           X & \; \mapsto \; U \, \mathrm{diag}( \tilde{\phi}(\sigma_1, \epsilon), \tilde{\phi}(\sigma_2, \epsilon), \dots, \tilde{\phi}(\sigma_n, \epsilon)) \, V^T \numberthis
%\end{align*}
It may be shown that $\Phi_\epsilon$ is a continuously differentiable operator in $\mathbb{R}^{n \times m}$ for any $\epsilon > 0$. 
%The operator $\Phi_\epsilon$ from~\eqref{} is called \emph{Lwners} operator. One can verify that the nuclear norm of the operator is given by composing $\phi(\cdot, \epsilon)$ with the singular value function. 
In fact, if $\phi$ is twice-differentiable at each $\sigma_i(X)$ for all $i = 1, \dots, n$, then~\eqref{eq:lowner} is also twice continuously differentiable at $X$~\cite{ding2018spectral}. 
Before summarizing additional properties of $\Phi_\epsilon$ and its relationship with~\eqref{eq:rank_def}, we connect $\Phi_\epsilon$ and $\phi$ with a definition:
\begin{definition}[Spectral Rank Approximation]
	Given $X \in \mathbb{R}^{n \times m}$ with singular value decomposition $X = U \Sigma V^T$ and a fixed $\epsilon > 0$, any choice of $\phi: \mathbb{R}_+ \times \mathbb{R}_{++}$ satisfying~\eqref{eq:phi} defines a \emph{spectral rank approximation} $\lVert \Phi_\epsilon(X) \rVert_\ast$ of $X$ via: 
	\begin{equation}\label{def:cont_rank}
		\lVert \Phi_\epsilon(X) \rVert_\ast = \sum\limits_{i=1}^n \phi(\sigma_i, \epsilon)
	\end{equation}
%	where $\sigma_\ast$ are the singular values $\Sigma = \mathrm{diag}(\sigma_1, \sigma_2, \dots, \sigma_n)$ of $X$. 
\end{definition}
%whose nuclear norm is semismooth on $\mathbb{R}^{n \times m}$. 
\noindent Apart from serving as a smooth approximation of the $\mathrm{rank}$ function, this quantity turns out to have a variety of attractive properties related to monotonicity and differentiability.
\noindent
%The set of functions whose nuclear norms arecompatible 
A few such properties are recorded below. 
\begin{proposition}[Bi et al.~\cite{bi2013approximation}] The operator $\Phi_\epsilon: \mathbb{R}^{n \times m} \to \mathbb{R}^{n \times m}$ defined by~\eqref{eq:lowner} satisfies: 
	\begin{enumerate}
		\item For any $\epsilon \leq \epsilon'$, $\lVert \Phi_{\epsilon}(X) \rVert_\ast \geq \lVert \Phi_{\epsilon'}(X) \rVert_\ast$ for all $X \in \mathbb{R}^{n \times m}$.
		\item For any given $X \in \mathbb{R}^{n \times m}$ with rank $r = \mathrm{rank}(X)$, if $\epsilon$ satisfies $0 < \epsilon \leq \sigma_r / r$, then: 
		$$ 0 \leq r - \lVert \Phi_\epsilon(X) \rVert_\ast \leq c $$
		where $c$ is a positive constant that depends only on $r$, $\nu$, and $p$. 
		\item $\lVert \Phi_\epsilon(X) \rVert_\ast$ is globally Lipshitz continuous and semismooth\footnote{The notion of semismoothness here refers to the existence certain directional derivatives in the limit as $\epsilon \to 0^+$, see~\cite{bi2013approximation, bhatia2013matrix} for more details.} on $\mathbb{R}^{n \times m}$.
%		\item $\phi(\cdot, \epsilon)$ is continuously differentiable in $\mathbb{R}_+$ if $p$ is continuous on $\mathbb{R}_+$
%		\item For any given $x \in \mathbb{R}_+$, $\phi(x, \epsilon') \leq \phi(x, \epsilon)$ whenever $\epsilon' > \epsilon$
%		\item If $p$ is bounded above on $\mathbb{R}_+$, then $\phi(\cdot, \epsilon)$ is globally Lipshitz continuous on $\mathbb{R}_+$
	\end{enumerate}
\end{proposition}
\noindent Note that $\lVert\Phi_\epsilon(X)\rVert_\ast$ is not necessarily differentiable on $\mathbb{R}^{n \times m}$ due to Proposition 2.2(e) in~\cite{bi2013approximation}, but it is differentiable on the positive semi-definite cone $\mathbb{S}^n_+$. As we will discuss in section~\ref{sec:laplacian_theory}, this is sufficient for our purposes here. 
\\
\\
\noindent To adapt these relaxations to the rank expressions given in Proposition~\ref{prop:mu_betti_1}, we need to modify the boundary chains in~\eqref{eq:alt_sum} to vary continuously in $\mathcal{A}$.
\begin{figure}
\centering
%	\includegraphics[width=0.45\textwidth]{mult_both}
	\includegraphics[width=0.90\textwidth]{cont_relax}
	\caption{From left to right---the $\ell_1$ norm (red) forms a convex envelope over the $\ell_0$ (black) pseudo-norm on the interval $[-1, 1]$; 
	$\tilde{\phi}(\cdot, \epsilon)$ at various values of $\epsilon$, with $p(x) = 2x (x^2 + 1)^{-2}$ and $\nu(\epsilon) = \sqrt{\epsilon}$ (red) and at $\epsilon = 0$ (black); 
	the step function $S_i(x)$ from definition~\ref{def:time_boundary_matrix}; 
	the smoothstep relaxation $\mathcal{S}_i^{\omega}$ from~\eqref{eq:smoothstep}.
	}
	\label{fig:smoothstep}
\end{figure}
Due to our parameterized boundary chains from definition~\ref{def:time_boundary_matrix}, the discontinuity here comes from the use of step functions~\eqref{eq:step_functions}. 
We exchange $S : \mathbb{R} \to \{ 0, 1\}$ with clamped \emph{smoothstep} functions $\mathcal{S}: \mathbb{R} \to [0, 1]$ of the form: 
%In particular, we swap out the step functions $S : \mathbb{R} \to \{ 0, 1\}$ from~\eqref{eq:param_boundary_matrix} with \emph{smoothstep} functions $\mathcal{S}: \mathbb{R} \to [0, 1]$. 
\begin{equation}\label{eq:smoothstep}
\mathcal{S}_a^{\omega} (x) = \begin{cases}
	0 & x \leq a \\
	P_n\big( \omega^{-1}((a + \omega) - x) \big) & a < x < a + \omega \\
	1 & a + \omega \leq x
\end{cases}
\end{equation} 
where $P_n: [0,1] \to [0,1]$ is a $n$-th order polynomial satisfying $P_n(0) = 0$ and $P_n(1) = 1$.
These function interpolate the discontinuous step portion of $S$ along a fixed interval $(a,a+\omega)$ (see Figure~\ref{fig:smoothstep}), and are used in computer graphics and learning applications due to their ease of composability and efficiency in evaluation. 
%\begin{equation}
%S_n(x)  = a_n x^n + a_{n-1} x^{n-1} + \dots + a_1 x + a_0
%\end{equation} 
%Our motivation to use them here is based on the observations shown in Figure~\ref{fig:smoothstep}: by substituting $\mathcal{S}_n^{\omega}$ for the step functions 	in~\eqref{eq:param_boundary_matrix} and adding a small translation, we may replace the step functions in the parameterized boundary matrix with entries varies continuously in $\mathcal{A}$ and retain the correct sign. 
Since smoothstep functions retain the sign of their input, composing with~\eqref{eq:mu_four} and substituting $\mathrm{rank}(\cdot)$ with $\Phi(\cdot, \epsilon)$ yields a continuously-varying $\epsilon$-approximation of the rank function, which we call a \emph{spectral rank function}. 
\begin{definition}[Spectral Rank]\label{def:smooth_mu}
Let $K$ denote a simplicial complex and $f: K \times \mathcal{A} \to \mathbb{R}$ a parameterized family of functions which vary continuously in $\mathcal{A}$. For some fixed $R = [i,j] \times [k,l] \subset \Delta_+$ and parameters $(p, \epsilon, \omega)$ satisfying $p \geq 0$, $\epsilon > 0$, and $\omega > 0$, respectively, define the $\mathcal{A}$-parameterized \emph{spectral persistent betti number} and \emph{spectral multiplicity} of $R$ as:
	\begin{equation}\label{eq:bett_cont}
\hat{\beta}_p^{i,j}(\alpha) = 
\lVert \Phi_\epsilon^\alpha(\,\hat{I}_{p}^i\,)\rVert_\ast -
\lVert \Phi_\epsilon^\alpha(\hat{\partial}_{p}^{1, i}) \rVert_\ast - 
\lVert \Phi_\epsilon^\alpha(\hat{\partial}_{p+1}^{1, j}) \rVert_\ast + 
\lVert \Phi_\epsilon^\alpha(\hat{\partial}_{p+1}^{i\+\delta, j}) \rVert_\ast \numberthis
	\end{equation}
	\begin{equation}\label{eq:mu_cont}
	\hat{\mu}_{p,\epsilon}^R(\alpha) = 
		 \lVert \Phi_\epsilon^\alpha(\hat{\partial}_{p\+1}^{j \+ \delta, k}) \rVert_\ast - 
		 \lVert \Phi_\epsilon^\alpha(\hat{\partial}_{p\+1}^{i \+ \delta, k}) \rVert_\ast -  
		 \lVert \Phi_\epsilon^\alpha(\hat{\partial}_{p\+1}^{j \+ \delta, l}) \rVert_\ast + 
		 \lVert \Phi_\epsilon^\alpha(\hat{\partial}_{p\+1}^{i \+ \delta, l}) \rVert_\ast \numberthis
\end{equation}
where $\Phi_\epsilon^\alpha(\partial) = (\Phi_\epsilon \circ \partial)(\alpha)$ for any $\alpha \in \mathcal{A}$, $\hat{I}_{p}^i = \mathrm{diag}(\{ \mathcal{S}_i^\omega(f(\tau_j)) \}_{j=1}^{n} )$, and $\hat{\partial}_p$ is $\mathcal{A}$-parameterized $p$-th boundary matrix from definition~\ref{def:time_boundary_matrix} with step functions $S_\ast$ replaced by the smoothstep $\mathcal{S}_\ast^\omega$ from~\eqref{eq:smoothstep}.
\end{definition}
\noindent Since the sum $f + g$ of two Lipshitz functions $f$ and $g$ is also Lipshitz, it is easy to verify both $\hat{\mu}_p^R$ and $\hat{\beta}_p^{i,j}$ are Lipshitz continuous functions in $\alpha$ whenever $f$ is Lipshitz by combining the smoothness of $\mathcal{S}_\ast^\omega$ with the global Lipshitz continuity of~\eqref{def:cont_rank}. 
%We now discuss the various properties that these smoothed-counting invariants obey. 
%\subsubsection*{Properties}
%
%Basic properties of $\hat{\mu}$. % pseudo-monotonicity, additivity 
%Consider a filtration $(K, f)$ indexed over the real line $f : K \to \mathbb{R}$ whose filter function $f$ is perturbed by some positive amount $\delta > 0$, such that $f' = f + \delta$.
%\begin{equation}
%	\lVert f - f' \rVert_{\infty} \leq \delta 
%\end{equation} 
\\
\\
\noindent
\textbf{Regularization:} A common approach used in sparse inverse problems to relax the rank function is to impose a regularization scheme. 
For example, the classical least-squares problem solves the [possibly ill-posed] linear system $A x = b$ via the minimization:
\begin{equation}\label{eq:least_squares}
	x^\ast = \argmin\limits_{x \in \mathbb{R}^n} \lVert A x - b \rVert^2	
\end{equation}
% ng $A = X$, solutions $x_\epsilon^\ast$ to TR problems in standard form satisfy 
For a variety of reasons, such as giving preference to solutions $x^\ast$ with small norm,~\eqref{eq:least_squares} is often substituted with an explicit \emph{Tikhonov regularization} (TR) for some $\epsilon > 0$:
\begin{equation}\label{eq:tikhonov}
	x_\epsilon^\ast = \argmin\limits_{x \in \mathbb{R}^n} \lVert Ax - b\rVert^2 + \epsilon \lVert x \rVert^2
\end{equation}
The solution to~\eqref{eq:tikhonov} is given by the regularized form of the pseudo-inverse given below:  
\begin{equation}\label{eq:tikhonov_soln}
	x_\epsilon^\ast = (A^T A + \epsilon I)^{-1} A^T b
\end{equation}
To illustrate the relevance of TR with our proposed approximation, consider the parameterization
%\footnote{Similar regularization interpretations may be derived for other choices of $\nu(\epsilon)$ and $p(x)$.}
 of $\phi$ obtained by setting $\nu(\epsilon) = \sqrt{\epsilon}$ and $p(x) = 2x (x^2 + 1)^{-2}$. By~\eqref{eq:phi}, one obtains:
\begin{equation}
	\phi(x, \epsilon) = \int\limits_{-\infty}^x \hat{\delta}(z, \epsilon) dz = \frac{x^2}{x^2 + \epsilon}
\end{equation}
Substituting $\mathrm{sgn}_+ \mapsto \phi$ and composing with the singular value function~\eqref{def:cont_rank}, the corresponding spectral rank approximation reduces to:
\begin{equation}\label{eq:tikhonov_1}
	\lVert \Phi_\epsilon(X) \rVert_\ast = \sum\limits_{i = 1}^n \frac{\sigma_i(X)^2}{\sigma_i(X)^2 + \epsilon} = \mathrm{Tr}\left[(X^T X + \epsilon \, I_n)^{-1} X^T X \right] 
\end{equation}
The connection between the TR and the right hand side of~\eqref{eq:tikhonov_1} is now clear: obtaining $ \lVert \Phi_\epsilon(\cdot) \rVert_\ast$ is equivalent to solving $n$ TR least-squares problems of the form~\eqref{eq:tikhonov} where $X$ is substituted with either the boundary/coboundary operator $\partial_p$/$\partial_p^T$ and $b$ is substituted with an $\alpha$-parameterized $p$-(co)chain.
In this sense, the spectral rank invariants proposed in definition~\ref{def:smooth_mu} are essentially regularized rank invariant approximations.
 
 \begin{remark}
 One interpretation of the relaxation parameter $\epsilon$ is as a bias term that preferences the (pseudo)-inverse towards smaller singular values.
Larger values of $\epsilon$ smooth out $\lVert \Phi_\epsilon(\cdot) \rVert_\ast$ by making the pseudo-inverse less sensitive to perturbations, whereas smaller values of $\epsilon$ lead to a more faithful approximation of the rank.
This can be seen directly by~\eqref{eq:tikhonov_soln} as well, as increasing $\epsilon$  lowers the condition number of $A^T A + \epsilon I$ monotonically, signaling a tradeoff in stability at the expense of accuracy. 
 \end{remark}



%Moreover, since operator norm determines the Lipshitz constant of a given function, $\epsilon$ not only plays the role of an accuracy parameter, but as a \emph{smoothing parameter}. 


%Moreover, by the cyclic property of the trace operator, since $X = \partial_1$ here and $L = \partial_1 \partial_1^T = [l_1, l_2, \dots, l_n]$ we have: 
%\begin{equation*}
%	\lVert \Phi_\epsilon(X) \rVert_\ast = \mathrm{Tr}[(L + \epsilon I_n)^{-1} L] = \sum\limits_{i=1}^n (L_\epsilon^{-1} l_i)_i
%\end{equation*}

%Thus we may solve this problem by solving $n$ sparse linear systems of that take the form $Ax = b$, where here $A$ is a Laplacian matrix with an $\epsilon \cdot I_n$ addition to it's diagonal. 
%

\subsection{Combinatorial Laplacians}\label{sec:laplacian_theory}
%To better understand the interpretation of the smoothed relaxations of the counting invariants from definition~\ref{def:smooth_mu}, it is imperative to understand what the spectral information  constitutive terms encodes. 
%In section~\ref{sec:param_boundary}, we exploited permutation invariance to simplify the persistence rank invariants~\eqref{eq:betti_four} and~\eqref{eq:mu_four} and in section~\ref{sec:spectral_relax} we used the fact\footnote{
%This fact can be proven using the spectral theorem or observed directly due to the inertia theorem~\cite{bhatia2013matrix}. %Section I.2 of stewart 
%} that $\sigma_i(X) > 0$ whenever $\mathrm{rank}(X) \leq i$ to derive a spectral relaxation of the rank function. 
Continuing the theme of studying invariances of the rank function to simplify expressions~\eqref{eq:betti_four} and~\eqref{eq:mu_four}, in this section we exploit another well known identity of the rank function: 
$$\mathrm{rank}(X) = \mathrm{rank}(X X^T) = \mathrm{rank}(X^T X)$$ 
On the spectral side, it is well-known the square roots of the non-zero eigenvalues of either $XX^T$ or $X^T X$ yield the singular values of $X$. Since $\partial_1 \partial_1^T$ is the well studied \emph{graph Laplacian}, we may consider the study of spectra of \emph{combinatorial Laplacians}---generalizations of the graph Laplacian---as the study of singular values of boundary operators.  
%Towards revealing  
% "Many combinatorial operations that one can perform on a simplicial complex do not affect its homology typically leave characteristic traces in the spectrum of a suitable Laplace operator. In the weighted case, there is additional geometric information that likewise influences the spectrum." 

The natural extension of the graph Laplacian $L$ to simplicial complexes is the \emph{$p$-th combinatorial Laplacian} $\Delta_p$, whose explicit matrix representation is given by: 
\begin{equation}\label{eq:comb_lap}
	\Delta_p(K) = 
	\underbrace{\partial_{p\+1} \circ \partial_{p\+1}^T}_{L_p^{\text{up}}} + \underbrace{\partial_{p}^T  \circ  \partial_{p}}_{L_p^{\text{dn}}} 
%	\simeq H() 
\end{equation}
Indeed, when $p = 0$, $\Delta_0(K) = \partial_1 \partial_1^T = L$ recovers the graph Laplacian. 
Moreover, like $L$, all three operators $\Delta_p$, $L_p^{\text{up}}$, and $L_p^{\text{dn}}$ are symmetric, positive semidefinite, and compact.
To translate the continuity results from definition~\ref{def:smooth_mu} to $\Delta_p$, we must consider weighted versions of $\Delta_p$:  
\begin{equation}\label{eq:weighted_up_laplace}
	\Delta_p(K, f) \triangleq  W_p^{-1} \partial_{p+1} W_{p+1} \partial_{p+1}^T \, + \,  \partial_{p}^T W_p^{-1} \partial_p W_{p+1} 
\end{equation} 
where $W_\ast = \mathrm{diag}(\{ f(\sigma_i) \}_{i=1}^n)$ represent positive diagonal weight matrices. 
%More generally, given a simplicial complex $K$, 
The Laplace operator is uniquely determined by the choice of \emph{weight function} $f: K \to \mathbb{R}_+$. For example,~\eqref{eq:comb_lap} is determined by the constant map $f(\sigma) = 1$. 
%Collecting $f$ into diagonal matrices , the 
 %whether they are weighted, symmetric, or normalized
% suggesting several classical results (e.g. the matrix tree theorem) may be adapted to simplicial complexes.
As with boundary operators, $\Delta_p(K)$ encodes simplicial homology groups in its nullspace, a result known as the discrete Hodge Theorem~\cite{}: 
\begin{equation}\label{eq:laplace_hom}
	\tilde{H}_p(K; \mathbb{R}) \cong \mathrm{ker}(\Delta_p(K)), \quad \beta_p = \mathrm{nullity}(\Delta_p(K))
\end{equation}
In fact, the nullity of all the Laplace operators discussed here are identical. 
Thus, any general Laplacian operator in theory can substituted into step (4) in the outline given in section~\ref{sec:rank_invariant_summary}.
However, the non-zero part of their spectra varies significantly both in the choice of operator and weight function.
%be used as a rank invariant 
As our spectral rank approximation~\eqref{def:smooth_mu} is driven entirely by this non-zero portion, it is pertinent to consider viable choices and their ramifications.

%Indeed, normalization and symmetrization are themselves weighting schemes. 
%Indeed, it is known that the spectra of Laplacians reflects the geometric aspects of about intersection pattern 
%Thus, we will use the terms \emph{weight function} and \emph{scalar product} interchangeably. 

Unfortunately, various difficulties arise with weighting combinatorial Laplacians that aren't suffered by their integral counterparts, such as asymmetry, instability, and scale-dependence. 
Indeed, observe that in general neither terms in~\eqref{eq:weighted_up_laplace} are symmetric unless $W_p = I_n$ (for $L_p^{\text{up}}$) or $W_{p+1} = I_m$ (for $L_p^{\text{dn}}$). 
%$W_p^{-1} \partial_{p+1} W_{p+1} \partial_{p+1}^T$ 
However, as noted in~\cite{memoli2022persistent}, $L_p^{\text{up}}$ may be written as follows: 
\begin{equation}\label{eq:l_up}
	L_p^{\text{up}} = W_p^{-1} \partial_{p+1} W_{p+1} \partial_{p+1}^T  = W_p^{-1/2} \big( W_p^{-1/2}  \partial_{p+1} W_{p+1} \partial_{p+1}^T W_p^{-1/2}  \big ) W_p^{1/2} 
\end{equation}
Since ~\eqref{eq:l_up} is of the form $W^{-1} P W$ where $P \in S_n^+$ and $W$ is a positive diagonal matrix.
The same result holds for up-, down-, and combinatorial Laplacians, circumventing the issue of asymmetry.  
%Thus, we may retain the inner product interpretation from~\eqref{} while working with PSD operators---
%One shortcoming of the stability result shown above is that the Lipshitz constant depends explicitly on the magnitude of $f_\alpha$, which in-turn affects the stability of the proposed relaxation if $f_\alpha$ is scaled poorly, i.e. $\lambda \cdot f_\alpha$. 
%Moreover, the continuous differentiability of $\lVert \Phi_\epsilon(\cdot) \rVert_\ast$ only extends to linear operators defined over the positive semi-definite cone, and the explicit form of the differential $\partial \lVert \Phi_\epsilon(\cdot) \rVert_\ast$ depends on the domain of its corresponding spectral function $\phi(\cdot)$ being bounded to some interval $(a,b)$~\cite{bhatia2013matrix}.  
%
%Consider the $\mathcal{H}$-parameterized boundary matrix from Definition~\eqref{def:time_boundary_matrix}. Assume that the filter function $f : K \to \mathbb{R}_+$ is strictly positive. By taking the product $(\hat{\partial}_p^{i,j})(\hat{\partial}_p^{i,j})^T$ for some choice of $h \in \mathcal{H}$, we have:
%\begin{align*}\label{eq:param_up_lap}
%	\hat{L}_p^{\text{up}}(K, f) &= (\hat{\partial}_p^{i,j})(\hat{\partial}_p^{i,j})^T \\
%	&= (V_p^i)^{+} \circ \partial_p \circ (W_{p+1}^j)^2 \circ \partial_p^T \circ (V_p^i)^{+} \\
%	&\propto (V_p^i)^{+/2} \circ \partial_p \circ W_{p+1}^j \circ \partial_p^T \circ (V_p^i)^{+/2}  \numberthis
%\end{align*}
%where $A^{+/2} = (A^{1/2})^+$. 

A second difficulty that arises with both the symmetric and asymmetric forms of~\eqref{eq:l_up} is the problem of unbounded spectra. It is not difficult to see that the inverse terms in~\eqref{} drive $\lVert L_p^{\textrm{up}}\rVert_2$ upwards indefinitely as $f(\sigma) \to 0^{+}$, which by extension brings instability to the spectra. Towards rectifying this, Horek and Jost~\cite{} propose normalizing $\Delta_p$ by way of setting the weight function: 
\begin{equation}
	f(\tau) = \mathrm{deg}(\tau) \quad \forall \tau \in K^{p}
\end{equation}
Substituting these degrees as weights for the $p$-simplices yields the \emph{normalized combinatorial Laplacian}
\begin{equation}\label{eq:normalized_up_lap}
	 \mathcal{L}_p^{\text{up}}(K, f) = (\mathcal{D}_p)^{+/2} \partial_p W_{p+1} \partial_p^T (\mathcal{D}_p)^{+/2} 
\end{equation}
The benefits of this normalized is that we have $\Lambda( \mathcal{L}_p^{\text{up}}) \subseteq [0,p+2]$ for any choice of weight function, which lends stability to the spectrum. As the other operators mentioned, the nullity is not affected by this normalization.
Numerous other properties and implications follow from this normalization, such as tight bounds on the spectral norm and explicit connections between dual graphs.  
%By choosing the weights, one effectively determines the corresponding inner product, which in-turn determines the spectral range of the corresponding operator.
%We expect the spectra of $\hat{L}_p^{\text{up}}$ to obey many of the properties enjoyed by $L_p^{\text{up}}$ and its variants. 
%Indeed, though the spectrum of $\hat{L}_p^{\text{up}}$ is unbounded for general choices of $f$, we show that the normalized up-Laplacian given by:
%\begin{equation}\label{eq:param_normalized_up_lap}
%	 \hat{\mathcal{L}}_p^{\text{up}}(K, f) = \mathcal{D}_p(\mathcal{S}_i \circ f_\alpha)^{+/2} \circ \partial_p \circ W_{p+1}(\mathcal{S}_j \circ f_\alpha) \circ \partial_p^T \circ (\mathcal{D}_p^i)^{+/2} 
%\end{equation}
%has its spectrum bounded in the interval $[0, p+2]$, where $\tilde{D}_p^i$ corresponds to the weighted degree matrix with entries $\{ \mathrm{deg}_w(\sigma) \}$. We defer discussion of the exact forms of these matrices, including the computational details the matrix-vector product $x \mapsto \hat{L}_p^{\text{up}}$, to section~\eqref{sec:comb_lap}. 



%\begin{remark}
%~\cite{} studied a persistent version of the~\eqref{}, a \emph{persistent laplacian}, whose nullity yields the persistent Betti number. Additionally, satisfies many attractive qualities one would want out of a laplacian, including multiplicity of its 0th eigenvalue, disjoint spectra of disjoint components, and connections to notions of effective resistance. 
%Thus, one would hope there to be a $p$-th persistent version of equation~\eqref{eq:laplace_hom} whose form is akin to ~\eqref{}, however as~\cite{} showed, the matrix representation. 
%\end{remark}

%In this effort, we consider a very general kind of combinatorial Laplacian. GHere, we define the combinatorial $p$-th weighted up-Laplacian of the triple $(w_{p}^l, w_{p+1}, w_{p}^r)$ by: 
%\begin{equation}\label{eq:weighted_up_laplace}
%	L_p^{\mathrm{up}}(K) := W_p^l \circ \partial_{p+1} \circ W_{p+1} \circ \partial_{p+1}^T \circ W_p^r
%\end{equation} 
%where $W_p^\ast = \mathrm{diag}(\{ w_i \})$ is a diagonal weight matrix whose corresponding weights are given by the appropriate function of $(w_{p}^l, w_{p+1}, w_{p}^r)$, respectively. 
%Setting all weight functions to the identity yields the combinatorial Laplacian~\cite{}. 
%When $p = 0$ and $K = (V,E)$ is a graph, setting the diagonal entries of $W_1$ to edge weights and $W_0^l = W_0^r = I$ yields Kirchoff's \emph{weighted graph Laplacian}. 
%Alternatively, setting $W_0^l[i,i] = 1/\mathrm{deg}(v_i)$ and $W_0^r = I$ yields the \emph{normalized graph Laplacian} studied by Chung~\cite{}.
%Setting $W_p^l[i,i] = w_{p}^l(\sigma_i)^{-1}$ for all $\sigma \in K^p$ and $W_p^r = I$ yields the \emph{up Laplacian} from~\cite{}.
%\begin{align*}\label{eq:up_laplace_2}
%	\tilde{L}_p^{\mathrm{up}} &= \partial_{p+1} W_{p+1} \partial_{p+1}^T W_p^{-1}  \\
%	&= W_p^{1/2} \left ( W_p^{-1/2}  \partial_{p+1} W_{p+1} \partial_{p+1}^T   W_p^{-\frac{1}{2}} \right ) W_p^{-\frac{1}{2}} \numberthis
%\end{align*}
%Note that $\tilde{L}_p^{\mathrm{up}}$ is not be symmetric matrix in general. However, like the vertex-weighted graph laplacian from~\cite{}, the expression from~\eqref{eq:up_laplace_2} is of the form $W^{-1} P W$ where $P$ is symmetric positive semi-definite and $W$ is a positive diagonal matrix. 

\subsubsection*{Stability}
One disadvantage of rank functions restricted to subsets of the real-plane is that they are integer-valued and unstable. One may easily construct examples of $\mathcal{A}$-parameterized filtrations 
$(K, f_\alpha)$ where $\lVert \mu_p^R(\alpha) - \mu_{p}^R(\alpha + \delta) \rVert \sim O(\lvert K_p \rvert )$ for some arbitrarily small $\delta > 0$, as there may be up to $O( \lvert K_p \rvert)$ points in $\mathrm{dgm}_p(K)$.
%Intuitively, since both counting invariants are $0$ outside of the portions of $\Delta_+$ they restrict  too, it's always possible to encounter situations where small changes in the input affect the corresponding invariant in a non-Lipshitz way. 
%Relaxing $\mu_p^R \mapsto \hat{\mu}_p^R$ fixes this instability when $\epsilon > 0$, though the Lipshitz constant may be arbitrarily high.
In what follows, we investigate how to exploit the smoothness of $f_\alpha$ to stabilize the constitutive terms in $\hat{\mu}_p^\ast$ and $\hat{\beta}_p^{\ast}$. 

A common way of quantifying the sensitivity of the spectrum of a given linear operator $M$ is through its condition number. For $M = X X^T$ a given positive definite matrix, its \emph{condition number} $\kappa(M)$ is defined as: 
\begin{equation}
	\kappa(M) = \lVert M^{-1} \rVert \lVert M \rVert = \lvert \lambda_1(M) \rvert \; / \; \lvert \lambda_n(M) \rvert
\end{equation}
The condition number $\kappa(M)$ directly measures of how sensitive the spectrum of $M$ is too perturbations in its entries. 
%If $M$ is singular, then $\kappa(M)$ is infinite. 
In particular, if $E \in \mathbb{R}^{n \times n}$ represents a small perturbation of $M \in \mathbb{R}^{n \times n}$, then: 
\begin{equation}
\frac{\lVert (M + E)^{-1}  - M^{-1} \rVert}{\lVert M^{-1} \rVert} \leq \kappa(M) \frac{\lVert E \rVert}{ \lVert M \rVert }
\end{equation}
Thus, the effect of adding $\epsilon I_n$ to a given matrix can be interpreted as a means of reducing $\kappa$ arbitrarily---at the expense of accuracy---to stabilize the pseudo-inverse. 
For operators $\Phi_\epsilon(\cdot)$ in the form above, we can quantify this stabilization using perturbation analysis. 
\begin{proposition}
	TODO stability statement
\end{proposition}  


\section{Computational Implications}\label{sec:computational_imp}
%\subsection*{An quadratic-time rank computation}
In this section, we discuss computational advantages that stem from replacing the counting invariants with their spectral-based approximations from definition~\ref{def:smooth_mu}.
As the spectral relaxations are defined completely in terms of singular values of boundary operators---which can be obtained are the square roots of eigenvalues of an appropriate Laplacian operator---we focus on spectral methods specialized for symmetric positive semi-definite operators.
In particular, we focus on \emph{iterative} methods which only require matrix-vector products as their primary operations for estimating the spectrum, as this enables us to decouple the simplicial representation of the  Laplacian operator from its explicit matrix representation.

In the following sections, we (1) recall the method of minimized iterations in section~\ref{sec:lanczos_it}, (2) explore the efficiencies and subtleties of simplicial Laplacian $\mathtt{matvec}$ operators, and (3) discuss the modern advances to estimating spectral quantities of combinatorial Laplacians efficiently, such as preconditioning methods and convergence rates. 

%In particular, combinatorial Laplacians form a subset of the very important class of diagonally dominant (DD) matrices. % combinatorial number system

% Benefits of rank(A) = < non-zero eigenvalues >
\subsection{The Lanczos iteration}\label{sec:lanczos_it}
% Summarized from "The Lanczos Method: Evolution and Application" 
For a real, square matrix $A$ of order $n$, the quadratic form $x^T A x$ defines a continuous real-valued function of $x \in \mathbb{R}^n$. When $A$ is symmetric, each eigenvalue $\lambda$ satisfying $A v = \lambda v$ is real-valued and each pair of eigenvectors $v, v' \in \mathbb{R}^n$ satisfying $\lambda \neq \lambda'$ is orthogonal.  
% https://netlib.org/lanczos/vol2/Chp_2_RealSymmetric.pdf
Thus, we may reveal the spectrum $\Lambda(A)$ of $A$ via orthogonal diagonalization:
%by constructing an orthonormal eigenvector basis $V$ of $A$,
\begin{equation}\label{eq:eigen_decomp}
	A = V \Lambda V^T = \sum\limits_{i=1}^n \lambda_i v_i v_i^T
\end{equation}
% Insert similarity transforms  
Factorizing $A$ as in~\eqref{eq:eigen_decomp} is known as the \emph{symmetric eigenvalue problem}. 
Computing the eigen decompositions of symmetric matrices generally consists of two phases: (1) reduction to tridiagonal form $Q^T A Q = T$ via orthogonal similarity transformations $Q$, and (2) diagonalization of the tridiagonal form $T = Y \Theta Y^T$. 
% For phase (2), also see: https://www.cs.utexas.edu/users/flame/laff/alaff-beta/chapter10-francis-implicit-qr-step.html
While the latter may be performed in $O(n \log n)$ time~\cite{gu1995divide}, the former is effectively bounded below by $\Omega(n^3)$ for dense full-rank matrices using non-Strassen-like operations, and thus it is the reduction to tridiagonal form that dominates the computation. 
Lanczos~\cite{lanczos1950iteration} proposed the \emph{method of minimized iterations}---now known as the \emph{Lanczos method}---as an attractive alternative for reducing $A$ into a tridiagonal form and thus revealing its spectrum. As it is the key iterative method we exploit in this effort, we review it below. 
% Mention reflectors destroy sparsity 
% orthogonally diagonalizable 
%The eigen-decomposition of a symmetric $A$ not only produces an orthogonal basis for $A$, but for every $1 \leq r \leq \mathrm{rank}(A)$, the matrix $A_{(r)} = V_r \, \mathrm{diag}(\{\sigma_1, \dots, \sigma_r\}) \, V^T_r$ is the closest matrix to $A$ in the Frobenius norm: 
%\begin{equation}
%	A_{(r)} = \argmin_{B \in \mathbb{R}^{n \times n}}\lVert A - B \rVert_F
%\end{equation}
% History of Lanczos 
%The success of iterative methods at extracting spectral-information from large, sparse matrices in the past three decades is unprecedented. Core to these methods is what is known today as the Lanczos method~\cite{}, or as Lanczos called it, the \emph{method of minimized iterations}. Although discovered before the widely successful direct methods (Householder transformations, QR factorization, etc.), the Lanczos algorithm became the staple for applications where the underlying decomposition of system was too large to fit into memory. 
%We review the method briefly in what follows.
%reduces $A$ to tridiagonal form via a series of orthogonal similarity transformations
 
The means by which the Lanczos method estimates eigenvalues is by projecting onto successive Krylov subspaces. Given a large, sparse, symmetric $n \times n$ matrix $A$ with eigenvalues $\lambda_1 \geq \lambda_2 > \dots \geq \lambda_r > 0$ and a vector $v \neq 0$, the order-$j$ Krylov subspaces of the pair $(A, v)$ are the spaces spanned by: 
\begin{equation}
	\mathcal{K}_j(A, v) := \mathrm{span}\{ v, Av, A^2 v, \dots, A^{j-1}v \} = \mathrm{range}(K_j(A, v))
\end{equation}
where $K_j(A, v) = [ v \mid Av \mid A^2 v \mid \dots \mid A^{j-1}v]$ are their corresponding Krylov matrices. 
% The utility of Krylov subspace methods is motivated by the fact that the minimal polynomial of $A$ can be used to express $A^{-1}$ in terms of powers of $A$.
Krylov subspaces arise naturally from using the minimal polynomial of $A$ to express $A^{-1}$ in terms of powers of $A$. In particular, if $A$ is nonsingular and its minimal polynomial has degree $m$, then $A^{-1}v \in K_m(A, v)$ and $K_m(A, v)$ is an invariant subspace\footnote{Recall that if $S \subseteq \mathbb{R}^n$, then $S$ is called an \emph{invariant subspace} of $A$ or $A$-\emph{invariant} iff $x \in A \implies Ax \in S$ for all $x \in S$.} of $A$.
% "Krylov subspace methods for shifted unitary matrices and eigenvalue deflation applied to the Neuberger Operator and the matrix sign function"
%A non-trivial fact that motivates the use of Krylov subspaces is that subspace $K_n(A, v)$ is the smallest $A$-invariant space that contains $v$. 
%The vectors $A^j v$ are proportional to those obtained through power iteration and tend to be good approximations to the eigenvectors associated with the largest eigenvalues.
Since $A$ is symmetric, the spectral theorem implies that $A$ is orthogonally diagonalizable and that we may obtain $\Lambda(A)$ by generating an orthonormal basis for $\mathcal{K}_n(A, v)$. 
To do this, the Lanczos method constructs successive QR factorizations of $K_j(A,v) = Q_j R_j$ for each $j = 1, 2, \dots, n$.
Due to $A$'s symmetry and the orthogonality of $Q_j$, the identity $q_k^T A q_l = q_l^T A^T q_k = 0$ is satisfied for all $k > l + 1$, giving the corresponding $T_j = Q_j^T A Q_j$ a tridiagonal structure:
\begin{equation}
	T_j = \begin{bmatrix} 
	\alpha_1 & \beta_1 & & & \\
	\beta_1 & \alpha_2 & \beta_2 & & \\
	 & \beta_2 & \alpha_3 & \ddots & \\
	& & \ddots & \ddots & \beta_{j-1} \\
	& & & \beta_{j-1} & \alpha_{j} 
	\end{bmatrix}, \; \beta_j > 0, \; j = 1, 2, \dots, n
\end{equation}
Unlike the spectral decomposition $A = V \Lambda V^T$---which identifies a diagonalizable $A$ with its spectrum $\Lambda(A)$ up to a change of basis $A \mapsto M^{-1} A M$---there is no canonical choice of $T_j$ due to the arbitrary choice of $v$. 
%which unitarily diagonalizes $A$ into a form such that any other decomposition $A = \tilde{U}\Lambda\tilde{U}^T$ is related by a similarity transform, 
However, there is a connection between the iterates $K_j(A,v)$ and the full tridiagonalization of $A$: if $Q^T A Q = T$ is tridiagonal and $Q= [\, q_1 \mid q_2 \mid \dots \mid q_n \,]$ is an $n \times n$ orthogonal matrix $Q Q^T = I_n = [e_1, e_2, \dots, e_n]$, then:
\begin{equation}
	K_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \, e_1 \mid T e_1 \mid T^2 e_1 \mid \dots \mid T^{n-1} e_1 \, ]
\end{equation}
is the QR factorization of $K_n(A, q_1)$---that is, tridiagonalizing $A$ with respect to a unit-norm $q_1$ determines $Q$. 
%that is, $Q$ may be generated completely by tridiagonalizing $A$ with respect to an orthogonal matrix $Q$ whose first column vector is $q_1$. 
Indeed, the Implicit Q Theorem~\cite{golub2013matrix} asserts that if an upper Hessenburg matrix $T \in \mathbb{R}^{n \times n}$ has only positive elements on its first subdiagonal and there exists an orthogonal matrix $Q$ such that $Q^T A Q = T$, then $Q$ and $T$ are \emph{uniquely} determined by $(A, q_1)$. 
As a result, given an initial pair $(A, q_1)$ satisfying $\lVert q_1 \rVert = 1$, we may restrict and project $A$ to its $j$-th Krylov subspace $T_j$ via: 
\begin{equation}\label{eq:krylov_proj}
	A Q_j = Q_j T_j + \beta_j q_{j\+1} e_{j}^T \quad\quad (\beta_j > 0)
\end{equation}
where $Q_j = [\, q_1 \mid q_2 \mid \dots \mid q_j \,]$ is an orthonormal set of vectors mutually orthogonal to $q_{j\+1}$.
%$q_{j+1} \in \mathbb{R}^n$ is a unit vector satisfying $q_{j+1} \perp \{ q_1, q_2, \dots, q_j \}$, 
Equating the $j$-th columns on each side of~\eqref{eq:krylov_proj} and rearranging the terms yields the \emph{three-term recurrence}: 
\begin{equation}\label{eq:three_term_rec}
	 \beta_{j} \, q_{j+1} = A q_j - \alpha_j \, q_j - \beta_{j\text{-}1} \, q_{j\text{-}1}  
\end{equation}
where $\alpha_j = q_j^T A q_j$, $\beta_j = \lVert r_j \rVert_2$, $r_j = (A - \alpha_j I)q_j - \beta_{j\text{-}1} q_j$, and $q_{j+1} = r_j / \beta_j$. 
% See (3.3) "Connections between Lanczos iteration and orthogonal polynomials"
Equation~\eqref{eq:three_term_rec} is a variable-coefficient second-order linear difference equation, and it is a known fact that such equations have unique solutions: if ($q_{j\text{-}1}, \beta_j, q_j$) are known, then ($\alpha_j$, $\beta_{j+1}, q_{j+1}$) are completely determined. 
% One may begin the iteration by setting $\beta_0$ and $q_0$ to $0$, and $q_1$ arbitrarily. 
The sequential process that iteratively builds $T_j$ via the recurrence from~\eqref{eq:three_term_rec} is called the \emph{Lanczos iteration}. 
Note that if $A$ is singular and we encounter $\beta_j = 0$ for some $j < n$, then $\mathrm{range}(Q_j) = \mathcal{K}_j(A, q_1)$ is an $A$-invariant subspace, the iteration stops, and we have solved the symmetric eigenvalue problem~\eqref{eq:eigen_decomp}: $\Lambda(T_j) = \Lambda(A)$, $j = \mathrm{rank}(A)$, and $T_j$ is orthogonally similar to $A$. 
% The spectrum is indeed identical when j = rank(A), see e.g. An Explicit Formula for Lanczos Polynomials
%This is called a projection because each $q_{j+1} \perp \{q_1, q_2, \dots, q_j \}$, and~\eqref{eq:krylov_proj} can be thought of as successive orthogonalization, 

 The Lanczos iteration and its many variants are part of a family of so-called ``matrix free'' methods---obtaining an eigen-decomposition of a symmetric real matrix $A$ requires only a matrix-vector product operator $v \mapsto Av$. 
Indeed, since $A$ is not modified during the computation, the entire iteration may be carried out without explicitly storing $A$ in memory. 
Moreover, the three-term recurrence from~\eqref{eq:three_term_rec} implies each iteration requires just three $O(n)$-sized vectors and a few $O(n)$ vector operations, justifying the following proposition:
\begin{proposition}[\cite{parlett1994we, simon1984analysis}]\label{lemma:exact_arith_lanczos}
	Given a symmetric rank-$r$ matrix $A \in \mathbb{R}^{n \times n}$ whose matrix-vector operator $A \mapsto A x$ requires $O(\eta)$ time and $O(\nu)$ space, the Lanczos iteration computes $\Lambda(A) = \{ \lambda_1, \lambda_2, \dots, \lambda_r \}$ in $O(\max\{\eta, n\}\cdot r)$ time and $O(\max\{\nu, n\})$ space, when computation is done in exact arithmetic. 
\end{proposition}
\noindent As in~\cite{parlett1994we}, the assumption of exact arithmetic simplifies both the presentation of the theory and the corresponding complexity statements. 
Although this assumption is unrealistic in practical settings, it gives us a grounded expectation of what is possible to achieve with any \emph{finite-precision} algorithm based on the Lanczos method.
\begin{corollary}\label{cor:finite_arith_lanczos}
	Given the same inputs as Lemma~\ref{lemma:exact_arith_lanczos}, any implementation that computes $\Lambda(A) = \{ \lambda_1, \lambda_2, \dots, \lambda_r \}$ using the Lanczos iteration in finite-precision arithmetic requires $\Omega(\max\{\eta, n\} \cdot r)$ time and $\Omega(\max\{\nu, n\})$ space complexity. 
\end{corollary}
\noindent
In practice, finite-precision arithmetic introduces both rounding and cancellation errors into the computation, which manifests as loss of orthogonality between the Lanczos vectors. These errors not only affect the methods convergence rate towards an invariant subspace, but in fact they muddle the termination condition entirely. 
As a result, several decades of research have been dedicated to developing orthogonality-enforcement schemes that retain the simplicity of the Lanczos iteration without increasing either the time or space complexities by non-trivial factors. 
As these extensions are complex, multifaceted, and beyond the scope of the present work, we defer their discussion to the appendix~\ref{subsec:rounding} and refer the curious reader to~\cite{golub2013matrix, parlett1994we, simon1984analysis} and references therein for an overview. 

%Independent of orthogonality-enforcement scheme of choice,

%As Lemma~\ref{lemma:exact_arith_lanczos} makes clear, the usefulness of the Lanczos iteration hinges on the availability of a fast matrix-vector product. 
%In the next section, we demonstrate how to reduce this complexity using the structure of the boundary operator. 
%The Lanczos iteration's widespread adoption is due in part to its computational simplicity at reaching tridiagonal form: each step consists of a single matrix-vector product and a few vector operations.  
%Thus, if $A$ is sparse and/or has special structure, the iteration may be carried in far fewer than $\approx n^3$ operations and with far less than the $\approx n^2$ memory---the typical time and storage complexities exhibited by direct methods. 
% The Rayleigh-Ritz properties are preserved under semi-orthogonality w_j \approx \sqrt{\epsilon}
 %% , then the Lanczos method can be quite efficient. 
%% Whereas direct methods typically require $O(n^3)$ operations before any output is derived, 
%% such as those tridiagonalizations that destroy the sparsity structure of the matrix that Householder reflectors.

% ----- Laplacian's encoding geometry: https://arxiv.org/pdf/1105.2712.pdf ----
% "In terms of the spectrum, they are given by the dimensions of the eigensets for the eigenvalue 0. This is the same for all the Laplace operators investigated here. These operators, however, differ in the nonzero part of the spectrum, and thereby encode specific combinatorial or geometric features of a (perhaps weighted) simplicial complex in addition to its topological aspects...many combinatorial operations that one can perform on a simplicial complex do not affect its homology; nevertheless, they typically leave characteristic traces in the spectrum of a suitable Laplace operator, and that is what we are trying to explore... in the weighted case, there is additional geometric information that likewise influences the spectrum"

\subsection{The combinatorial Laplacian \texttt{matvec}}\label{sec:comb_lap}
The efficiency of the Lanczos method depends crucially on the existence of the three-term recurrence and a fast matrix-vector product. 
The former arises in the decomposition of symmetric matrices while the latter on the structure of $A$. 
For general symmetric matrices $A \in \mathbb{R}^{n \times n}$, Lanczos requires $O(n\nu r)$ operations per iteration when $A$ has an average of $\nu$ nonzeros per row~\cite{golub2013matrix}. 
This is markedly improved when $A$ is a graph Laplacian $L = \partial_1 \partial_1^T$: the complexity of the $x \mapsto L x$ operation is linear in $\lvert E \rvert$, and $L$ need not be explicitly constructed.
However, it is not immediately clear whether these results generalize to Laplacian operators derived from simplicial complexes.
%Without loss of generality, we exclusively consider the symmetric version of $L_p^{\mathrm{up}}$; the down and combinatorial Laplacians can be handled in essentially an identical way. 
%We examine these nuanced differences more in detail.

% and as previously mentioned, the Lanczos iteration is only efficient if one as has access to a fast matrix-vector product.  By our definition of rank in~\eqref{eq:rank_def}, this implies $\mathrm{rank}(\partial) = \mathrm{rank}(\partial \partial^T) = \mathrm{rank}(\partial^T \partial)$ and thus the Lanczos method may be readily applied to $\partial \partial^T$ or $\partial^T \partial$. % up to a change in the multiplicity of the zero 
%since the latter are by definition the nonnegative square roots of former and thus . 

We first recall the characteristics of the graph Laplacian. 
Given a simple undirected graph $G = (V, E)$, let $A \in \{0,1\}^{n \times n}$ denote its binary adjacency matrix satisfying $A[i,j] = 1$ if the vertices $v_i,v_j \in V$ are path-connected $i \sim j$ in $G$ and denote with $D = \mathrm{diag}(\{ \, \mathrm{deg}(v_i) \, \})$ the diagonal \emph{degree} matrix, where $\mathrm{deg}(v_i) = \sum_{j \neq i} A[i,j]$.
The \emph{graph Laplacian}'s adjacency, incidence, and element-wise definitions are: 
\begin{equation}
L = D - A = \partial_1 \circ \partial_1^T \, , \quad\quad
	L\,[i,j] = \begin{cases}
		\mathrm{deg}(v_i) & \text{ if } i = j \\
		-1 & \text{ if } i \sim j \\
		0 & \text{ if } i \nsim j
	\end{cases}
\end{equation}
It is well known that $L$ is symmetric, positive semi-definite, and has a combinatorial structure that captures the connectivity structure of $G$~\cite{newman2001laplacian}. 
Moreover, as in~\cite{chung1997spectral}, the linear and quadratic forms of $L$ may be succinctly expressed using the path-connected relation $i \sim j$, shown below:
\begin{flalign}\label{eq:lap_quad_from}
	(\, \forall \, x \in \mathbb{R}^n \,)  & & \quad\quad\quad 
	(Lx)_i = \mathrm{deg}(v_i) \cdot x_i - \sum\limits_{i \sim j} x_j \, , \quad \quad &
	 x^T L x = \sum\limits_{i \sim j} (x_i - x_j)^2  & &&
%	  x^T L x = \sum\limits_{i \sim j} f(v_i, v_j) (x_i - x_j)^2  & &&
\end{flalign}
If $G$ has $m$ edges and $n$ vertices taking labels in the set $[n]$, computing the matrix-vector product from~\eqref{eq:lap_quad_from} requires just $O(m)$ time and $O(n)$ storage via two edge traversals: one to accumulate vertex degrees and one to remove components from incident edges. By precomputing the degrees, the operation can be reduced further to a single $O(n)$ product and $O(m)$ edge pass, which is useful when repeated evaluation is necessary. 
%As we show below, generalizing this procedure when $p > 0$ requires .
%, as the orientations of $[\sigma] \in K^{p+1}$ change the sign of the off-diagonal entries in $L_p^\ast$. 
%In particular, writing down a succinct expression for $x \mapsto L_p^{\textrm{up}} x$ (or $x \mapsto L_p^{\textrm{dn}} x$) requires a generalization of the notion of path-connectedness between $p$-simplices, as well as knowledge of the orientation of every simplex to evaluate the sign..

To extend the two-pass algorithm outlined above when $p > 0$, we first require a generalization of the path-connected relation from~\eqref{eq:lap_quad_from}.
 Denote with $\mathrm{co}(\tau) = \{ \, \sigma \in K^{p+1} \mid \tau \subset \sigma \, \}$ the set of proper cofaces of $\tau \in K^p$, or \emph{cofacets}, and the (weighted) \emph{degree} of $\tau \in K^p$ with: 
$$\mathrm{deg}_f(\tau) = \sum_{\sigma \in \mathrm{co}(\tau)} f(\sigma) $$
Setting $f(\sigma) = 1$ for all $\sigma \in K$ recovers the integral notion of degree representing the number of cofacets a given $p$-simplex has. 
Since $K$ is a simplicial complex, if the faces $\tau, \tau'$ share a common cofacet $\sigma \in K^{p+1}$, this cofacet $\{\sigma\} = \mathrm{co}(\tau) \cap \mathrm{co}(\tau')$ is in fact \emph{unique}~\cite{goldberg2002combinatorial}. 
Thus, we may use a relation $\tau \overset{\sigma}{\sim} \tau'$ to rewrite the operator from~\eqref{eq:l_up} element-wise: 
\begin{align}\label{eq:up_laplace_theory}
	 L_p^{\text{up}}(\tau, \tau')= \begin{cases}
		 \mathrm{deg}_f(\tau) \cdot f^{+}(\tau) & \text{ if } \tau = \tau' \\ 
%		\mathrm{deg}(\tau_i) & \text{ if } i = j \\ 
		s_{\tau, \tau'} \cdot  f^{+/2}(\tau) \cdot f(\sigma) \cdot f^{+/2}(\tau') & \text{ if } \tau \overset{\sigma}{\sim} \tau' \\
		0 & \text{ otherwise} 
	\end{cases}
\end{align}
where $s_{\tau, \tau'} = \mathrm{sgn}([\tau], \partial[\sigma]) \, \cdot \, \mathrm{sgn}([\tau], \partial[\sigma])$. Ordering the $p$-faces $\tau \in K^p$ along a total order and choosing an indexing function $h : K^p \to [n]$ enables explicit computation of the corresponding matrix-vector product: 
\begin{equation}\label{eq:l_up_matvec}
	(L_p^{\textrm{up}} \, x)_i =  \mathrm{deg}_f(\tau) \cdot f(\tau) \cdot x_i + \sum\limits_{\tau \overset{\sigma}{\sim} \tau'} s_{\tau, \tau'} \cdot x_{h(\tau')} \cdot  f^{+/2}(\tau) \cdot f(\sigma) \cdot f^{+/2}(\tau') 
\end{equation}
Observe~\eqref{eq:l_up_matvec} can be evaluated now via a very similar two-pass algorithm as described for the graph Laplacian if the simplices of $K^{p+1}$ can be enumerated quickly and the indexing function $h$ can be efficiently evaluated. We summarize this with a proposition. 
%Since a $p+1$-simplex has $p$ proper faces in its boundary, and we need to iterate through the $p+1$ simplices $\sigma \in K^{p+1}$ just once in the case of the $\mathrm{up}$-Laplacian,
%We combine the above observations into a proposition. 
\begin{proposition}
	For any $p \geq 0$ and simplicial pair $(K, f)$, if there exists an indexing function $h: K^p \to [n]$ with $O(k)$ access time and $O(c)$ storage, then there exists a two-phase algorithm for computing the product: 
	\begin{equation}
		L_p^{\mathrm{up}} x = (W_p \circ \partial_{p+1} \circ W_{p+1} \circ \partial_{p+1}^T \circ W_p)x
	\end{equation}
	in $O(mk(p+1))$ time and $O(\mathrm{max}(c,m))$ storage, where $n = \lvert K^p \rvert$ and $m = \lvert K^{p+1} \rvert$. 
	%Moreover, the matrix-vector product may be implemented \emph{efficiently} in the sense that the memory access pattern to $K^{p+1}$ can easily be made cache-optimal. 
	%If $K$ is sufficiently dense such that $m \sim O(\lvert V \rvert^{p+1})$, we may accelerate the computation via~\cite{}. 
\end{proposition}
\noindent From a practical perspective, many hash table implementations achieve expected $O(1)$ access time using only a linear amount of storage, and as $p \geq 0$ is typically quite small---typically no more than $2$---in practice the operation $x \mapsto L x$ tends to exhibit $\approx O(m)$ time and $\approx O(m)$ storage complexities. 
As more concrete statements about the computation require moving beyond asymptotic analysis, we delegate the practical details---along with pseudocode of the two-phase algorithm---to appendix~\ref{sec:up_laplace_matvec}, for the curious reader.

%There are multiple optimizations we may make in the evaluation of~\eqref{eq:l_up_matvec} over the traditional sparse matrix multiplication---some due to the simplicial structure of the underlying operator, and some due to the permutation invariance mentioned in the previous section. 

\subsection{Computing diagrams}\label{sec:pers_alg}
The direct methods to computing a $p$-th persistence diagram of a simplicial complex $K$ of size $\lvert N \rvert = N$ typically have complexity $O(N^3)$ time and $O(N^2)$ storage complexity, respectively. In fact, these bounds are tight $\Theta(N^3)$ on certain pathological inputs~\cite{}; in practice the matrices involved in the computation are much 


%Since we are free to choose the permutation of $\hat{L}_P^{\text{up}}$ ahead of time, we may order the $p+1$ simplices $\{\sigma\}$ in such a way as to minimize the latency associated with memory access to $K^{p+1}$.
%the study of \emph{block-based} or \emph{cache-oblivious} approaches are well studied topic in the high performance computing field~\cite{}.
%Another optimization known to the TDA community due to its usage in the persistence algorithm is the integer-representation of simplices via the \emph{combinatorial number system}. 

%Suppose we have access to an efficient way of enumerating 

%\subsection{Practical preconditioning \& convergence rates}\label{sec:precon}

%\subsection{Effective Resistance \& The Laplacian Paradigm}

%\subsubsection*{Effective Resistance \& The Laplacian Paradigm}
%We now turn to our main result. 




% https://www.cs.wm.edu/~andreas/publications/optimal_many.pdf
% Newton steps, RQI, etc 

% TODO: Convergence rates, SDD+ "the Laplacian Paradigm", the persistent Betti Number complexity 
 
%As one can see from~\eqref{eq:oriented_simplex}, there are effectively only two classes of orientations, and thus the position of $[\tau]$ as a summand in $\partial[\sigma]$, modulo 2.
%However, notice that the signs of the coefficients of $(\tau, \tau')$ in $\partial[\sigma]$ do not depend on the orientation of $[\sigma]$. 
%Thus, we may evaluate the  


%It may be shown that such a coface containing two distinct faces $\tau, \tau' \in K^p$ is unique~\cite{}, and thus we refer to this simplex $\sigma$ as the \emph{common upper simplex} of $(\tau, \tau')$.   

%% --- Hodge theorem
%A discrete version of the Hodge theorem, due to Eckmann~\cite{}, shows that: 
%$$ H_p(K; \mathbb{R}) \simeq \mathrm{Null}(\Delta_p) $$
%Since $\partial_{p} \partial_{p+1} = 0$ and $\partial_{p+1}^T \partial_p^T = 0$, we have $\mathrm{Im}(L_p^{\text{dn}}) \subset \mathrm{ker}(L_p^{\text{up}})$ and $\mathrm{Im}(L_p^{\text{up}}) \subset \mathrm{ker}(L_p^{\text{dn}})$, thus if $\lambda \neq 0$ is an eigenvalue of $\Delta_p(K)$, it must be an eigenvalue of $L_p^{\text{dn}}$ or $L_p^{\text{up}}$. Indeed, ~\cite{} showed that the pair $\Lambda(L_p^{\text{up}})$ and $\Lambda(L_p^{\text{dn}})$ determines $\Lambda(\Delta_p)$ and vice versa. 
%Thus, for each $p \geq 0$ the $p$-th Betti number of $K$ is given by $\beta_p = \mathrm{nullity}(\Delta_p)$.
%This result suggests the Laplacian a connection or alternative way to obtaining the persistent Betti number $\beta_p^{\ast}$ by first forming a persistent version of $\Delta_p(K)$ whose nullspace 

  
% Persistent laplacian 
% Shwo the sptrums are the same


%We consider weighted versions of the Laplacian whose weights are given by the filter function $f$. To make this clear, let $\mathcal{F} : K^p \to \mathbb{R}$ and $\mathcal{C} : K^{p+1} \to \mathbb{R}$ denote restrictions of $f$ to $p$ and $p+1$ dimensional faces of $K$. We 

%the connection between $L$ and $\partial_1$ via~\eqref{eq:L_form} suggests the elementary chain modifications from~\eqref{eq:param_boundary_matrix} can be adapted to the linear and quadratic forms from~\eqref{eq:lap_quad_from}. 
% Shwo the sptrums are the same



% It turns out that the residual error between $\theta$ and $\lambda$ can be easily computed, so we may assess the quality of $\Lambda(T_j)$ during the iteration.   

%In principle, if the Lanczos iteration doesn't break down due to rounding errors, then the characteristic polynomial of $T_k$ is the unique polynomial $p^k \in P^k$ that achieves~\cite{}:
%\begin{equation}
%	\lVert p^k(A)b \rVert = {minimum}
%\end{equation}
% the explicit formulas of the of the characteristic polynomials of $A_k$ are given in..., wherein the convergenence of the Lanczos procedure is studied more in detail.

% Thus, if $\mathrm{rank}(A) = r$, the eigenvalues of $T_k$ for some $k < r$ tend to converge quickly to the extremal eigenvalues of $A$. 
% Rayleigh 
% the explicit formulas of the of the characteristic polynomials of $A_k$ are given in...
% where $S_k^T T_k S_k = \mathrm{diag}\{\theta_1, \theta_2, \dots, \theta_k \}$ satisfying $T_k = \lVert A Q_k - Qk B \rVert_2$

% Still working on this part
% Observe each column of $\partial_p$ corresponds to a $p$-chain of the form~\eqref{eq:matrix_pchain}, which is a constant up to difference in sign. Thus, evaluating $y = \partial_p^T v$ reduces to computing the locations $(k_1, k_2, \dots, k_{p+1})$ (how to phrase this?):
%\begin{equation}
%	y_{i} = y_i \pm S_{i,j}(\sigma_k, \sigma_l)
%\end{equation}
%In particular, the operator $v \mapsto \partial_p \partial_p^T v$ need not be represented explicitly in memory: as long as we have access to the underlying simplicial complex $K$ and its corresponding filter function $f$, we may take advantage of the .
%To make this more precise, we start with a useful lemma:
%\begin{lemma}
%Let $K$ denote a simplicial complex with $(m, n)$ simplices of dimension $(p, p - 1)$. Assume $m > n$. For any fixed $p \geq 0$ and arbitrary vector $v \in \mathbb{R}^n$, the operation $v \mapsto \partial_p \partial_p^T v$ can be evaluated $O(m)$ time and $O(m)$ memory. 
%\end{lemma}

%\< TODO \> 
% For expository purposes, the preceding sections assumed exact arithmetic. 
% SDD
%The structure of $L_p = \partial_p \partial_p^T$ is actually amenable to more than just computational efficiency in terms of matrix-vector multiplication. A matrix $A = \{ a_{ij} \}$ is said to be \emph{diagonally dominant} (DD) if it satisfies: 
%\begin{equation}\label{eq:dd}
%a_{ii} \geq \sum\limits_{j \neq i} \, \lvert a_{ij} \rvert \,, \quad \forall \, i,j \in [n]
%\end{equation}  
%Similarly, when the inequality in~\eqref{eq:dd} is strict, $A$ is said to be \emph{strictly diagonally dominant} (SDD). 
%It is well known that for any $p \geq 0$, $L_p$ is the smallest diagonally dominant matrix in the sense that $L_p[i,i] = \sum\limits_{j \neq i} \lvert L_p[i,j] \rvert$. Diagonally dominant matrices are of particular interest becsue.... their spectrums can be approximated rather quickly. 


%Extending from this, corollary~\ref{} implies $p$-th PBN at index $(i,j) \in \Delta_{+}^{m}$ can be computed in... note that for any $p \geq 1$, this is significant reduction in complexity compared to e.g. the $O(m^3)$ reduction algorithm.
%Indeed, if $K$ has $n$ vertices, the $\Theta(m^3)$ complexity of the reduction algorithm implies computing the $p$-th persistence diagram requires reducing $\partial_{p+1}$; one deduces loose though asymptotically accurate bounds as $O(n^3(p+2))$ operations, i.e. $O(n^9)$ for $p = 1$, $O(n^{12})$ for $p = 2$, etc. In contrast, combining proposition~\ref{prop:exact_arith_matvec} with corollary~\ref{}, we have that the PBN at index $i,j$ may be compute in... a substantial difference indeed. 
%Obviously, the assumption of exact arithmetic is too strong to be of any practical use. Round-off errors plague the simple Lanczos iteration.  


%Thus far, we have discussed various ways of re-writing expressions of the PBN, and the advantages/disadvantages each form implies on the computational side. We now pivot to focus on the computation of the constitutive terms of these expressions themselves, again with a motivation of understanding the PBN computation in continuously parameterized settings. 

% start with the numerical rank? 

%For a square matrix of order $n$, the quadratic form: 
% \begin{equation}
% x^T A x = \sum\limits_{k=1}^n \sum\limits_{l=1}^n	a_{kl} x_k x_l 
% \end{equation}
%defines a continuous function in $x \in \mathbb{R}^n$. 
%$x = \left( x_i \right)_{i=1}^n$
% Spectral graph theory plug
% Due to its connection as the discrete analogue of the Laplace differential operator, the graph Laplacian and its spectrum have been studied in many areas of mathematical research as they often have particular geometric interpretations in physical chemistry, and financial mathematics~\cite{}. Since the investigation of the question ``can you hear the shape of a drum?'' by~\cite{}, the question of what geometric properties of a manifold are determined by its Laplace operator has been of pertinent interest to practitioners and theoreticians alike. 


%\begin{corollary}
%	For every matrix $$ any $\epsilon > 0$, the approximations $\beta_p$ and $\tilde{\mu}_p^\ast$ vary continuously in $\mathcal{H}$. Moreover, there exists some $\bar{\epsilon} > 0$ such that: 
%	$$ \lfloor \tilde{\beta}_p^{\ast}(K) \rfloor = $$
%\end{corollary}
%These approximations are also "betti-like". 

%% REVISIT LAPLACIAN!

% edge or vertex weighted 

% The stability of the spectral relaxation proposed in~\eqref{} lends some stability the computation of the Betti and Multiplicity computations we have discussed in parameterized setting. 


\newpage

\newpage

\section{Applications \& Experiments}\label{sec:applications}

% --- APPLICATION 1: Fast spectral computation ---
\subsection{Rank computation}
 The first and most general application of the work presented here is the matrix-free computation of persistent rank invariants in \emph{essentially} $O(n^2)$ time and $O(m)$ storage, where $n = \lvert K^p \rvert$ and $m = \lvert K^{p+1}\rvert$. 
 Our use of the word essentially stems from...
%Here, the primary difference between the persistent Betti number $\beta_p^{i,j}$ at index $(i,j) \in \Delta_+$ and the multiplicity function $\mu_p^{R}$ on some box $R=[i,j] \times [k,l]$ is that the former can capture essential homology classes (cycles which never die), whereas the multiplicity function by definition can only capture those classes whose persistence is bounded. 
%Our approach to computing persistence diagrams is to simply compute the multiplicities $\mu_p^{i,j}$ of all points $i,j \in \Delta_+$ following the algorithm from Chen \& Kerber~\cite{chen2011output}. The central idea of their approach is to compute the diagram by repeated multiplicity computations (``$\mu$-queries'') on the filtration boundary matrix $\partial$ in a divide-and-conquer like approach.  
%The first and most general application of the work presented here is the matrix-free computation of the spectra of various combinatorial Laplacian operators. 
%Since we may recover both the persistence diagram~\cite{chen2011output} and the rank invariants by repeated spectral computations, we focus examining the time and storage characteristics of Laplacian spectral computation itself rather than its corresponding persistence computations. 

We sampled $30$ random graphs according to the Watts-Strogatz~\cite{} rules with parameters $n=500, k=10, p=0.15$. These graphs tend to exhibit `small world' characteristics inherited by many real-world networks, such as social networks, gene networks, and transportation networks.  
For our purposes, since the graph distance between pairs of nodes scale logarithmically with the size of the graph, we ensure the sampled random graphs to be uniformly sparse. 
The corresponding incidence matrix $\partial_1 \in \mathbb{R}^{n \times m}$ and up-Laplacians $L_0^{\mathrm{up}} \in \mathbb{R}^{n \times n}$ would have $\approx 5,000$ and  $\approx 5,500$ non-zero entries, were they to be formed explicitly, which are weighted according randomly by embedding the graph in the plane and filtering  graph via its sublevel sets in a random direction. 
A much smaller Watts-Strogatz graph of the same type (but with only $50$ nodes) is shown on the left-side of figure~\ref{fig:watts_strogatz}, colored by the filtering of its lower stars. 
\begin{figure}[ht]
	\includegraphics[width=\textwidth]{presentation/watts_strogatz_perf.png}
	\caption{Random Watts-Strogatz ``Small world'' graph example}
	\label{fig:watts_strogatz}
\end{figure}
To test the scability of the laplacian operator studied here, we computed various percentages of the spectra of these $30$ graphs via iterative methods discussed in section~\ref{} and reported various of their time- and storage- related statistics in figure~\ref{fig:watts_strogatz}. 
All statistics reported are the average statistics collected from all 30 random graphs, which were collected using  various iterative methods implemented the PRIMME software~\cite{}. 
On the far left of figure~\ref{fig:watts_strogatz}, we display a random metric embedding of a small Watts-Strogatz graph to convey the structure of the type of graphs we consider. 

\textbf{Storage requirements}: On the left side of figure~\ref{fig:watts_strogatz} next to the example network model, we record the ratio of $\mathtt{matvec}$ operations (relative to $n$) needed to compute $p\%$ of the spectrum as a function of the maximum number basis vectors kept in-memory for reorthogonalization purposes. 
The ideal Lanczos method needs just $3$ such vectors in exact arithmetic due to the three-term recurrence, justifying the space complexity record in~\ref{lemma:exact_arith_lanczos}; in contrast, with finite-precision arithmetic, one needs additional basis vector to ensure the orthonormality of the eigenvectors to machine precision.
Each additional basis vector simultaneously increases both the cost of performing a Lanczos step and the accuracy of the orthogonalization, which subsequently decreases the number of total $\mathbb{matvec}$ operations needed. 
As one can see from the plot, having $\approx 20-25$ basis vectors is more than enough to ensure the ratio of $\mathtt{matvec}$ operations is kept to a small constant (in this case, less than $5$) when approximating any portion of the spectra. 
This justifies our claim that combinatorial Laplacian operators, for many real-world data sets, requires just $O(m)$ memory complexity to compute eigenvalues (and thus, the persistent rank invariants).
 
 \textbf{Time requirements}: The remaining two figures on the right side of figure~\ref{fig:watts_strogatz} show the same ratio of $\mathtt{matvecs}/n$---effectively the constant associated with quadratic time complexity statement in~\ref{lemma:exact_arith_lanczos}---

%For example, weighting edges of the graph Laplacian originally studied by Kirchoff typically has natural interpretations as electrical \emph{conductance}/\emph{resistance} that arise electrical circuit networks, due to Ohms law and conservation of flow. 
%By normalizing 
%\emph{normalized} graph Laplacian arises naturally as a \emph{diffusion operator} in the study of transition probabilities. 
%As another example, the signless Laplacian...

% --- APPLICATION 2: Continuous Persistent Betti Curves (PBCs) as Shape Signatures --- 
% Benefits of rank(A) \approx < nuclear norm of A >
%\subsection*{Application: Relaxation}
%\subfile{spectral_relaxation}

% --- APPLICATION 2: Directional transforms as Shape Signatures ---- 
\subsection{Shape comparison}
In general, both combinatorial and topological aspects of a given topological space are encoded in the spectra of Laplacian operators. 
For example, the Laplacians of a simplicial complex encode its basic topology via its homology groups, which is characterized by the nullspace of the corresponding operator---this is identical for most of the Laplace operators, whether they are normalized, weighted, signless, and so on~\cite{}.
In contrast, these operators differ in the nonzero part of the spectrum, which when equipped with a scalar-product encode specific geometric features in addition to topological properties. 

 
\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{shape_signatures}
\end{figure}


\subsection{Filtration optimization}
A common setting in topological data analysis is the setting wherein one has access to a means of building a filtration $(K,f)$ where $f : K \to \mathbb{R}$ is a filter function satisfying $f(\tau) \leq f(\sigma)$ for all $\tau \subseteq \sigma \in K$, but the filter function $f$ itself is parameterized by some hyper-parameters. 
For example, a common setting is the one where the data set $(X, d_X)$ comes equipped with some notion of density $f: X \to \mathbb{R}_+$, and one would like to build a persistence diagram on $d_X$ in a way that is robust to local fluctuations in density. 
This is a common practical setting often encountered in practice, as it is known that persistence is unstable with respect to \emph{strong outliers}~\cite{}, which prevents persistent homology from detecting a spaces prominent underlying topological structure, when it exists. 
Most work seeking to remedy this issue proceeds by either (1) removing such outliers according to some heuristic~\cite{}, (2) transforming the  metric to lessen the importance of such points in the persistence diagram~\cite{}, or (3) creating a 2-parameter persistence module with one dimension filtered by (co)-density. 
Of the three, (1) is ultimately a heuristic not useful for complex data sets as it discards important data; (2)  imposes a parameter that must be set to proceed, and (3) is perhaps ideal but currently considered both analytically and computationally intractable in practice. 

To illustrate an alternative approach to the ways mentioned above, consider a fixed Delaunay complex $K$ built on a set of points sampled noisily around $S^1$ in the plane, shown in figure~\ref{fig:codensity}. Ultimately, we would like to detect the presence of the circle in $X$ via its persistence diagram, as that is the original purpose of persistence~\cite{}. 
We reframe the problem as follows: rather than filtering $K$ according to its ambient metric $d_X$, we ask first whether there \emph{exists} significant topological information at any density scale $\alpha$. Generically, we consider the following optimization problem:   
\begin{equation}\label{eq:mult_opt}
	\alpha^\ast = \argmax_{\alpha \in \mathbb{R}} \; 
	\restr{\mu_p(K, f_\alpha)}{R}
\end{equation}
If there exists a density scale $\alpha \in \mathbb{R}_+$ wherein a cycle $c \in H_1(K ; \mathbb{R})$ is highly persistent and $R \subset \Delta_+$ is appropriately chosen, then any maximizer of~\eqref{eq:mult_opt} yields an appropriate density scale $\alpha^\ast$ with which to detect the topology of the data. 
From another perspective, we refer to this process of choosing appropriate filtration (hyper-)parameters to yield persistence information as \emph{filtration optimization}. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.88\textwidth]{presentation/codensity_dgm_ex}
	\caption{A fixed Delaunay complex filtered by a kernel (co)density estimate for different bandwidth parameters $\alpha$. Observe that either too small (left) or too large (right) a choice of bandwidth can obscure the underlying topological structure, whereas an appropriate choice of bandwidth creates a filtration that detects the underlying circle. 
	}\label{fig:codensity}
\end{figure}

As an introductory example, we sampled $80$ points noisily around $S^1$ and then sampled an additional $30$ points in $[-1,1] \times [-1,1]$ to act as ``strong outliers.'' After constructing a Delaunay complex $K$ of these points, we parameterized a filter function $f_\alpha : K \times \mathbb{R} \to \mathbb{R}_+$ by assigning the filtration values of each simplex according to the lower stars of a kernel (co)density estimate, upon which we computed its vineyard~\cite{cohen2006vines} along a subset $\mathcal{A} \subset \mathbb{R}$. The vineyard, colored by $\alpha$, is shown on the left side of figure~\ref{fig:vineyard_codensity}. In this example, we choose the rectangle $R = \frac{1}{5}([1, 2] \times [3,4])$ out of simplicity; the corresponding multiplicity function is shown in the black curve on the right of figure~\ref{fig:vineyard_codensity}. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.90\textwidth]{presentation/combined_mult}
	\caption{(Left) Vineyard of of the codensity $\alpha$-parameterized filtration from figure~\ref{fig:codensity}. (Right) The exact multiplicity $\mu_1(K, f_\alpha)$ (black) and the proposed spectral relaxation (smoothed, blue) with relaxation parameter $\epsilon = 1\mathrm{e}{-3}$.}\label{fig:vineyard_codensity}
\end{figure}
By inspection, the optimal density parameter satisfying~\eqref{eq:mult_opt} is any parameter $\alpha$ lying approximately in the interval $[0.40, 0.44]$. 
Observe that any first-order optimization procedure initialized in the interval $\alpha_0 \in [0.3, 0.5]$ yields a maximizer $\hat{\alpha}$ in the interval $[0.36, 0.46]$, which is quite close to the interval $[0.40, 0.44]$. In this toy example, this is sufficient, however if a better estimate was required (e.g. the multiplicity was required to be positive as a constraint) then observe one could iteratively shrink $\epsilon$ to obtain a better approximation of $\mu_1$, and then repeat the first-order optimization. 
This is synonymous to the \emph{iterative thresholding} techniques often in high-dimensional statistics and machine learning, see~\cite{} for an overview.


%\begin{equation}
%	\alpha^\ast = \argmax_{\alpha \in \mathbb{R}} \; \ell(\mathrm{dgm}(K, f_\alpha))
%\end{equation}
%A common setting one would like to choose $\alpha \in \mathbb{R}$ such that some persistence-derived statistic is maximized. Since disparate choices of $\alpha$ corresponds may correspond to non-isomorphic filtration, we refer to this problem as the \emph{filtration optimization problem}. Unfortunately, unless much more is known both about the objective $\ell$ and the space of persistence duagrams, optimizing such a quantity seems at worst imposs 



  
%Like the rank function, the multiplicity objective function is discontinuous and non-differentiable. Moreover, by restricting $\mu_p(K, f_\alpha)$ to a fixed box $R \subset \Delta_+$ introduces many other complications into the objective, such as infeasibility outside of the support and instability in the objective.
%Maximizing~\eqref{eq:mult_opt} with current methods globally essentially reduces to computing the vineyard of the entire family $\{ \, \mathrm{dgm}(f_\alpha) : \alpha \in I \subset \mathbb{R} \, \}$.



\newpage 
\shipout\null
\shipout\null

\appendix
\section{Appendix}

\subsection*{Expanded Intro}
% Gromov-Hausdorff Stable Signatures for Shapes using Persistence
Though homology is primarily studied as a topological invariant, the fact that persistent homology encodes both topological and geometric information in its diagram has motivated its use not only as a shape descriptor but also as a metric invariant. 
Metric invariants, or ``signatures,'' are commonly used in metric learning to ascertain whether two comparable data sets $X, Y$ represent the same object---typically up to a some notion of invariance.
%the similarity of thdistances $d(X,Y) = 0$.
One mathematically attractive model for measuring the dissimilarity between shapes/datasets is the Gromov-Hausdorff (GH) distance $d_{\text{GH}}((X, d_X), (Y,d_Y)$ between compact metric spaces $(\mathcal{X}, d_X), (\mathcal{Y}, d_Y)$: by altering the choice of metric $(d_X, d_Y)$, the corresponding metric-distance $d_{\text{GH}}$ can be adapted to a chosen notion of invariance~\cite{} or to increase its discriminating power~\cite{}. 
Though it is NP-hard to compute~\cite{}, the GH distance defines a metric on the set of isomorphism classes of compact metric spaces endowed with continuous real-valued functions, justifying its study as a mathematical model for shape matching and metric learning. 
Moreover, it is known that the GH distance is tightly lower-bounded by the bottleneck distance between persistence diagrams constructed over Rips filtrations $R(X, d_X), R(Y, d_Y)$~\cite{}, which can be computed in polynomial time. 
Indeed, Solomon et al~\cite{} showed distributed persistence invariants characterize the quasi-isometry type of the underlying space, allowing one to provably interpolate between geometric and topological structure.
% curvature sets?

%For example, it is often the case one wishes to construct a (pseudo-)mettric on a given space of objects for comparison or classification purposes. Though the information contained in some topological invariants is immense, strictly topological information is often not enough to distinguish things---geometry is needed. 
%In many ways, persistence is an important tool in the widely studied \emph{manifold hypothesis} problem, wherein $\mathcal{X}$ is a compact Riemannian submanifold of Euclidean space, and $X \subset \mathcal{X}$ is sampled according to the intrinsic uniform distribution.
% since $\beta_p^{i,j}(K; \mathbb{F}) = \mathrm{dim}(H_p^i \to H_p^j)$.
%Despite being information rich, persistence diagram reflect persistent homology group, and homology groups as a topological invariant are quite weak. 
%An exemplary case of this, consider the Euler characteristic $\chi(X) = V - E + F$: by itself is rich enough to distinguish connectedness of space, though obviously as a single number its discriminatory power is quite limited. 
%To increase discriminatory power, a given set of complexes $K$ are typically filtered with respect to a direction $f : K \to \mathbb{R}$, and then $\chi$ is calculated on each sub level set, producing a Euler characteristic curve (ECC).
% By choosing $f$ more carefully, such as by filtering with respect to curvature, one can imbue the corresponding featurization to depend more on the geometry of the underlying embedding~\cite{}.
% Recent work suggests that families of ECCs contain sufficient enough information to reconstruct the input perfectly, or to construct distance metrics on shape space.  
% More generally, the directional transform... PHT.. 
%This is very much so an exciting and active area of research, more recent work has extended this, yielding a foundation for shape analysis on shape space. 

Though theoretically well-founded and information dense, persistence diagrams come with their own host of practical issues: they are sensitive to strong outliers, far from injective, and their de-facto standard computation exhibits high algorithmic complexity. 
Moreover, the space of persistence diagrams $\mathcal{D}$ is a Banach space, preventing one from doing even basic statistical operations, such as averaging~\cite{}. 
As a result, many researchers have focused on extending, enhancing, or otherwise supplementing persistence diagrams with additional information. 
% template functions...
Turner et al~\cite{} proposed associating a collection a shape descriptors with a PL embedded $X \subset \mathbb{R}^d$---one descriptor for each point on $S^{d-1}$---which they called a \emph{transform}. 
More exactly, suppose both the data $X$ and its geometric realization $K$ are PL embedded in $\mathbb{R}^d$ and has centered and scaled appropriately.
The main theorem in~\cite{} is that associating a persistence diagram, or even a simpler descriptor such as the Euler characteristic, for every point on $S^{d-1}$ is actually sufficient information to theoretically reconstruct $K$. 

 Missing from the above work is the are two important directions: how do you configure such transforms to retain the important topological/geometric information and discard irrelevant information, and (2) how may we efficiently compute them? 
The former question is synonymous with choosing the invariance model in the GH framework, which seems to be highly domain specific. 
% TODO: template functions
 In the latter case, though we know the number of directions is bounded~\cite{}, the bound is simply too high to be of any practical use. While there are efficient algorithms for both the ECC and persistence computations in static settings, the state of the art in parameterized settings is non-trivial and ongoing research area.
 

\subsection*{Letters}
As topological invariants, Betti numbers are invariant under homeomorphisms: any pair of filtrations $(K, f)$ and $(K', f')$ that are homotopy equivalent have identical homology classes and thus isomorphic persistence diagrams. 
This invariance can be a useful thing at the level of homology, as non-homeomorphic spaces can sometimes be differentiated by inspecting differences between their corresponding homology classes. 
However, invariance under homeomorphisms can at times discard geometric information that may be useful for differentiating objects.
For example, consider creating a classifier for the alphabet of English characters in the font shown below:
\vspace{0.5em}
\\
\vspace{0.5em}
\begin{ttfamily}
	\fontfamily{lmss}\selectfont \bfseries
	\hfill A B C D E F G H I J K L M N O P Q R S T U V W X Y Z \hfill 
\end{ttfamily}

\noindent If one were to triangulate images of each of the letters shown above and compute their Betti numbers, one would find just three homology classes: one class for those letters that have two holes (B), one class of letters that have one hole (A, D, O, P, Q, and R), and one class for the rest of the letters, which collapse to points. Thus, if one were concerned with differentiating letters of the alphabet, one may conclude that homology is not simply not strong enough of an invariant to do so. 

It would be beneficial to have an invariant that was sensitive to the geometries between shapes, but also stable in some sense.
\newpage

\subsection*{Inner Products}
Though the general Laplacian operator carries with it an interpretation of its eigensets as representing information about the intersection pattern of the underlying complex, a more precise interpretation of the eigensets depends both the operator and weighting scheme in question.
%herein the interpretation of the eigensets varies greatly. 
%various combinatorial or geometric interpretation arise from which operator is considered and how it is weighted. 
Many early results followed Kirchhoff's observations about the properties of $L$ reflecting certain physical laws of electrical flows in circuit networks, wherein eigenvectors have certain interpretations useful for graph sparsification and graph partitioning~\cite{chung1997spectral}. 
%via \emph{effective resistances}.
%Along a similar vein, Fiedler's characterization of the second eigenvector spawned many efforts in graph partitioning~\cite{chung1997spectral}, due to a connection to Cheegers inequality.
More recently, Nadler observed the \emph{normalized} graph Laplacian given by: 
\begin{equation}\label{eq:normal_graph_laplacian}
\mathcal{L} = D^{-1/2}(D - A) D^{-1/2}
\end{equation}
connects the process of diffusion (over a probability density) to the eigensets to $\mathcal{L}$.
%This gives the spectra of these operators a certain interpretation as representing random walk behavior.   
Yet another choice of normalization relates the eigenfunctions of $\mathcal{L}$ to the discrete LaplaceBeltrami operator on manifolds~\cite{}, which carries a certain ``heat'' interpretation with it. 
Ultimately, just as persistence diagrams encode geometric interpretations through their domain-specific filter functions, the geometry contained in the spectra of combinatorial Laplacians is reflected by the choice of a domain-specific weight function.

%convergence results etween eigenvectors of each diffusion map converge to the eigenfunctions of a corresponding differential

%, which has found applications i
%operator defined on the support of the probability distribution.


%\emph{diffusion}, which are often studied in random walk processes .
%%on the data based on isotropic and anisotropic diffusion.
%On the theory front, connection to discrete to the weighted Laplace-Beltrami operator on manifold.

%Historically, one of the motivators of the study of graph eigenvalues was the study of vibrations of membranes is closely tied in with the study of eigenvalues and eigenfunctions on Riemannian manifolds.
%As an infinitesimal generator of a diffusion process and its connection to discrete to the weighted Laplace-Beltrami operator on manifold, the graph Laplacian has served as the foundational operator for many applications both in theory and in practice. 
%, like random walks or diffusion, a different operator appears...

%Among the first to study these combinatorial operators was Eckmann~\cite{}, who proved a discrete  Hodge Theorem: 
%\begin{equation}\label{eq:laplace_hom}
%	\tilde{H}_p(K; \mathbb{R}) \cong \mathrm{ker}(\Delta_p(K)), \quad \beta_p = \mathrm{nullity}(\Delta_p(K))
%\end{equation}
%More directly relevant to this work, Horak and Jost developed a systematic study of the spectra of these operators unifies many of the Laplace operators on simplicial complexes~\cite{}. 
%Among other results, they showed that the \emph{non-zero} parts of the spectra $\Lambda_+$ satisfy:
%\begin{equation}
%	\Lambda_+(\Delta_p(K)) = \Lambda_+(L_p^{\text{up}}(K)) \cup \Lambda_+(L_p^{\text{dn}}(K))
%\end{equation}
%Moreover, it may be shown that the non-zero eigensets $\Lambda_+(L_p^{\text{up}}(K))$ and $\Lambda_+(L_{p+1}^{\text{dn}}(K))$ are identical---thus, it suffices to consider only one of them. 
%Many researchers studying higher order combinatorial Laplacians have found applications to areas such as graph optimization~\cite{}, network circuit theory~\cite{}, and graph sparsification~\cite{}. 

Weight functions may be interpreted through their action on the coboundary vector space $C^p(K, \mathbb{R}) := \mathrm{Hom}(K, \mathbb{R})$.
%are defined as the duals of the chain groups $C_p(K, \mathbb{R})$. 
As with $C_p(K, \mathbb{R})$, a basis for $C^p(K, \mathbb{R})$ is given by the set of its \emph{elementary cochains}:
\begin{equation}
	\{ \; \chi([\sigma]) \mid [\sigma] \in B_p(K, \mathbb{R}) \; \}, \text{ where } \chi([\sigma']) = \begin{cases}
		1 & \text{ if } [\sigma'] = [\sigma] \\
		0 & \text{ otherwise }
		\end{cases}
\end{equation}
It can be shown that for any choice of inner product on $C^p(K, \mathbb{R})$, there exists a positive weight function $f: K \to \mathbb{R}_+ \setminus \{0\}$ satisfying: 
\begin{equation}
	\langle \, g, h \, \rangle_{f} = \sum\limits_{\sigma \in K^p} f(\sigma) g([\sigma]) h([\sigma])
\end{equation}
Furthermore, the set of weight functions and scalar product on $C^p(K, \mathbb{R})$ wherein elementary cochains are orthogonal are in one-to-one correspondence~\cite{}. 
 Indeed, if $f : (\mathbb{R}^n, H_n) \to (\mathbb{R}^m, H_m)$ be a linear map between inner product matrices $H_n \in \mathbb{R}^{n \times n}$ and $H_m \in \mathbb{R}^{m \times m}$, then by Proposition~\cite{} for any $x \in \mathbb{R}^n$ and $y \in \mathbb{R}^m$, we have the following equivalence of inner products: 
	$$ \langle f x, y \rangle_{\mathbb{R}^m}  = \langle x, f^\ast y \rangle_{\mathbb{R}^n} = x^T F^T H_m y = x^T H_n F^\ast y $$
	where $F \in \mathbb{R}^{m \times n}$ denotes the matrix representative of $f$ and $F^\ast = H_n^{-1} F^T H_m$ a representative of the adjoint $f^\ast : (\mathbb{R}^m, H_m) \to (\mathbb{R}^n, H_n)$ of $f$. 
	 In this way, we say that the choice of weight function \emph{induces} an inner product on $C^p(K, \mathbb{R})$\footnote{Nullspace comment}. 
In this way, we reduce the study of geometry to the study ``weight functions'' of laplacian operators. 
%Thus, in this way we think of our definition of a parameterized boundary matrix as corresponding to a choice of a \emph{non-negative scalar-product} on the vector space $C^p(K, \mathbb{R})$, which induces a (degenerate) inner product $\langle \cdot, \cdot \rangle : C^p(K, \mathbb{R}) \times C^p(K, \mathbb{R}) \to \mathbb{R}$ on $C^p(K, \mathbb{R})$. 
%Equivalently, our choice of weight function induces an inner product on the quotient space $W = C^p(K, \mathbb{R}) / \{ b : \lvert b \rvert = 0 \}$.
%\begin{remark}
%In general, neither terms in~\eqref{eq:weighted_up_laplace} are symmetric unless $W_p = I_n$ (for $L_p^{\text{up}}$) or $W_{p+1} = I_m$ (for $L_p^{\text{dn}}$). 
%%$W_p^{-1} \partial_{p+1} W_{p+1} \partial_{p+1}^T$ 
%However, note that $L_p^{\text{up}}$ may be written as follows: 
%\begin{equation}\label{eq:l_up}
%	L_p^{\text{up}} = W_p^{-1} \partial_{p+1} W_{p+1} \partial_{p+1}^T  = W_p^{-1/2} \big( W_p^{-1/2}  \partial_{p+1} W_{p+1} \partial_{p+1}^T W_p^{-1/2}  \big ) W_p^{1/2} 
%\end{equation}
%Though the operator from~\eqref{eq:l_up} is not necessarily symmetric, it is of the form $W^{-1} P W$ where $P \in S_n^+$ and $W$ is a positive diagonal matrix.
%The same result holds for up-, down-, and combinatorial Laplacians~\cite{}.  
%We refer to these inner matrices $P$ as the \emph{symmetric} weighted Laplacian of $K$.
%\end{remark}


\subsection*{Directional Transform}
The canonical interpretation of the information displayed by a persistence diagram is that is summarizes the persistence of the sublevel sets of filtered space. Given a filtration pair $(\, K, f \, )$ where $K$ is a finite simplicial complex and $f : K \to \mathbb{R}$ is a real-valued function, the sublevel sets $\lvert K \rvert_i=f^{-1}(-\infty, i]$ deformation retract to... % say more about stars, homotopy equivalence
% simplexwise-linear function
% http://www.csun.edu/~ctoth/Handbook/chap24.pdf
If $K$ is embedded in $\mathbb{R}^d$, then geometrically $f$ takes on the interpretation of a `height' function whose range yields the `height' of every simplex in $K$. 
%Obviously, this notion of height depends on the embedding of $K$: viewing $K$ (and thus, $X$) from different `directions' induces potentially distinct sublevel sets, 

%Given a simplicial complex $K$ embedded in $\mathbb{R}^d$, 
Let $X \subset \mathbb{R}^d$ denote a data set which can be written as a finite simplicial complex $K$ whose simplices are PL-embedded in $\mathbb{R}^d$. Given this setting,  define the \emph{directional transform} (DT) of $K$ as follows:
\begin{align*}\label{eq:pht}
	\mathrm{DT}(K): S^{d-1} &\to  K \times C(K, \mathbb{R}) \\
	v &\mapsto (K_\bullet, f_v)
\end{align*}
where we write $(K_\bullet, f)$ to indicate the filtration on $K$ induced by $f_v$ for all $\alpha \in \mathbb{R}$, i.e.: 
\begin{equation}
	K_\bullet = K(v)_\alpha = \{\, x \in X \mid \langle x, v \rangle \leq \alpha  \,\} %_{\alpha = -\infty}^{\infty}
\end{equation}
Conceptually, we think of DT as an $S^{d-1}$-parameterized family of filtrations. 

% Conceptually, the $p$-th dimensional persistence diagram $\mathrm{dgm}_p(K, v)$ summaries how the topology of the filtration $K(v)$ changes in the direction of  $v$. Similarly, the PHT summarizes how the topology of $K$ changes in \emph{all} directions

The Persistent Homology Transform (PHT) is a shape statistic that establishes a fundamental connection between the topological information summarized by $K$'s PH groups and the geometry of its associated embedding. Given a complex $K$ built from $X$, it is defined as: 
\begin{align*}\label{eq:pht}
	\mathrm{PHT}(K): S^{d-1} &\to \mathcal{D}^d \\
	v &\mapsto \left( \, \mathrm{dgm}_0(K, v), \mathrm{dgm}_1(K, v), \dots, \mathrm{dgm}_{d-1}(K, v) \, \right)\numberthis
\end{align*}
where $\mathcal{D}$ denotes the space of $p$-dimensional persistence diagrams, for all $p = 0, \dots, d-1$ and $S^{d-1}$ the unit $d-1$ sphere. The stability of persistence diagrams ensures that the map $v \mapsto \mathrm{dgm}_p(K, v)$ is Lipschitz with respect to the bottleneck distance metric $d_B(\cdot, \cdot)$ whenever $K$ is a finite simplicial complex. 
Thus, the PHT may be thought of as an element in $C(S^{d-1}, \mathcal{D}^d)$: . 

%thus the PHT may be thought of naturally as a parameterized family of diagrams.

The primary result of~\cite{} is that the PHT is injective on the space of subsets of $R^d$ that can be written as finite simplicial complexes\footnote{Implicit in the injectivity statement of the PHT is that, given a subset $X \subset \mathbb{R}^d$ which may be written as finite simplicial complex $K$, the restriction $f: X \to \mathbb{R}$ to any simplex in $K$ must is linear.}, which we denote as $\mathcal{K}_d$. 
Equivalently, $\mathcal{K}_d$ decomposes space of all pairs $(K, f)$ under the equivalence $(K, f) \sim (K,f')$ when $f(K) = f'(K)$.

%Like the directional transform, the PHT is essentially the ompositon of the DT with PH: PHT= PH \circ DT. 
% One of the constructing metrics capable of differentiating non-diffeomorphic shapes.


\subsection{Complexity of Persistence \& Related work}
We briefly recount the main complexity results of the persistence computation. 
With a few key exceptions, the majority of persistent homology implementations and extensions is based on the \emph{reduction algorithm} introduced by Edelsbrunner and Zomorodian~\cite{edelsbrunner2000topological}. 
This algorithm factorizes the filtered boundary into a decomposition $R = \partial V$, where $V$ is full rank upper-triangular and $R$ is said to be in reduced form: if its $i$-th and $j$-th columns are nonzero, then $\mathrm{low}_R(i) \neq \mathrm{low}_R(j)$, where $\mathrm{low}_R(i)$ denotes the row index of the lowest non-zero in column $i$. 
We refer to~\cite{edelsbrunner2000topological, bauer2020persistence, dey2022computational} for details. 

Given a filtration $(K, f)$ of size $m = \lvert K \rvert$ with filter $f : K \to [m]$, the reduction algorithm in form given in ~\cite{edelsbrunner2000topological} computes $\mathrm{dgm}_p(K; \mathbb{Z}/2) = \{ \, (\tau_1, \sigma_1), (\tau_2, \sigma_2), \dots, (\tau_k, \sigma_k) \, \}$ runs in time proportional to the sum of the squared (index) persistences $\sum_{i=1}^k (f(\sigma_i)-f(\tau_i))^2$. As $k$ is at most $m / 2$, this implies a $O(m^3)$ upper bound on the complexity of the general persistence computation, which incidentally Morozov showed was a tight $\Theta(m^3)$ under the assumption that each column reduction takes $O(m)$ time. 
By exploiting the matrix-multiplication results, a similar result can be shown to reduce to $O(m^\omega)$, where $\omega$ is the matrix-multiplication constant, which is $\approx 2.37$ as of this time of writing. 
It worth remarking that the complexity statements above are all given in terms of the number of \emph{simplices} $m$: if $n = \lvert K^0 \rvert$ is the size of the vertex set, the above implies a worst-case bound of $O(n^{\omega(p+2)})$ on the general persistence computation. For example, if we use non-Strassen-based matrix multiplication $(\omega = 3)$ and we are concerned with $p=1$ homology computation, the complexity of the reduction algorithm scales $O(n^9)$ in the number of vertices of the complex, which is essentially intractable for most real world application settings. 

Despite the seemingly immense intractability of the persistence computation, decades of advancements have been made in reducing the complexity or achieving approximate results in reasonable time and space complexities. 
The complexity of the reduction algorithm is complicated by the fact that it depends heavily on the structure of the associated filtration $K$, the homology dimension $p$, the field of coefficients $\mathbb{F}$, and the assumptions about the space $K$ manifests from.
In~\cite{}, Sheehy presented an algorithm for producing a sparsified version $(\tilde{K}, \tilde{f})$ of a given Vietoris-Rips filtration $(K, f)$ constructed from an $n$-point metric space $(X, d_X)$ whose total number of $p$-simplices is bounded above by $n\cdot (\epsilon^{-1})^{O(pd)}$, where $d$ is the doubling dimension of $X$.
%By assuming $d$ and $\epsilon$ are constant, one infers the size of the filtration is $O(n)$. 
It was shown that $\mathrm{dgm}_p(\tilde{K})$ is guaranteed to be a multiplicative $c$-approximation to the $\mathrm{dgm}_p(K)$, where $c = (1 - 2\epsilon)^{-1}$ and $\epsilon \leq 1/3$ is a positive approximation parameter.
When $p = 0$ and the filtration function $f : K \to \mathbb{R}$ is PL, the reduction algorithm can be bypassed entirely in favor of simple $O(n \log n + \alpha(n) m) \approx O(m)$ algorithm (see Algorithm 5 in~\cite{dey2022computational}), where $n = \lvert K^0 \rvert$ and $m = \lvert K^1 \rvert$ and $\alpha(n)$ is the extremely slow-growing inverse Ackermann function. 
Moreover, the $d-1$ persistence pairs can be computed in $O(n \alpha(n))$ time algorithm for filtrations of simplicial $d$-manifolds essentially reducing the problem to computing persistence on a dual graph~\cite{dey2022computational}.
For clique complexes, the apparent pairs optimization---which preemptively removes zero-persistence pairs from the computation prior to the reduction---has been empirically observed to reduce the number of columns needing reduced for clique complexes by $\approx 98-99\%$~\cite{bauer2020persistence}. 
Numerous other optimizations, including e.g. the \emph{clearing optimization}, the use of \emph{cohomology}, the \emph{implicit reduction} technique, have further reduced both the non-asymptotic constant factors of the reduction algorithm significantly, see~\cite{bauer2020persistence} and references therein for a full overview. 

Despite the dramatic reductions in time and space needed for the persistence algorithm to complete, to the author knowledge relatively little has been done in improving the complexity and effective runtime of the reduction in parameterized settings. 
% vineyards
% moves and warm restarts
Although both of these algorithms have shown significant constant-factor reductions in the (re)-reduction of the associated sparse matrices, all of the techniques require $O(m^2)$ storage to execute as the $R$ and $V$ matrices must be maintained throughout the computation. Moreover, all three of the above methods intrinsically work within the reduction framework, wherein simulating persistence in dynamic contexts effectively reduces to the combinatorial problem of maintaining a valid $R = \partial V$ decomposition. 

As noted in~\cite{dey2022computational}, the reduction algorithm is essentially a variant of Gaussian elimination. Indeed, the persistence of a given filtration can be computed by the PLU factorization of a matrix.
The explicit decompositional approach of factorizing a large matrix into constitutive parts is known historically in numerical linear algebra as a \emph{direct method}---methods would yield the exact solution within a finite number of steps. 
In contrast, iterative methods start with approximate solution and progressively update the solution up to arbitrary accuracy. 
The iterative methods well-known to the numerical linear algebra community, such as Krylov methods, are typically often attractive not only due to the reduction in computational work over direct approaches but also of the limited amount of memory that is required. 
Despite the success of iterative methods in efficiently solving linear systems manifesting from diagonally dominant sparse matrices is~\cite{}, such advancements have not yet been extended to the persistence setting. 


\subsection*{Output sensitive multiplicity and Betti}
We record this fact formally with two corollaries. Let $\mathrm{R}_p(k)$ denotes the complexity of computing the rank of square $k \times k$ matrix with at most $O((p+1)k)$ non-zero $\mathbb{F}$ entries. Then we have:
\begin{corollary}
	Given a filtration $K_\bullet$ of size $N = \lvert K_\bullet \rvert$ and indices $(\,i,j\,) \in \Delta_+^N$, computing $\beta_p^{i,j}$ using expression~\eqref{eq:betti_four} requires $O\big(\max \{\mathrm{R}_{p}(n_i), \mathrm{R}_{p+1}(m_j) \} \big)$ time, where $n_i = \lvert K_i^p \rvert$ and $m_j = \lvert K_j^{p+1} \rvert$.
	%$O(\mathrm{R}_p(n, i, p), \, R_\partial(m, j, p+1) \,\})$ where $R_\partial(a,b,c)$ is the complexity of computing the rank of a $c$-dimensional $a\times b$ boundary matrix with $b\cdot (c+1)$ non-zero $\mathbb{F}$ entries. 
	%with $n < m$
\end{corollary} 
\noindent Observe the relation $\partial_{p\+1}^{i \+ 1, j} \subseteq \partial_{p\+1}^{1, j}$ implies the  dominant cost of computing~\eqref{eq:betti_four} lies in computing either $\mathrm{rank}(\partial_p^{1,i})$ or $\mathrm{rank}(\partial_{p+1}^{1,j})$, which depends on the relative sizes of $\lvert K^p\rvert$ and $\lvert K^{p+1}\rvert$. In contrast, $\mu_p^R$ is localized to the pair $(K_i, K_l)$ and depends only on the $(p+1)$-simplices in the interval $[i, l]$, yielding the following corollary. 
\begin{corollary}
	Given a filtration $K_\bullet$ of size $N = \lvert K_\bullet \rvert$ and a rectangle $R = [i,j] \times [k,l]$ with indices $0 \leq i < j \leq k < l \leq N$, computing $\mu_p^{R}$ using expression~\eqref{eq:mu_four} requires $O(\mathrm{R}_{p+1}(m_{il}))$ time $m_{il} = \lvert K_l^{p+1}\rvert - \lvert K_i^{p+1}\rvert$.
	%$ j(p+1) - i$
	%time and storage complexity $O(\max \{\, R_\partial(n, i, p), \, R_\partial(m, j, p+1) \,\})$ where $R_\partial(a,b,c)$ is the complexity of computing the rank of a $c$-dimensional $a\times b$ boundary matrix with $b\cdot (c+1)$ non-zero $\mathbb{F}$ entries. 
	%with $n < m$
\end{corollary} 


\subsection{Finite-precision arithmetic}\label{subsec:rounding} 
It is well established in the literature that the Lanczos iteration, as given in its original form, it effectively useless in practice due to significant rounding and cancellation errors. Such errors manifest as loss of orthogonality between the computed Lanczos vectors, which drastically affects the convergence of the method. 
At first glance, this seems to be a simple numerical issue, however the analysis from Parlett~\cite{parlett1994we} showed, loss of orthogonality is not merely the result of gradual accumulation of roundoff error---it is in fact is intricately connected to the convergence behavior of Lanczos iteration. 
One obvious remedy to this is to reorthogonalize the current Lanczos vectors $\{q_{j-1}, q_{j}, q_{j+1}\}$ against all previous vectors using Householder matrices~\cite{golub2013matrix}---a the \emph{complete reorthogonalization} scheme. This process guarantees orthogonality to working precision, but incurs a cost of $O(jn)$ for each Lanczos step, effectively placing the iteration back into the cubic time and quadratic memory regimes the direct methods exhibit. 
A variety of orthogonality enforcement schemes have been introduced over years, including implicit restart schemes, selective reorthogonalization, thick restarts, block methods, and so on; see~\cite{} for an overview.  

\subsection{Laplacian Interpretation}
In what follows we make a connection between boundary matrices and the graph Laplacian to illustrate how the Laplacian captures the ``connectivity'' aspects of the underlying simplicial complex. 
 \begin{example}[Adapted from~\cite{newman2001laplacian}]\label{ex:laplacian}
Suppose the vertices of $G$ are ordered and labeled from $1$ to $n$ arbitrarily such that, given any subset $X \subseteq V$, we may define column vector $x = (\, x_i\, )$ whose components $x_i = 1$ indicate $i \in X$ and $x_i = 0$ otherwise. Given such a set $X \subseteq V$, let $X' = V \setminus X$ denote its complement set. By $L$'s definition, we have:
\begin{align*}
	(Lx)_i > 0 &\Longleftrightarrow i \in X \text{ and } \lvert c_i(X) \rvert = (Lx)_i  \\
	(Lx)_i < 0 &\Longleftrightarrow i \in X' \text{ and } \lvert c_i(X') \rvert = \lvert (Lx)_i \rvert \\
	(Lx)_i = 0 &\Longleftrightarrow i \in X \cup X' \text{ and } c_i(X) = \emptyset
\end{align*}
where $c_v(X) = \{ (v,w) \in E \mid v \in X \text{ and } w \in V \setminus X \}$ denotes the \emph{cutset}  of $X$ restricted to $v$, i.e. the set of edges having as one endpoint $v \in X$ and another endpoint outside of $X$.
\end{example}
\noindent In other words, example~\ref{ex:laplacian} demonstrates that $L$ captures exactly how $X$ is connected to the rest of $G$. Notice that if $X  = V$, then $Lx = 0$ and thus $0$ must be an eigenvalue of $L$ with an eigenvector pair $\mathbf{1}$. Like the adjacency matrix, the interpretation of the matrix-vector product has a natural extension to powers of $L$, wherein just as entries in $A^k$ model paths, entries in $L^k$ are seen to model boundaries~\cite{newman2001laplacian}.

\subsection*{Parameterizing Settings}
We include a few examples of potential application areas of work. Namely, we show a few promising examples of ``parameterized settings'' that may naturally benefit from our efforts here.
\\
\\ 
\textbf{Dynamic Metric Spaces:} Consider an $\mathbb{R}$-parameterized metric space $\delta_X = ( X, d_X(\cdot) )$ where $X$ is a finite set and $d_X(\cdot): \mathbb{R} \times X \times X \to \mathbb{R}_{+}$, satisfying: 
\begin{enumerate}
	\item For every $t \in \mathbb{R}, \delta_X(t) = (X, d_X(t))$ is a pseudo-metric space\footnote{This is required so that if one can distinguish the two distinct points $x, x' \in X$ incase $d_X(t)(x, x') = 0$ at some $t \in \mathbb{R}$. } 
	\item For fixed $x, x' \in X$, $d_X(\cdot)(x, x'): \mathbb{R} \to \mathbb{R}_{+}$ is continuous.
\end{enumerate}
When the parameter $t \in \mathbb{R}$ is interpreted as \emph{time}, the above yields a natural characterization of a ``time-varying'' metric space. More generally, we refer to an $\mathbb{R}^h$-parameterized metric space as \emph{dynamic metric space}(DMS). 
Such space have been studied more in-depth~\cite{} and have been shown...


%Let $\delta_\mathcal{X}$ denote an $\mathrm{T}$-parameterized metric space $\delta_\mathcal{X}(\cdot) = ( X, d_X(\cdot) )$, where $d_X: \mathrm{T} \times X \times X \to \mathbb{R}_+$ is called a \emph{time-varying metric}  and $X$ is a finite set with fixed cardinality $\lvert X \rvert = n$. $\delta_X$ as called a \emph{dynamic metric space} (DMS) iff $d_X(\cdot)(x, x')$ is continuous for every pair $x, x' \in X$ and $\delta_\mathcal{X}(t) = (X, d_X(t))$ is a pseudo-metric space for every $t \in \mathrm{T}$. 
%For a fixed $t \in \mathrm{T}$, the Rips complex at scale $\epsilon \in \mathbb{R}$ is the abstract simplicial complex given by 
%\begin{equation}
%	\mathrm{Rips_{\epsilon}}(\delta_\mathcal{X}(t)) := \{ \sigma \subset X : d_X(t)(x, x') \leq \epsilon \text{ for all } x, x' \in \sigma \}
%\end{equation}
%\noindent As before, the family of Rips complexes for varying $\epsilon > 0$ yields a filtration whose inclusion maps induce linear maps at the level of homology. The time-varying counterpart is analogous.  
%In this context, we write the $p$-th persistent Betti number with respect to fixed values $i,j \in I$ as a function of $t \in \mathrm{T}$: 
%\begin{equation}
%\beta_{p}^{i,j}(t) = \left(\mathrm{dim} \circ \mathrm{H}_p^{i,j} \circ \mathrm{Rips} \circ \delta_\mathcal{X} \right)(t)
%\end{equation}

% Rayleigh-Ritz discussion
\textbf{Rayleigh Ritz values}
Though the Lanczos iterations may be used to obtain the full tridiagonalization $A = Q T Q^T$,  intermediate spectral information is readily available in $T_j$, for $j < \mathrm{rank}(A)$.
% Even before an $A$-invariant subspace is obtained, 
%If $T_j$ represents projection of $A$ onto the $j$-th Krylov subspace $\mathcal{K}_j(A, q_1)$, 
Diagonalizing $T_j = Y \Theta Y^T$ yields value/vector pairs $\{  (\theta_1^{(j)}, y_1^{(j)}), \dots, (\theta_j^{(j)}, y_j^{(j)}) \}$ satisfying $w^T (Ay - \theta y) = 0$ for all $w \in \mathcal{K}_j(A, q_1)$, called \emph{Ritz pairs}. The values $\theta$ are called \emph{Ritz values} and their associated vectors $v = Q y$ in the range of $Q$ are called \emph{Ritz vectors}.
From the Ritz perspective, the Lanczos iteration implicitly maintains two orthonormal basis for $K_j(A, q_1)$---a Lanczos basis $Q$ and the Ritz basis $Y$:
$$ A = Q T Q^T = Q Y \Theta Y^T Q^T \Longleftrightarrow A Q Y = Q Y \Theta $$
In principle, the Lanczos basis $\{q_i\}_{i=1}^j$ changes each iteration, while the Ritz basis $\{ Q y_i^{(j)} \}_{i=1}^{j}$ changes after each subspace projection. The way in which the Ritz values approach the spectrum of $A$ is well-studied~\cite{}, as they are known to be Rayleigh-Ritz approximations of $A$'s eigenpairs $\Lambda(A) = \{  (\lambda_1, v_1), \dots, (\lambda_j, v_j) \}$, and they are collectively known to be optimal in the sense that $T_k = B$ is the matrix that minimizes $\lVert A Q_k - Q_k B \rVert_2$ over the space of all $k \times k$ matrices. 
Moreover, Ritz values contain intrinsic information of the distance between $\Lambda(T_j)$ and $\Lambda(A)$. To see this, note that: 
\begin{equation}
	\lVert A v_i^{(j)} - v_i^{(j)} \theta_i^{(j)} \rVert = \beta_i^{(j)} = \beta_{j+1} \cdot \lvert \langle e_j, y_i^{(j)} \rangle   \rvert 
\end{equation}
Thus, we need not necessarily keep the Lanczos vectors $Q$ in memory to monitor how close the spectra of the $T_j$'s approximate $\Lambda(A)$. In fact, it is known that the Ritz values $\{ \theta_1^{(1)}, \theta_1^{(2)}, \dots, \theta_1^{(j)} \}$ of $T_j$ satisfy: 
 \begin{equation}
 	\lvert \lambda - \theta_i^{(j)} \rvert \leq \big ( \beta_i^{(j)} \big )^2 / \big( \min_\mu \; \lvert \mu - \theta_i^{(j)} \rvert \big ) 
 \end{equation}
The full convergence of the Ritz values to the eigenvalues of $A$ is known to converge at a rate that depends on the ratio between $\lambda_1/\lambda_n$. A full analysis is done in terms of Chebychev Polyonomials in~\cite{golub2013matrix}. In practice, it has been observed that the Lanczos iteration converges super-linearly towards the extremal eigenvalues of the spectrum, whereas for interior eigenvalues one typically must apply a shifting scheme.


\subsection*{Convergence Rate}
The ability of the Krylov subspace iteration to capture the extremal portions of the spectrum remains unparalleled, and by using $O(n)$ memory, the Lanczos iteration uses optimal memory. 
As mentioned in section~\ref{}, when the computation is carried out in finite-precision arithmetic, one may observe loss of orthogonality in the Lanczos vectors. 
Fortunately, the connection between the Lanczos method and the Rayleigh quotient ensures \emph{eventual} termination of the procedure under by restarting the Lanczos method, and continue with the iteration until the spectrum has been approximated to some prescribed accuracy. 
Unfortunately, if the number of iterations $k$ is e.g. larger than $n^2$, then the method may approach to $O(r\max(\mathcal{M}(n), n), n) \approx O(n^3)$ complexity one starts with. 
If the supplied matrix-vector product operation is fast, the number of iterations $k$ needed for convergence of the Lanczos method becomes the main bottleneck estimating the spectrum of $A$.

% Rate of convergence definition
Loss of orthogonality can be mitigated by re-orthogonalizaing against all previous Lanczos vectors, but this increases the Lanczos complexity to $\approx O(n^2)$ per iteration. 
Thus, the goal is strike a balance: find a way to keep all $n$ Lanczos numerically orthonormal, so as to ensure super-linear convergence of the Ritz values $\theta$, but do so using $c \cdot n$ memory, where $c$ is a relatively small constant.

Since rates of convergence $\alpha$ increases the number of correct digits by an expoentnial rate with factor $alpha$, any super-linear convergent ($\alpha > 1$) method needs at most $c$ terms to approximate an eigen-pair up to numerical precision. 
In the context of the Lanczos method, achieving even quadratic convergence would imply the number of iterations needed to obtain machine-precision is bounded by $T(c \cdot \mathcal{M}(n) \cdot r)$, where $c$ is a small constant. We say that a method which achieves \emph{superlinear} convergence has complexity \emph{essentially} $O(c\cdot n) \approx O(n)$.


Among the more powerful methods for achieving super linear convergence towards a given eigenvalue $\lambda$ is the Jacobi-Davidson method. This method seeks to correct:

Solving for $t$ results in the \emph{correction equation}
\begin{equation}
	(I - u u^T)(A - \sigma I )(I - u u^T) t = \theta u - A u 
\end{equation}
where, since $u$ is unit-norm, $I - u u^T$ is a projector onto the complement of $\mathrm{span}(u)$. 
It's been shown that solving exactly for this correction term essentially constructs an cubically-convergent sequence towards some $\theta \mapsto \lambda$ in the vicinity of $\sigma$. Solving for the correction equation exactly is too expensive, sparking efforts to approximate it. It turns out that, just as the Lanczos method in exact arithmetic is highly related to the conjugate gradient method for solving linear systems, solving for the correction equation exactly is in some ways conceptually similar to making an Newton step in the famous Newtons method from nonlinear optimization. Since~\eqref{} is approximated, the JD method is often called in the literature akin to making an ``inexact newton step''~\cite{}.


The JD method with inexact Newton steps yields an individual eigenvalue estimate with quadratic convergence---\emph{essentially} $O(m)$ time after some constant number matrix-vector products and $O(n)$ memory.
The Lanczos method, in contrast, estimates all eigenvalues in essentially quadratic time if the convergence rate is superlinear. Pairing these two methods is a non-trivial endeavor. 
In a sequence of papers, Stathopoulos et al~\cite{} investigated various strategies for approximately solving the correction equation. 
In , they give both theoretical and empricial evidence to suggest that by employing generalized Davidson and Jacobi-Davidson like solvers within an overarching Lanczos paradigm, they achieve nearly optimal methods for estimating large portions of the spectrum using $O(1)$ number of basis vectors. By approximating the inner iterations with the symmetric Quasi-Minimal Residual (QMR) method, they argue that JD cannot converge more than three times slower than the optimal method, and empirically they find the constant factor to be less than $2$. 
%usually are significantly less than two times slower 
%If we fast forward several decades of research, one finds that there is no longer just 


\subsection{Proofs}
\subsubsection*{Proof of rank equivalence}
In general, it is not true that $\mathrm{rank}(A) = \mathrm{rank}(\mathrm{sgn}(A))$. 
% sign pattern classes
However, it is true that $\mathrm{rank}(\partial_p) = \mathrm{rank}(\mathrm{sgn}(\partial_p))$.
\subsubsection*{Proof of Lemma 1}
\begin{proof}
	The Pairing Uniqueness Lemma~\cite{dey2022computational} asserts that if $R = \partial V$ is a decomposition of the total $m \times m$ boundary matrix $\partial$, then for any $1 \leq i < j \leq m$ we have $\mathrm{low}_R[j] = i$ if and only if $r_\partial(i,j) = 1$. 
	As a result, for $1 \leq i < j \leq m$, we have:
\begin{equation}
	\mathrm{low}_R[j] = i \iff r_R(i,j) \neq 0 \iff r_\partial(i,j) \neq 0
\end{equation} 
Extending this result to equation~\eqref{eq:lower_left_rank} can be seen by observing that in the decomposition, $R = \partial V$, the matrix $V$ is full-rank and obtained from the identity matrix $I$ via a sequence of rank-preserving (elementary) left-to-right column additions.  
\end{proof}

\subsubsection*{Proof of Proposition 1}
\begin{proof}
We first need to show that $\beta_p^{i,j}$ can be expressed as a sum of rank functions. Note that by the rank-nullity theorem, so we may rewrite~\eqref{eq:pbn} as:
%$$ \beta_p^{i,j} = \mathrm{dim} \left( Z_p(K_i) \right) - \mathrm{dim}\left( Z_p(K_i) \cap B_p(K_j) \right ) $$
$$ \beta_p^{i,j} = \mathrm{dim} \left( C_p(K_i) \right) - \mathrm{dim} \left( B_{p-1}(K_i) \right) - \mathrm{dim}\left( Z_p(K_i) \cap B_p(K_j) \right ) $$
The dimensions of groups $C_p(K_i)$ and $B_p(K_i)$ are given directly by the ranks of diagonal and boundary matrices, yielding:  
$$
	\beta_p^{i,j} = \mathrm{rank}(I_p^{1, i}) - \mathrm{rank}(\partial_p^{1,i}) - \mathrm{dim}\left( Z_p(K_i) \cap B_p(K_j) \right )
$$
To express the intersection term, note that we need to find a way to express the number of $p$-cycles born at or before index $i$ that became boundaries before index $j$. 
Observe that the non-zero columns of $R_{p \+ 1}$ with index at most $j$ span $B_p(K_j)$, i.e $\{ \, \mathrm{col}_{R_{p\texttt{+}1}[k] } \neq 0 \mid \, k \in [j] \,\} \in \mathrm{Im}(\partial_{p+1}^{1,j})$. Now, since the low entries of the non-zero columns of $R_{p \+ 1}$ are unique, we have:
\begin{equation}\label{eq:s1}
	\mathrm{dim}(Z_p(K_i) \cap B_p(K_i)) = \lvert \Gamma_p^{i,j} \rvert
\end{equation}
where $\Gamma_p^{i,j}  = \{ \, \mathrm{col}_{R_{p\texttt{+}1}[k] } \neq 0 \mid \, k \in [j], \, 1 \leq \mathrm{low}_{R_{p\texttt{+}1}}[k] \leq i \,\}$. Consider the complementary matrix $\bar{\Gamma}_p^{i,j}$, given by the non-zero columns of $R_{p \+ 1}$ with index at most $j$ that are not in $\Gamma_p^{i,j}$, i.e. the columns satisfying $\mathrm{low}_{R_{p\texttt{+}1}}[k] > i$. Combining rank-nullity with the observation above, we have: 
\begin{equation}\label{eq:s2}
	 \lvert \bar{\Gamma}_p^{i,j} \rvert = \mathrm{dim}(B_p(K_j)) - \lvert \Gamma_p^{i,j} \rvert = \mathrm{rank}(R_{p\+1}^{i\+1,j})
\end{equation}
Combining equations~\eqref{eq:s1} and~\eqref{eq:s2} yields:
\begin{equation}\label{eq:s3}
	\mathrm{dim}(Z_p(K_i) \cap B_p(K_j))  = \lvert \Gamma_p^{i,j}  \rvert 
	= \mathrm{dim}(B_p(K_j)) -  \lvert \bar{\Gamma}_p^{i,j}  \rvert 
	= \mathrm{rank}(R_{p\+1}^{1, j}) - \mathrm{rank}(R_{p\+1}^{i\+1,j})
\end{equation}
Observing the final matrices in~\eqref{eq:s3} are \emph{lower-left} submatrices of $R_{p\+1}$, the final expression~\eqref{eq:betti_four} follows by applying Lemma~\ref{lemma:rank} repeatedly. 
\end{proof}

\subsubsection*{Proof of boundary matrix properties}
\begin{proof}
First, consider property (1). For any $t \in T$, applying the boundary operator $\partial_p$ to $K_t = \mathrm{Rips}_\epsilon(\delta_{\mathcal{X}}(t))$ with non-zero entries satisfying~\eqref{eq:matrix_pchain} by definition yields a matrix $\partial_p$ satisfying $\mathrm{rank}(\partial_p) = \mathrm{dim}(\mathrm{B}_{p-1}(K_t))$. In contrast, definition~\eqref{def:time_boundary_matrix} always produces $p$-boundary matrices of $\Delta_n$; however, notice that the only entries which are non-zero are precisely those whose simplices $\sigma$ that satisfy $\mathrm{diam}(\sigma) < \epsilon$. Thus, $\mathrm{rank}(\partial_p^t) = \mathrm{dim}(\mathrm{B}_{p-1}(K_t))$ for all $t \in T$. 
$<$ (show proof of (2))$>$
Property (3) follows from the construction of $\partial_p$ and from the inequality $\lVert A \rVert_2 \leq \sqrt{m} \lVert A \rVert_1$ for an $n \times m$ matrix $A$, as $\lVert \partial_p^t \rVert_1 \leq (p+1) \, \epsilon$ for all $t \in T$.

	% Assume that $\delta_{\mathcal{X}}$ is $C$-Lipshitz. Then $d_X(t)(x, x') \leq C d_X(t')(x, x')$ for all $x, x' \in X$, then observe $\partial_p^\ast$. 
\end{proof}



%\subsection*{Rank relaxation}
%A common approach in the literature to optimize quantities involving $\mathrm{rank}(A)$ for some $m \times n$ matrix $A$ is to consider optimizing its \emph{nuclear norm} $\lVert A \rVert_\ast = \mathrm{tr}(\sqrt{A^T A}) = \sum_{i=1}^r \lvert \sigma_i \rvert$, where $\sigma_i$ denotes the $i$th singular value of $A$ and $r=\mathrm{rank}(A)$. One of the primary motivations for this substitution is that the nuclear norm is a convex envelope of the rank function over the set: 
%$$
%S := \{ A \in \mathbb{R}^{n \times m} \mid \lVert A \rVert_2 \leq m \}
%$$
%That is, for an appropriate $m > 0$, the function $A \mapsto \frac{1}{m}\lVert A \rVert_\ast$ is a lower convex envelope of the rank function over $S$. The nuclear norm also admits a subdifferential... thus, we may consider replacing~\eqref{} with: 
%\begin{align}\label{eq:betti_four_nuc}
%	\beta_p^{i,j}(t) &= \lvert \partial_{p,t}^{1,i} \rvert -
%	m_1\inv \lVert \partial_{p,t}^{1,i} \rVert_\ast - 
%	m_2\inv \lVert \partial_{\bar{p},t}^{1,j}\rVert_\ast - 
%	m_3\inv \lVert\partial_{\bar{p},t}^{\bar{i},j}\rVert_\ast 
%\end{align}
%where $\bar{c} = c + 1$. Now, if $t \mapsto \partial_p^\ast(t)$ is a non-decreasing, convex function in $t$, then the composition ... is convex, as each of the individual terms are convex. Moreover, we have...
%
%$<$ Insert proof about this relaxation always lower-bounding $\beta$ $>$

%\subsection*{Computation}
%In this section, we discuss the computation of suitable bases for the subspaces $Z_p(X_\ast)$, $B_p(K_\ast)$, and $Z_p(X_\ast) \cap B_p(X_\ast)$. In particular, we address two cases: the \emph{dense} case, wherein the aforementioned bases are represented densely in memory, and the \emph{sparse} case, which uses the structure of a particular decomposition of the boundary matrices to derive bases whose size in memory inherits the sparsity pattern of the decomposition.
%\\
%\\
%\textbf{Sparse case:} We require an appropriate choice of bases for the groups $B_{p-1}(K_\ast)$ and $Z_p(X_\ast) \cap B_p(X_\ast)$. 
%For some fixed $t \in T$, let $R_p = \partial_p V_p$ denote the decomposition discussed above, and let $b, d \in \mathbb{R}_+$ be fixed constants satisfying $b \leq d$. Since the boundary group $B_{p-1}(K_b)$ lies in the image of the $\partial_{p}$, it can be shown that a basis for the boundary group $B_{p-1}(K_\ast)$ is given by: 
%\begin{flalign}
%	&& M_p^b = \{ \, \mathrm{col}_{R_{p+1}}(j) \neq 0 \mid j \leq b \, \}  && span()
%\end{flalign}
%Moreover, since $B_{p-1}(K_b) = \mathrm{Im}(\partial_p^b)$, we have $\mathrm{span}(M_p^b) = B_{p-1}(K_b)$ and thus $\mathrm{rank}(M_p^b) = \mathrm{rank}(\partial_p^b)$. Indeed, it can be shown that every lower-left submatrix of $\partial_p^\ast$ satisfies $\mathrm{rank}(\partial_p^\ast) = \mathrm{rank}(R_p^\ast)$. Thus, although $M_p^b$ does provide a minimal basis for the boundary group $B_{p-1}(K_b)$, it is unneeded here. 
%
%A suitable basis for the cycle group can also be read off from the reduced decomposition directly as well. Indeed, let $R_p = \partial_p V_p$ as before. Then the cycle group is spanned by linear combinations of columns of $V_p$: 
%\begin{equation}
%	Z_p^b = \{ \, \mathrm{col}_{V_p}(j) \mid \mathrm{col}_{R_{p}}(j) = 0, j \leq b \, \}	
%\end{equation}
%The formulation of a basis spanning $Z_p(K_i) \cap B_p(K_j)$ is more subtle, as we can no longer use the  fact that every lower-left submatrix of $R_p$ has the same rank as the same lower-left submatrix of $\partial_p$. 
%Nonetheless, a basis for this group can be obtained by reading off specific columns from $R_p$: 
%\begin{equation}
%	M_p^{b, d} := \{\, \mathrm{col}_{R_{p+1}}(k) \neq 0 \mid 1 \leq k \leq d \text{ and } 1 \leq \mathrm{low}_\mathrm{R_{p+1}}(k) \leq b \, \}
%\end{equation}
%%\begin{flalign}
%%	(\, Z_p(K_i) \cap B_p(K_j) \, ) && M_p^{b, d} := \{\, \mathrm{col}_{R_p}(k) \mid 1 \leq k \leq d \text{ and } 1 \leq \mathrm{low}_\mathrm{R_p}(k) \leq b \, \} &&
%%\end{flalign}
%One can show that $M_b^d$ does indeed span $Z_p(X_\ast) \cap B_p(X_\ast)$ by using the fact that the non-zero columns of $R_p$ with indices at most at most $d$ form a basis for $B_p(K_d)$, and that each low-row index for every non-zero is unique. 
%%The issue here is that 
%\\
%\\
%\noindent
%\textbf{Dense case:} 
%In general, persistent homology groups and its various factor groups are well-defined and computable with the reduction algorithm with coefficients chosen over any ring. By applying operations with respect to a field $\mathbb{F}$, both the various group structures $Z_p(K_\bullet) \subseteq B_p(K_\bullet)  \subseteq C_p(K_\bullet) $ and their induced quotient groups $H_p(K_\bullet)$ are vector spaces; thus, the computation of suitable bases can be approached from a purely linear algebraic perspective.
%In particular, by fixing $\mathbb{F} = \mathbb{R}$, we inherit not only many useful tools for obtaining suitable bases for these groups, but also access to their corresponding optimized implementations as well. 
%
%Consider the $p$-th boundary operator $\partial_p^\ast : C_p(K_\ast) \to C_{p-1}(K_\ast)$ whose matrix realization with respect to some choice of simplex ordering $\{\sigma_i\}_{1 \leq i \leq m}$ we also denote with $\partial_p$. By definition, the boundary group $B_p(K_\ast)$ is given by the image $\mathrm{Im}(\partial_{p+1}^\ast) = B_p(K_\ast)$, thus one may basis for $B_p(K_\ast)$ by computing the considering the first $r > 0$  columns of the reduced SVD: 
%\begin{equation}
%	M_p^\ast = [\, u_1 \mid u_2 \mid \dots \mid u_r \, ] = \{ \,  \, \}
%\end{equation}
%
%
%\subsection{Old}
%
%We consider the problem of maximizing the $p$-th \emph{persistent} Betti number $\beta^{i,j}_p$ over some set $T \subseteq \mathrm{T}$: 
%\begin{equation}
%	t_\ast = \argmax_{t \in T}	 \beta_{p}^{i,j}(t)
%\end{equation}
%As an illustrative example, see Figure. 
%$<$ insert SW1Pers vineyards plot $>$



%Since Betti numbers are integer-valued invariants, direct optimization is difficult. Moreover, the space of persistence diagrams is [banach space statement]....
%Nonetheless, the differentiability of persistence has been studied extensively in [show chain rule paper on persistence diagrams]...



%For a fixed $t \in T$, we obtain a boundary matrix $\partial_{p}^{b,d}$ up to filtration value (diameter) $d \in \mathbb{R}$ for $d_X(t)$. We recall the integer-valued function (equation~\eqref{eq:pb_rank}) we would like to relax. To do this, we substitute the nuclear norm $\lVert \, \cdot \, \rVert_\ast$  for the $\mathrm{rank}$ function and a sigmoid-like function $S_b : K \to \mathbb{R}_{+}$ for the order function $\lvert \, \cdot \, \rvert$, obtaining: 
%\begin{equation}\label{eq:relaxation_pb}
%\hat{\beta}_p^{b,d} = S_b(K) - \lVert \partial_p^b \rVert_{\ast} - \lVert \partial_p^{b,d} \rVert_\ast
%\end{equation} 
%where $S_b(K) = \sum_{\sigma \in K} \mathrm{sigmoid}(\lvert b - \mathrm{diam}(\sigma)\rvert)$.
%Our choice of the nuclear norm is motivated by the fact that it is often used due to its close relationship to the rank function, as first observed by Fazel et al~\cite{} (we discuss this more in section~\ref{}). 

%$<$ TODO: the goal $>$
%First, we that prove the following properties of equation~\eqref{eq:relaxation_pb}:
%\begin{enumerate}
%	\item If $t^\ast = \argmin\limits_{t \in T} \beta_{p}^{b,d}$ and $\hat{t}^\ast = \argmin\limits_{t \in T} \hat{\beta}_{p}^{b,d}$, then $t^\ast = \hat{t}^\ast$
%	\item $\hat{\beta}_{p}^{b,d}(t)$ is continuous as a function of $t \in T$
%	\item $\hat{\beta}_{p}^{b,d}(t)$ admits a subgradient $\hat{\beta}_{p}^{b,d}(t)$
%\end{enumerate}
%We first begin with properties (2) and (3). (2) is obvious... To see (3), consider:
%Equation~\eqref{eq:relaxation_pb} admits a differentiable form amenable to optimization. 
%\begin{equation}
%	\nabla \hat{\beta}_p^{b,d} = \nabla S_b(K) - \nabla \lVert \partial_p^b \rVert_{\ast} \cdot J_b - \nabla \lVert \partial_p^{b,d} \rVert_\ast \cdot J_{b,d}
%\end{equation}
%For any matrix $M \in \mathbb{R}^{n \times m}$ whose corresponding singular value decomposition (SVD) is $M = U \Sigma V^T $, the characterization of the (sub)gradient of $\lVert M \rVert_\ast$ is given by\cite{}: 
%\begin{equation}
%	\partial\|M\|_{*}=\left\{U V^T + W: P_{U} W=0, W P_{V}=0,\|W\| \leq 1\right\}
%\end{equation}
%where $P_U$ ($P_V$, resp.) is an orthogonal projector onto the column space of $U$ ($V$, resp.). For simplicity we set $W = 0$ and obtain: % TODO: write as functional way
% \begin{equation}
%	\nabla \hat{\beta}_p^{b,d} = \nabla S_b(K) - U_b V_b^T J_b - U_{b,d} V_{b,d}^T J_{b,d}
%\end{equation}

%Given a Rips complex, 	$H_p(K_1) \to H_p(K_2) \to \dots \to H_p(K_m)$


\bibliography{pbsig_bib}
\bibliographystyle{plain}


\appendix

\section{Boundary matrix factorization}
\begin{definition}[Boundary matrix decomposition]
Given a filtration $K_\bullet$ with $m$ simplices, let $\partial$ denote its $m \times m$ filtered boundary matrix. We call the factorization $R = \partial V$ the \emph{boundary matrix decomposition} of $\partial$ if:
 \begin{enumerate}[labelsep=3pt, topsep=3pt, itemsep=-0.10ex,parsep=1.2ex]
 	\item[I1.] $V$ is full-rank upper-triangular
 	\item[I2.] $R$ satisfies $\mathrm{low}_R[i] \neq \mathrm{low}_R[j]$ iff its $i$-th and $j$-th columns are nonzero
 	\end{enumerate} 
 	where $\mathrm{low}_R(i)$ denotes the row index of lowest non-zero entry of column $i$ in $R$ or $\mathrm{null}$ if it doesn't exist. Any matrix $R$ satisfying property (I2) is said to be  \emph{reduced}; that is, no two columns share the same low-row indices.
\end{definition}



\section{Laplacian facts}
In general, the spectrum of the graph Laplacian $L$ is unbounded,~\cite{} and instead many prefer to work within the ``normalized'' setting where eigenvalues are bounded. The \emph{normalized Laplacian} $\mathcal{L}$ of a graph $G$ is typically given as: 
\begin{equation}
	\mathcal{L}(G) = D^{-1/2} L D^{-1/2}
\end{equation} 
with the convention that $D^{-1}(v_i,v_i) = 0$ for $\mathrm{deg}(v_i) = 0$. 
% Note that $\mathcal{L}(G)$ can also be written in a form $\partial_1 \partial_1^T$ by replacing entries column entries $(1,-1)$ corresponding to edges $e = (u,v)$ with $(1 /\sqrt{\mathrm{deg(u)}}, -1 /\sqrt{\mathrm{deg(v)}})$~\cite{ChungSGT}.
The variational characterization of eigenvalues in terms of the Rayleigh quotient of $\mathcal{L}$ convey a particular form. Specifically, for any real-valued function $f : V \to \mathbb{R}$ on $G$, when viewed as a column vector, $\mathcal{L}$ satisfies:
\begin{equation}\label{eq:lap_rayleigh}
	\frac{\langle f, \mathcal{L} f \rangle}{\langle f, f \rangle} = \frac{\sum\limits_{i \sim j} (g(v_i) - g(v_j))^2}{\sum\limits_{i} g(v_i)^2  \cdot \mathrm{deg}(v_i) }
\end{equation}
where $f = D^{1/2} g$ and $\langle f, g\rangle$ denotes the standard inner product in $\mathbb{R}^n$.
Equation~\eqref{eq:lap_rayleigh} may be used to show that the spectrum $\Lambda(\mathcal{L})$ is bounded in the interval $[0, 2]$. In particular, it is known that:
\begin{equation}
	\lambda_i \leq \sup_{f} \frac{\langle f, \mathcal{L} f \rangle}{\langle f, f \rangle} \leq 2
\end{equation}
Recall that, when $G$ is connected, $0$ is an eigenvalue of both $L$ and $\mathcal{L}(G)$, with multiplicity $\mathrm{cc}(G)$. Moreover, if $G$ is the union of disjoint graphs $G_1, G_2, \dots, G_k$, then it has as its spectrum the union of the spectra $ \Lambda(G_1), \Lambda(G_2), \dots, \Lambda(G_k)$. 
Certain parts of the spectrum of $\mathcal{L}$ can be deduced explicitly for very structured types of $G$, such as complete graphs, complete bipartite graphs, star graphs, path graphs, and cycle graphs, and $n$-cubes. 
For a list of additional properties the graph and normalized Laplacians satisfy, including bounds on eigenvalues, relation to random walks and rapidly-mixing Markov chains,  identities tied to isoperimetric properties of graphs, and explicit connections to spectral Riemannian geometry, see~\cite{chung1997spectral} and references within.



\section{Laplacian \texttt{matvec} products}\label{sec:up_laplace_matvec}
Below is pseudocode outlining how to evaluate a weighted (up) Laplacian matrix-vector multiplication built from a simplicial complex $K$ with $m = \lvert K^{p+1} \rvert$ and $n = \lvert K^{p} \rvert$ in essentially $O(m)$ time when $m > n$ and $p$ is considered a small constant. 
Key to the runtime of the operation being essentially linear is the constant-time determination of orientation between $p$-faces ($s_{\tau, \tau'}$)---which can be inlined during the computation---and the use of a deterministic $O(1)$ hash table $h : K^{p} \to [n]$ for efficiently determining the appropriate input/output offsets to modify ($i$ and $j$). 
Note the degree computation occurs only once. 

\begin{algorithm}[H]\label{alg:lap_matvec}
\renewcommand{\algorithmicensure}{\textbf{Optional:}}
%\algrenewcommand\algorithmicoptional{\textbf{Output:}}
\caption{\texttt{matvec} for weighted $p$ up-Laplacians in $O(m(p+1)) \approx O(m)$ time ($p \geq 0$)}
\begin{algorithmic}[1]
\Require Fixed oriented complex $K$ of size $N=\lvert K \rvert$ 
\Ensure Weight functions $w_{p+1}: K^{p+1} \to \mathbb{R}_{+}$ and  $w_{p}^l, w_{p}^r: K^p \to \mathbb{R}_{+}$
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Ensure $y = \langle \, L_p^{\mathrm{up}}, x \, \rangle =  (W_p \circ \partial_{p+1} \circ W_{p+1} \circ \partial_{p+1}^T \circ W_p)x$
\State \# Precompute weighted degrees $\mathrm{deg}_w$ 
\State $h : K^p \to [ \, n \, ]$ 
\State $\mathrm{deg}_w \gets \mathbf{0}$
\For{$\sigma \in K^{p+1}$}: 
	\For{$\tau \in \partial[\sigma]$}:
   	 	\State $\mathrm{deg}_w[h(\tau)] \gets \mathrm{deg}_w[h(\tau)] + w_p^l(\tau) \cdot w_{p+1}(\sigma) \cdot  w_{p}^r(\tau)$
   	\EndFor
\EndFor
\State 
%\State \# Compute Laplacian \texttt{matvec} for $x$ 
\Function{UpLaplacianMatvec}{$x \in \mathbb{R}^n$}
\State $y \gets \mathrm{deg}_w \odot x$ \; \text{(element-wise product)}
%\State $z_{h_p(\tau)} \gets z_{h_p(\tau)} \cdot f(\tau)^2 \quad \forall \tau \in K^p$
\For{$\sigma \in K^{p+1}$}:
%    $\mathrm{sgn}(\partial[\sigma])$
    \For{$\tau, \ \tau' \in \partial[\sigma] \times \partial[\sigma]$ \textbf{where} $\tau \neq \tau'$}: 
    		\State $s_{\tau, \tau'} \gets \mathrm{sgn}([\tau], \partial[\sigma]) \cdot \mathrm{sgn}([\tau'], \partial[\sigma])$
    		\State $i, \; j \gets \; h(\tau), \; h(\tau')$
    		\State $y_i \gets y_i + s_{\tau, \tau'} \cdot x_{j} \cdot w_p^l(\tau) \cdot w_{p+1}(\sigma) \cdot w_p^r(\tau')$
    \EndFor 
\EndFor
\State \Return $y$
\EndFunction
\end{algorithmic}
\end{algorithm}

In general, the signs of the coefficients $\mathrm{sgn}([\tau], \partial[\sigma])$ and $\mathrm{sgn}([\tau'], \partial[\sigma])$ depend on the position of $\tau, \tau'$ as summands in $\partial[\sigma]$~\eqref{eq:alt_sum}, which itself depends on the orientation of $[\sigma]$~\eqref{eq:oriented_simplex}. Thus, evaluation of these sign terms takes $O(p)$ time to determine for a given $\tau \in \partial[\sigma]$ with $\mathrm{dim}(\sigma) = p$, which if done naively via line (12) in the pseudocode~\ref{alg:lap_matvec} increases the complexity of the algorithm. 
However, observe that the sign of their product is in fact invariant in the orientation of $[\sigma]$ (see Remark 3.2.1 of~\cite{goldberg2002combinatorial})---thus, if we fix the orientation of the simplices of $K^p$, the sign pattern $s_{\tau, \tau'}$ for every $\tau \overset{\sigma}{\sim} \tau'$ can be precomputed and stored ahead of time, reducing the evaluation $s_{\tau, \tau'}$ to $O(1)$ time and $O(m)$ storage. 
Alternatively, if the labels of the $p+1$ simplices $\sigma \in K^{p+1}$ are given an orientation induced from the total order on $V$, then we can remove the storage requirement entirely and simply fix the sign pattern during the computation. 

A subtle but important aspect of algorithmically evaluating~\eqref{eq:l_up_matvec} is the choice of indexing function $h: K^p \to [n]$. This map is necessary to deduce the contributions of the components $x_\ast$ during the operation (line (13)). 
While this task may seem trivial as one may use any standard associative array to generate this map, typical implementations that rely on collision-resolution schemes such as open addressing or chaining only have $O(1)$ lookup time in expectation.
Moreover, empirical testing suggests that line (13) in~\ref{alg:lap_matvec} can easily bottleneck the entire computation due to the scattered memory access such collision-resolution schemes may involve. 
%Preliminary benchmarks suggest 
One solution avoiding these collision resolution schemes that exploits the fact that $K$ is fixed is to  build an order-preserving \emph{perfect minimal hash function} (PMHF) $h : K^p \to [n]$. 
It is known how to build PMHF's over fixed input sets of size $n$ in $O(n)$ time and $O(n \log m)$ bits~\cite{}, and such maps have deterministic $O(1)$ access time. Note that this process happens only once for a fixed simplicial complex $K$: once $h$ has been constructed, it is fixed for every $\mathtt{matvec}$ operation. 
% TODO: mention boissonants data structures



\section{Parameterized setting \& Perturbation theory}
If $f$ is a real-valued filter function that various smoothly in $\mathcal{H}$, one would expect the spectra of the constitutive terms in $\beta_p^\ast$ and $\mu_p^\ast$ to also vary smoothly as functions of $\mathcal{H}$.
Indeed, since Laplacian matrices are normal matrices, we expect their spectra to be quite stable under perturbations~\cite{}. 
%Moreover,

% The spectrum of Normal matrices are stable: if A is normal and is perturbed by a perturbation w/ norm at most \epsilon, then the spectrum moves by at most \epsilon. And Laplacian matrices are normal matrices
% https://terrytao.wordpress.com/2008/10/28/when-are-eigenvalues-stable/
%In what follows, we fix $p \geq 0$ and use $L_{(h)}^{i,j}$ to denote the $p$-th up Laplacian at time $h \in \mathcal{H}$.
%Let $\hat{\partial}_h$

Small condition numbers often improve the convergence of iterative solvers and improve stability of spectrum with respect to perturbations in the entries of the matrix. 
$\kappa(M^{-1} A)$

% the desired effect in applying a preconditioner is to make the quadratic form of the preconditioned operator {\displaystyle P^{-1}A}P^{{-1}}A with respect to the {\displaystyle P}P-based scalar product to be nearly spherical.
$$
M^{-1} A x = M^{-1}b 
$$
where $M$ is symmetric positive definite. 
\begin{equation}
	\min_{x \perp \mathbf{1} } \frac{1}{2} x^T (L + \epsilon I_n) x - b^T x
\end{equation}
Since this nonsingular, positive definite, strictly diagonally dominant matrix, thus we may apply the famous Conjugate Gradient (CG) algorithm to solve such a system. It's well known that CG converges to the solution of $A x = b$ in exactly $O(n)$ iterations (and often much earlier), of which each iteration requires one $O(m)$ matrix-vector product, implying a runtime of $O(mn^2)$ (compare with...).  
%http://www.cs.yale.edu/homes/spielman/561/2009/lect16-09.pdf
Moreover, and since this is a Laplacian matrix, the wealth of tools developed for said matrices may also be used.
In particular, ~\cite{} showed that \emph{low-stretch spanning trees} act as good preconditioners to accelerate Laplacian solvers, wherein it's been shown that the preconditioned Conjugate Gradient (PCG) requires as most $O(\sqrt{m} \log n)$ iterations, each of which requires one matrix-vector product using $L_G$ and in $O(m^{1/3} \log n \, \mathrm{ln} 1/\epsilon)$ iterations. 
This was later improved by, who showed that one can solve Laplacian systems effectively in $O(m \log^{O(1)} n)$ time, giving a bound of $O(r m \log^{O(1)} n)$ time to obtain.... 

Of course, if one wants to compute either of the counting invariants in... exactly for $p = 0$, of course, the fastest algorithm is to reduce the problem to the well-known elder-rule problem, which takes $O(m \log m + m \alpha(n))$ time for a general filtration. It is unlikely that we may beat this bound, either in theory or in practice, for $p = 0$.
However, the fastest known algorithm for computing the full persistence diagram for $p \geq 1$ is $O()$, which is quite a jump in complexity; there is no generalization of disjoint-set algorithm for the case where $p \geq 1$. 
Moreover, these direct methods tend to be memory bound operations, pushing researchers who want to compute these diagrams in practice to focus on ways of reducing the memory usage, such as using $\mathbb{Z}_2$ field coefficients. 
In contrast, the means by which we compute these invariants scales quite well with larger $p$, it produces a stronger invariant, and is far more reaching to other areas of mathematics. 




% ----- Junk ------
% 
% Lipshitz statement about boundary matrix
% At the algebraic level, persistent homology admits a canonical decomposition for coefficients in any choice of field~\cite{}, though at the expense of torsion information.\footnote{}
% Given a strict total order $(V, <)$ on the vertices of $K$, define the ranking function $\varsigma_p(\tau) : K_p \to [m_p]$ which ranks the $p$-simplices of $K$ in a fixed way according to the order given by $<$. 
% If $\sigma$
%We begin by extending the standard definition of an elementary $p$-chain to the dynamic setting. Recall a $p$-chain of a simplicial filtration $K_\bullet$ with coefficients in $\mathbb{F}$ is a function $c_p$ on the oriented $p$-simplices of $K$ satisfying $c_p(\sigma) = -c_p(\sigma')$ if $\sigma$ and $\sigma'$ are opposite orientations of the same simplex, and $c_p(\sigma) = 0$ otherwise. 
%A $p$-chain is called \emph{elementary with respect to $q \in \mathbb{F}$} if it satisfies:
%\begin{align*}
%	c_p(\sigma) &= +q  \quad & \\
%	c_p(\sigma') &= -q \quad &\text{if } \sigma' \text{ is the opposite orientation of }\sigma \\
%	c_p(\tau) &= 0 \quad & \text{otherwise}
%\end{align*}
%Once all $p$-simplices of $K$ are oriented, each $p$-chain can be written unique as a finite linear combination $c_p = \sum_{i=0}^p n_i \sigma_i$ 
%of the corresponding elementary chains $\sigma_i$. 
%\begin{equation}
%	\partial_p(\sigma_i) = \partial_p[ v_0, \dots, v_p ] = \sum\limits_{i = 0}^p q(-1)^i [v_0, \dots, \hat{v_i}, \dots, v_p]
%\end{equation}
%where the notation $\hat{v}_p$ means that $v_p$ is excluded in the $i$-th summand, and $[v_0, \dots, v_p]$ denotes the oriented simplex. 
%\begin{definition}[Time-varying elementary $p$-chain]
%	An elementary $p$-chain $c_p : T \times K$ is said to be time-varying if $c_p(\cdot)(\sigma) = f(\sigma; t)$ is continuous in $T$. 
%\end{definition}


\end{document}
