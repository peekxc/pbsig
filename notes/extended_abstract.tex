\documentclass[10pt twocolumn]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subfiles}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{scalefnt}

\usepackage{multicol} 

\usepackage{mdframed}
\newenvironment{boxedenumerate}
  {\begin{mdframed}[font=\small, linewidth=1pt]}
  {\end{mdframed}}
  
\usepackage{tcolorbox}

\numberwithin{equation}{section}

%\usepackage{titlesec}

\usepackage{algorithm}
\PassOptionsToPackage{noend}{algpseudocode}
\usepackage{algpseudocode}
\usepackage{float}% http://ctan.org/pkg/float

\RequirePackage{fix-cm}

\usepackage{nicematrix}

%\usepackage{mdframed}
\mdfdefinestyle{bframe}{%
    outerlinewidth=1pt,
    innertopmargin=0,
    innerbottommargin=5pt,
    innerrightmargin=8pt,
    innerleftmargin=8pt,
    backgroundcolor=white
   }
   
\usepackage{empheq}
\usepackage{xcolor}
%\subfile{whiteboxes}
\definecolor{shadecolor}{cmyk}{0,0,0,0}
\newsavebox{\mysaveboxM} % M for math
\newsavebox{\mysaveboxT} % T for text

\newcommand*\boxAppOne[2][Application \#1: Vectorizing persistence information]{%
  \sbox{\mysaveboxM}{#2}%
  \sbox{\mysaveboxT}{\fcolorbox{black}{white}{#1}}%
  \sbox{\mysaveboxM}{%
    \parbox[t][\ht\mysaveboxM+.5\ht\mysaveboxT+.5\dp\mysaveboxT][b]{\wd\mysaveboxM}{#2}%
  }%
  \sbox{\mysaveboxM}{%
    \fcolorbox{black}{shadecolor}{%
      \makebox[\linewidth-1em]{\usebox{\mysaveboxM}}%
    }%
  }%
  \usebox{\mysaveboxM}%
  \makebox[15pt][r]{%
    \makebox[\wd\mysaveboxM][l]{%
      \raisebox{\ht\mysaveboxM-0.5\ht\mysaveboxT+0.5\dp\mysaveboxT-0.5\fboxrule}{\usebox{\mysaveboxT}}%
    }%
  }%
}

\newcommand*\boxAppTwo[2][Application \#2: Differentiating persistence information]{%
  \sbox{\mysaveboxM}{#2}%
  \sbox{\mysaveboxT}{\fcolorbox{black}{white}{#1}}%
  \sbox{\mysaveboxM}{%
    \parbox[t][\ht\mysaveboxM+.5\ht\mysaveboxT+.5\dp\mysaveboxT][b]{\wd\mysaveboxM}{#2}%
  }%
  \sbox{\mysaveboxM}{%
    \fcolorbox{black}{shadecolor}{%
      \makebox[\linewidth-1em]{\usebox{\mysaveboxM}}%
    }%
  }%
  \usebox{\mysaveboxM}%
  \makebox[15pt][r]{%
    \makebox[\wd\mysaveboxM][l]{%
      \raisebox{\ht\mysaveboxM-0.5\ht\mysaveboxT+0.5\dp\mysaveboxT-0.5\fboxrule}{\usebox{\mysaveboxT}}%
    }%
  }%
}
  
  
\usepackage{chngcntr}


\counterwithin*{equation}{section}
\usepackage{scalerel}
\usepackage[small]{titlesec}

\newcommand{\+}{%
	\raisebox{0.18ex}{\scaleobj{0.55}{+}}
%  \raisebox{\dimexpr(\fontcharht\font`X-\height+\depth)/2\relax}{\scaleobj{0.5}{+}}%
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}

\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}

%\newlength\myindent 
%\setlength\myindent{6em} 
%\newcommand\bindent{
%  \begingroup 
%  \setlength{\itemindent}{\myindent} 
%  \addtolength{\algorithmicindent}{\myindent} 
%}
%\newcommand\eindent{\endgroup} % closes a group

\usepackage{dsfont}

\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{example}{Example}[section]

\newcommand{\inv}{^{\raisebox{.2ex}{$\scriptscriptstyle-1$}}}
\newcommand\sbullet[1][.5]{\mathbin{\vcenter{\hbox{\scalebox{#1}{$\bullet$}}}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\bigzero}{\mbox{\normalfont\Large\bfseries 0}}
\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}

% Smooth Betti curves in dynamic settings \\ using persistent spectral theory
\title{\vspace{-2.0em} 
Spectral families of \\ persistent rank invariants
\vspace{-0.5em}}

\author{Matt Piekenbrock\thanks{Khoury College of Computer Sciences, Northeastern University.} \, and Jose A. Perea\thanks{Department of Mathematics and Khoury College of Computer Sciences, Northeastern University.}}
\date{}

\begin{document}
\begin{multicols}{2}
\maketitle
\noindent
\textbf{Abstract:} \emph{Using a duality result between persistence diagrams and persistence measures, we introduce a framework for constructing families of continuous relaxations of the persistent rank invariant for parametrized families of persistence vector spaces indexed over the real line. Like the rank invariant, these families obey inclusion-exclusion, are derived from simplicial boundary operators, and encode all the information needed to construct a persistence diagram. 
Unlike the rank invariant, these spectrally-derived families enjoy a number of stability and continuity properties typically reserved for persistence diagrams, such as smoothness and differentiability over the positive semi-definite cone. 
%Key to achieving a $(1-\epsilon)$-approximation is the use of spectral L{\"o}wner decompositions 
%Our proposed $(1-\epsilon)$-approximation relies on uses nonconvex spectral functions  composition with the spectral decomposition relax boundary operators with L{\"o}wner operators. 
%equipping the space of cochains over $\mathbb{R}$ with an inner product. 
%Fundamental to the family we propose is their characterization as spectral functions.
%By exploiting a connection to combinatorial Laplacian operators, we find that the non-harmonic spectra from which our interpolation derives encodes rich geometric information about the underlying space, providing several avenues for geometric data analysis. 
Leveraging a connection to combinatorial Laplacian operators, we find the non-harmonic spectra of our proposed relaxation encode valuable geometric information about the underlying space, prompting several avenues for geometric data analysis.
%Surprisingly, we find the relaxation may be efficiently standard persistence computation and it may be iteratively approximated in a "matrix-free" fashion. 
%Exemplary applications in topological data analysis and machine learning, such as hyper-parameter optimization and shape classification, are investigated in the full paper.
}
\\
\\
\noindent \textbf{Background:} Persistent homology pipelines typically follow a well-established pattern: given an input data set $X$, construct a filtration $(K, f)$ from $X$ such that useful topological or geometric information may be profitably gleaned from its \emph{persistence diagram}---a 
%given a tame function $f: \mathcal{X} \to \mathbb{R}$ over a topological space $\mathcal{X}$,
%its $p$-th persistence diagram $\mathrm{dgm}_p(f)$ over $f$ is the 
multiset summary of $(K,f)$ constructed by pairing homological critical values $\{ a_i \}_{i=1}^n$ with non-zero \emph{multiplicities} $\mu_p^{i,j}$ or Betti numbers $\beta_p^{i,j}$~\cite{cohen2005stability}: 
\begin{align*}
	\mathrm{dgm}_p(f) \triangleq & \; \{ \, (a_i, a_j) :  \mu_p^{i,j} \neq 0 \, \} \; \cup \; \Delta \\
\mu_p^{i,j} \triangleq & \; \left(\beta_p^{i,j\shortminus1} - \beta_p^{i,j} \right) - \left(\beta_p^{i\shortminus1,j\shortminus1} - \beta_p^{i\shortminus1,j} \right)
\end{align*}\label{eq:dgm}
\noindent
By pairing simplices using homomorphisms between homology groups, diagrams demarcate homological features succinctly.
The essential quality of persistence is that this pairing exists, is unique, and is stable under additive perturbations~\cite{cohen2005stability}.
Persistence is the de facto connection between homology and the application frontier.

Though theoretically sound, diagrams suffer from several practical issues: they are sensitive to strong outliers, far from injective, expensive to compute, and expensive to compare. 
%Performing even basic statistical operations, such as averaging, has proven difficult under the standard matching metrics~\cite{}. 
Practitioners have tackled some of these issues by equipping diagrams with additional structure by way of maps to function spaces---examples include persistence landscapes~\cite{bubenik2015statistical} and references therein. 
These diagram vectorizations have proven useful for learning applications due to their stability and  metric configurability.
The scalability issue remains exacerbated though, as these vectorizations require diagrams as part of their input. %moves
\\
\\
\noindent \textbf{Approach:}
Rather than adding structure to precomputed diagrams, we propose a spectral method that performs both steps, simultaneously and approximately. 
Our approach constructs a vector-valued mappings over a \emph{parameter space} $\mathcal{A} \subset \mathbb{R}^d$: 
\begin{equation*}\label{eq:relaxation_mapping}
	(X_\alpha, \mathcal{R}, \epsilon, \tau) \mapsto \mathbb{R}^{O(\lvert \mathcal{R} \rvert)}
\end{equation*}
where $\{X_\alpha\}_{\alpha \in \mathcal{A}}$ is an  $\mathcal{A}$-parametrized family of data sets, $\mathcal{R} \subset \Delta_+$ a rectilinear \emph{sieve} over the upper half-plane $\Delta_+$, and $(\epsilon, \tau) \in \mathbb{R}_+^2$ are approximation/smoothness parameters, respectively.
Our strategy is motivated by measure-theoretic perspectives on $\mathbb{R}$-indexed persistence modules~\cite{cerri2013betti, chazal2016structure}, which generalize $\mu_p^{i,j}$ to arbitrary \emph{corner points} $(\, \hat\imath, \hat\jmath \,) \in \Delta_+$:
%= \{ \, (i,j) \in \bar{\mathbb{R}}^2  \mid i < j \, \}
\begin{multline*}\label{eq:measure}
\mu_p^{\hat\imath, \hat\jmath} = \min_{\delta > 0} \left(\beta_p^{\hat\imath \+ \delta, \hat\jmath \shortminus \delta} \shortminus \beta_p^{\hat\imath \+ \delta, \hat\jmath  \+ \delta} \right) \shortminus \left(\beta_p^{\hat\imath \shortminus \delta, \hat\jmath \shortminus \delta} \shortminus \beta_p^{\hat\imath \shortminus \delta, \hat\jmath  \+ \delta} \right)
\end{multline*}
and also by a technical observation that shows the multiplicity function is expressible as a sum of \emph{unfactored} boundary operators $\partial_p : C_p(K) \to C_{p-1}(K)$---that is, given a fixed $p \geq 0$, a filtration $K = \{K_i\}_{i\in [N]}$ of size $N = \lvert K \rvert$, and a rectangle $R = [i,j] \times [k,l] \subset \Delta_+$, the $p$-th multiplicity $\mu_p^{R}$ of $K$ is given by:
	\begin{equation*}
	\mu_p^{R} = 
	\mathrm{rank}\begin{bmatrix} \partial_{p\+1}^{j \+ 1, k} & 0 \\
	0 & \partial_{p\+1}^{i \+ 1, l}
	\end{bmatrix}
	- 
	\mathrm{rank}\begin{bmatrix} \partial_{p\+1}^{i \+ 1, k} & 0 \\
	0 & \partial_{p\+1}^{j \+ 1, l}
	\end{bmatrix}
%	\\
%	\mathrm{rank}(\partial_{p\+1}^{j \+ 1, k})  - \mathrm{rank}(\partial_{p\+1}^{i \+ 1, k})  - \mathrm{rank}() + \mathrm{rank}(\partial_{p\+1}^{i \+ 1, l}) 
	\end{equation*}
\noindent 
where $\partial_p^{i, j}$ denotes the lower-left submatrix of $\partial_p$ defined by the first $j$ columns and the last $m - i + 1$ rows.
%In the full paper, we show that in $\approx O(m)$ memory and $\approx O(mn)$ time, where $m, n$ are the number of $p+1, p$ simplices in the complex, respectively. 
An explicit proof of this can be found in~\cite{dey2022computational}, though it was also noted in passing by Edelsbrunner~\cite{edelsbrunner2000topological}---it can proved by combining the Pairing Uniqueness Lemma with the fact that left-to-right column operations preserves the ranks of ``lower-left'' submatrices.
Though often used to show the correctness of the reduction algorithm from~\cite{edelsbrunner2000topological}, the implications of this fact are quite general, as noted recently by Bauer et al.~\cite{bauer2022keeping}:
\begin{proposition}[\cite{bauer2022keeping}]\label{prop:bauer}
	Any persistence algorithm which preserves the ranks of the submatrices $\partial^{i,j}(K_\bullet)$ for all $i,j \in [N]$ is a valid persistence algorithm. 
\end{proposition}
\end{multicols}
%require explicitly filtered complexes $K$ to reside in memory .
%breaking away from the reduction paradigm of algorithms.
%We find the approximation to also be efficient in practice: the complexity reduction does not use Zigzag persistent homology~\cite{milosavljevic2011zigzag} nor does it rely on Strassen-like reductions to the matrix multiplication time $O(n^\omega)$.
\begin{figure}[t!]\label{fig:overview}
\centering
\includegraphics[width=0.85\textwidth]{spectral_spri_overview}	
\caption{ (Left) Vineyards analogy of diagrams at `snapshots' over time; 
(middle) vineyard curves flattened with a sieve $\mathcal{R} \subset \Delta_+$; 
(right) the integer-valued multiplicity function $\mu_p^{\mathcal{R}}(f_\alpha)$ 
as a function of time $\alpha \in \mathbb{R}$ (top) and a real-valued spectral relaxation (bottom)
}
\end{figure}

\begin{multicols}{2}
\noindent 
\textbf{Spectral rank invariant: }
Our proposed mapping exploits proposition~\ref{prop:bauer} via a spectral characterization of $\mu_p^R$. 
%Using the duality between rank functions and diagrams, we not only avoid explicitly constructing diagrams, but we in fact avoid using the reduction algorithm (\cite{edelsbrunner2022computational}) entirely.  
%As the vectorization we propose continuously interpolates between the rank invariant and a spectral operator, we elucidate a connection between persistence and other areas of applied mathematics, such as Tikhonov regularization, compressive sensing, and iterative subspace  methods.
%Moreover, inspired by a relationship established between the persistent Betti numbers and combinatorial Laplacian operators~\cite{}, we show our vectorization able to harvest the rich geometric information such operators encode for tasks like shape classification and filtration optimization.  
%Interestingly, our results also imply the existence of an efficient output-sensitive algorithm for computing $\Gamma$-persistence pairs with at least ($\Gamma >0$)-persistence (via~\cite{chen2011output}) that requires the operator $x \mapsto \partial x$ as its only input, which we consider to be of independent interest. 
% sifts through ... something like vineyards, limits towards multiplicity, A x W
%We interpret $\mathcal{A}$ \emph{parameter space} 
In particular, let $K$ denote a fixed simplicial complex constructed from a data set $X$ and $f_\alpha$ a continuous filter function satisfying, for all $\alpha \in \mathcal{A}$:
	\begin{equation*}
		(K, f_\alpha) \triangleq \{ \, f_\alpha : K \to \mathbb{R} \mid f_\alpha(\tau) \leq f_\alpha(\sigma)  , \tau \subseteq \sigma \in K\}
	\end{equation*}
Our methods inputs are $(K, f_\alpha)$, a sieve $\mathcal{R} \subset \Delta_+$,
%Exemplary choices of $f_\alpha$ include filtrations geometrically realized from methods that themselves are have parameters, such as time-varying filtrations~\cite{kim2021spatiotemporal} or restrictions of bifiltrations.
%Select a rectilinear \emph{sieve} $\mathcal{R} \subset \Delta_+ = \{ \, (i,j) \in \bar{\mathbb{R}}^2  \mid i < j \, \}$ that is decomposable to a disjoint set of rectangles.
%	\begin{equation}\label{eq:rect_sieve}
%		\mathcal{R} = R_1 \cup R_2 \cup \dots \cup R_h
%	\end{equation} 
%	This choice typically requires a priori knowledge and is application-dependent. 
	%In section~\ref{sec:applications} we give evidence random sampling may be sufficient for vectorization or exploratory purposes, when $\mathcal{R}$ is unknown.
%determines the dimension of the map~\eqref{eq:relaxation_mapping},
%	\item Choose rectangle $R = [i,j] \times [k,l] \subset \Delta_+$ in the upper-half plane $\Delta_+ = \{\}$
and parameters $(\epsilon, \tau ) \in \mathbb{R}_+^2$ representing how \emph{closely} and \emph{smoothly} the relaxation should model the quantity: 
	\begin{equation*}\label{eq:mu_alpha}
		\mu_p^{\mathcal{R}}(f_\alpha) \triangleq \big\{ \, 
	\mathrm{card}\left(\restr{\mathrm{dgm}_p(f_\alpha)}{\mathcal{R}} \right) \mid \alpha \in \mathcal{A} \, \big\}
	\end{equation*}
The intuition is that $\mathcal{R}$ filters and summarizes topological and geometric behavior exhibited by $X_\alpha$ for all $\alpha \in \mathcal{A}$, thereby \emph{sifting} the space $\mathcal{A} \times \Delta_+$. 
Our proposed approximation first associates a \emph{normalized combinatorial Laplacian} operator $\mathcal{L} : C^p(K, \mathbb{R}) \to C^p(K, \mathbb{R})$ to the corner points on the boundary of $\mathcal{R}$. Then, for some $v \in \mathrm{span}(\textbf{1})^\perp$, we restrict and project $\mathcal{L}$ onto the following Krylov subspace: 
	\begin{equation*}
	\mathcal{K}_n(\mathcal{L}, v) \triangleq \mathrm{span}\{ v, \mathcal{L}v, \mathcal{L}^2 v, \dots, \mathcal{L}^{n-1}v \}
	\end{equation*}
	We can show (1) the eigenvalues of $T = \mathrm{proj}_{\mathcal{K}} \restr{\mathcal{L}}{\mathcal{K}}$ provide an $(1-\epsilon)$-approximation of $\mu_p^{\mathcal{R}}(f_\alpha)$, 
	%, converging towards $\mu_p^{\mathcal{R}}(f_\alpha)$ as both $(\epsilon, \tau)$ decrease towards $0^+$.
	and (2) varying $\tau > 0$ yields a family of spectral operators whose Schatten-1 norms are Lipshitz continuous, stable under relative perturbations, and differentiable on the positive semi-definite cone. Moreover, as the parameters $\epsilon$ and $\tau$ approach zero, the multiplicity $\mu_p^{i,j}$ is recovered exactly. 

Unlike existing dynamic persistence algorithms, our approach requires no complicated data structures or maintenance procedures to implement, can be made \emph{matrix-free}, and is particularly efficient to compute over parameterized families of inputs.
We defer the formal analysis, properties, and applications of the method to full\footnote{The paper in preparation can be found at: \url{https://github.com/peekxc/pbsig/blob/main/notes/pbsig.pdf}} paper, in preparation. 
%The proposed relaxation is also , requiring only as much memory as is needed to enumerate simplices in the underlying complex $K$.

%\noindent \textbf{Future Remarks:} The remaining steps of the relaxation depend on the application in mind. 
%The duality between diagrams and rank functions suggests any application exploiting vectorized persistence information may benefit from our relaxation.
%The differentiability of our relaxation enables learning applications seeking to optimize persistence information, such as filtration optimization, incorporating topological priors into loss functions, and automated filtration sparsification.
%Compared to the classical reduction methods~\cite{edelsbrunner2022computational, zomorodian2004computing}, the primary advantage of the rank-based expressions from~\eqref{eq:betti_four}-\eqref{eq:mu_four} is that they imply the complexity of obtaining either $\beta_p^{i,j}(K_\bullet)$ or $\mu_p^{R}(K_\bullet)$ may be reduced to the complexity of computing the rank of a set of submatrices of $\partial$---a fact that actually motivated the rank-based persistence algorithm from Chen et al~\cite{chen2011output}.
%Our contributions in this effort stem from the observation that the constitutive terms in these expressions are \emph{unfactored} boundary (sub)matrices---thus, the operation $x \mapsto \partial x$ can be implemented without actually constructing $\partial$ in memory, enabling the use of e.g. iterative Krylov or subspace acceleration methods~\cite{golub2013matrix, parlett1994we} for their computation. 
%%Moreover, the measure-theoretic counter-parts~\eqref{eq:pbn_cont} and~\eqref{eq:measure} suggests we may re-use  
%Indeed, this line of thought suggests other algebraic properties of the rank function---such as invariance under permutations and adjoint multiplication---may simplify these rank-based expressions even further.
%%By working with persistence modules indexed with real-valued coefficients, we naturally recover the measure-theoretic perspective from~\eqref{eq:measure}.
%The full version of this abstract explores these questions and their ramifications in detail. 
\bibliography{pbsig_bib}
\bibliographystyle{plain}
 
\end{multicols}
\end{document}