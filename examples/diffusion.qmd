---
title: "Diffusion w/ Laplacians"
format: html
jupyter: python3
editor: 
  render-on-save: true
---

```{python}
#| label: Imports
import bokeh
from bokeh.io import output_notebook
from bokeh.plotting import figure, show
from bokeh.layouts import row, column
from pbsig.vis import figure_complex
from pbsig.color import bin_color
output_notebook()
```


```{python}
#| label: Heat kernel on random Delaunay
import numpy as np
from pbsig.linalg import * 
from splex import *
from scipy.sparse.linalg import eigsh

## Generate random laplacian
np.random.seed(1234)
X = np.random.uniform(size=(50,2), low=0, high=1)
S = delaunay_complex(X)

## Generic way to generate multiple plots with colored vertices
def figure_heat(S: ComplexLike, X: ArrayLike, vertex_weights: Sequence, lb, ub, **kwargs):
  p = figure_complex(S, X, **kwargs)
  v_color = bin_color(vertex_weights, color_pal="inferno", lb=lb, ub=ub)
  p.scatter(*X.T, color=v_color, size=10.5)
  return p

## Show heat diffusion from a single vertex 
ind = np.argmin(X @ np.array([0,1]))
HK = HeatKernel(timepoints=6, approx="mesh", bounds="informative").fit(X, S=S)
heat_diffusion = list(HK.diffuse(subset=ind))
figs = [figure_heat(S, X, heat, lb = 0.0, ub = max(heat), width=200, height=200) for heat in heat_diffusion]
show(row(*figs))
```

```{python}
#| label: Import MPEG7
from pbsig.datasets import mpeg7
from pbsig.distance import dist 
NV = 100
data_params = dict(simplify = NV+1, which=['bird', 'hammer', 'turtle'], shape_nums = np.arange(8))
X_data = mpeg7(**data_params)
X, y = mpeg7(**data_params, return_X_y = True)
S = cycle_graph(NV)
```

```{python}
#| label: Visualize MPEG7 shapes 
from pbsig.vis import * 
patch = lambda X, color: figure_plain(figure_patch(X, color=color, width=50, height=50, alpha=0.40, line_width=2.5))
p = column(
  row([patch(x.reshape(NV, 2), "red") for x in X[y == 0]]),
  row([patch(x.reshape(NV, 2), "green") for x in X[y == 1]]), 
  row([patch(x.reshape(NV, 2), "blue") for x in X[y == 2]])
)
show(p)
```

```{python}
#| label: REPLACE
from scipy.sparse import diags
from pbsig.distance import dist
from pbsig.utility import cycle_window
S = cycle_graph(NV)
triangles = list(cycle_window(range(NV), offset=4, w=5))
S.update(triangles)
S = simplicial_complex(chain(faces(S, 0), faces(S, 1), faces(S, 2)))
```

```{python}
#| label: Visualize HKS on MPEG7
## Formulate the mesh Laplacian the normalize by the mass matrices
from pbsig.linalg import * 
X_pos = X[14,:].reshape(NV, 2)
HK = HeatKernel(approx="mesh").fit(X_pos, S=S)

## informative seems like a good heuristic
HK.timepoints = geomspace(*HK.time_bounds("informative", interval=[0.0,1.0]), p=32, reverse=False)
hks = HK.signature(HK.timepoints)

## Plot the MDS embedding of the signatures 
hks_emb = cmds(squareform(pdist(hks)**2))
p = figure_scatter(hks_emb, color=bin_color(np.arange(X_pos.shape[0]), "turbo"), width=250, height=200)

## + their similarities to extremal points
ind = np.argmax(X_pos @ np.array([0,1]))
q = figure_complex(S, X_pos, width=250, height=200)
q.scatter(*X_pos.T, color=bin_color(np.linalg.norm(hks - hks[ind], axis=1), "turbo"), size=5.5)
q.scatter(*X_pos[ind].T, color="purple", size=10.5)

ind = np.argmax(X_pos @ np.array([0,-1]))
r = figure_complex(S, X_pos, width=250, height=200)
r.scatter(*X_pos.T, color=bin_color(np.linalg.norm(hks - hks[ind], axis=1), "turbo"), size=5.5)
r.scatter(*X_pos[ind].T, color="purple", size=10.5)

ind = np.argmax(X_pos @ np.array([np.cos((7/4)*np.pi), np.sin((7/4)*np.pi)]))
s = figure_complex(S, X_pos, width=250, height=200)
s.scatter(*X_pos.T, color=bin_color(np.linalg.norm(hks - hks[ind], axis=1), "turbo"), size=5.5)
s.scatter(*X_pos[ind].T, color="purple", size=10.5)

show(row(p,q,r,s))
```

The below uses `signal_dist` to resolve the correspondence problem per-signature. Because of this, there is no natural embedding 
for each timepoint/signature, as although they are all 1-dimensional the metric varies per pair. 

Since the number of vertices is fixed, if a fixed correspondence between shapes was set, we could use graph diffusion distance instead.  

```{python}
#| label: MDS + HKS + dist plot over time
#| description: Compare each individual feature vector in HKS w/ MDS embedding plot where each row represents a different t
from pbsig.dsp import signal_dist
HK = HeatKernel(approx="mesh", timepoints = 6, bounds = "effective")
# HK.param_laplacian(approx="mesh")
HKS = [HK.clone().fit(x.reshape(NV, 2), S=S, normed=False) for x in X]

signal_dist_norm = partial(signal_dist, normalize="sum") 
cfigs = []
for i in range(6):
  figs = []

  ## Compute minimum rollover distance between specific signatures
  sigs = np.array([hk.signature()[:,i] for hk in HKS])
  sigs_norm_shift = pdist(sigs, metric=signal_dist_norm)
  emb = cmds(squareform(sigs_norm_shift**2))

  ## MDS plot
  p = figure_scatter(emb, color=["red"]*7 + ["green"]*7 + ["blue"]*7, width=180, height=180)
  p.toolbar_location = None
  p.xaxis.visible = False
  p.yaxis.visible = False
  figs.append(column(p))

  ## Rows of shape HKS plots
  shape_row = []
  for j in np.unique(y):
    class_ind = np.flatnonzero(y == j)
    shape_sig_it = zip(X[class_ind], sigs[class_ind])
    scatters_figs = [figure_scatter(x.reshape(NV, 2), color=bin_color(sig, "turbo"), width=60, height=60) for x, sig in shape_sig_it]
    shape_row.append(row(*[figure_plain(p) for p in scatters_figs]))
  figs.append(column(shape_row))

  ## Distance plot 
  figs.append(column(figure_dist(sigs_norm_shift, width=180, height=180)))

  ## Combine
  cfigs.append(row(*figs))

show(column(*cfigs))
```

TODO: Figure out way to give more weight to closer distances in signal distance, say with gaussian exp kernel...  

Next, we generate an "average" heat kernel signature for each class of shapes at some fixed time `t > 0`. To do this, we phase-align the HKS of each 
signature at time `t` to fixed but arbitrary reference signature by minimizing the cross-correlation. This gives a canonical correspondence 
between each pair of vectors. We then take the average at each point using the correspondence, for all shapes in the given class.

To do this for multiple values of `t`, one can perform the same steps with a slight augmentation: in step 1, rather than minimizing cross correlation, minimize multiple cross correlation simulataneously across mutliple signals. This is like simultanous signal phase alignment. Then, in step 2, use generalized procrustes analysis...

```{python}
#| label: Class-specific signature barycenters for a fixed time t.  
from pbsig.dsp import phase_align, signal_dist
from pbsig.classifiers import HeatKernelClassifier
from sklearn import clone

## Globally fitted timepoints
# all_timepoints = np.array([hk.time_bounds("informative") for hk in HKS])
# t_bnd = np.min(all_timepoints[:,0]), np.max(all_timepoints[:,1])
# HK.timepoints = geomspace(*t_bnd, 6)


## The base heat kernel object is repeatedly cloned
HK = HeatKernel(approx="mesh")

## First fit takes about 22 seconds to complete once
## Takes ~15 seconds to refit 21 models 1500 times, => about 0.15 seconds need to change sample-weight
norm_dist = partial(signal_dist, normalize="sum") # improves scores dramatically
hk_model = HeatKernelClassifier(HK, dimension = 2, metric=norm_dist) # euclidean assumes correspondence
hk_model.initial_fit(X=X, y=y, S=S, normed=True, approx="mesh")
hk_model.fit(X, y)
hk_model.transform(X)
hk_model.warm_start
hk_model.X_hash
clone(hk_model).warm_start
clone(hk_model).dimension
clone(hk_model).X_hash
clone(hk_model).heat_kernels


# hk_model.predict(X)
# hk_model.score(X, y)

# hk_model.signature_ = np.reciprocal(hk_model.signature_.sum(axis=1))[:,np.newaxis] * hk_model.signature_

## Get an idea of how the scores are doing in absolute range
# scores = np.array([hk_model.fit(X, y).score(X, y) for _ in range(30)])

```

Why doens't sklearn respect clone? Ahh, needs >= 1.3
```{python}
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.utils.multiclass import unique_labels
from sklearn.metrics import euclidean_distances
class TemplateClassifier(BaseEstimator, ClassifierMixin):
  def __init__(self, demo_param='demo'):  
    self.demo_param = demo_param

  def fit(self, X, y):
    X, y = check_X_y(X, y)
    self.classes_ = unique_labels(y)
    self.X_, self.y_ = X, y
    return self

  def predict(self, X):
    check_is_fitted(self)
    X = check_array(X)
    closest = np.argmin(euclidean_distances(X, self.X_), axis=1)
    return self.y_[closest]
  
  def clone(self):
    print("clone called")
    return self
  
  def __copy__(self):
    print("copy called")
    return self

  def __deepcopy__(self):
    print("deepcopy called")
    return self
  
  def __sklearn_clone__(self):
    print("sklearn clone called")
    return self


from sklearn import clone
C = TemplateClassifier()
c = clone(C)
```


```{python}
#| label: Expanded MPEG random classifiers 
## TODO: expand mpeg7 size + classes 
data_params = dict(simplify = NV+1, which="default", shape_nums = np.arange(12))
X_data = mpeg7(**data_params)
X, y = mpeg7(**data_params, return_X_y = True)
S = cycle_graph(NV)
triangles = list(cycle_window(range(NV), offset=4, w=5))
S.update(triangles)
S = simplicial_complex(chain(faces(S, 0), faces(S, 1), faces(S, 2)), form="tree")

## Fit the initial model
HK = HeatKernel(approx="mesh")
hk_model = HeatKernelClassifier(HK, dimension = 2, metric=norm_dist) # euclidean assumes correspondence
hk_model.initial_fit(X=X, y=y, S=S, normed=True, approx="mesh")
print(hk_model.fit(X, y))

## Show the scores of randomized models
scores = np.array([hk_model.fit(X, y).score(X, y) for _ in range(30)])
```


```{python}
#| label: Boosting HKS weak learners

from sklearn.ensemble import AdaBoostClassifier
HK = HeatKernel(approx="mesh")
HK_classifier = HeatKernelClassifier(HK, dimension = 2, metric=norm_dist)
HK_classifier.initial_fit(X=X, y=y, S=S, normed=True, approx="mesh")
assert HK_classifier.X_hash == clone(HK_classifier).X_hash, "Weak learner needs to be cloneable"

HK_classifier.fit(X, y)
HK_classifier.transform(X)
HK_classifier.predict(X)

HK_classifier.X_hash
HK_classifier.clone()




ab = AdaBoostClassifier(estimator=HK_classifier, n_estimators=15, learning_rate=1e-2)

ab.fit(X, y)

ab.predict(X)
ab.score(X, y)
```

```{python}
## The 1-dimensional case for a fixed time point is easy 
HK_sigs = np.array([np.ravel(hk.signature([hk.timepoints[1]])) for hk in HKS])
HK_learner = barycenter_classifier("HKS")
wl = HK_learner(metric=signal_dist) # weak learner
wl.fit(HK_sigs, y)
wl.score(HK_sigs, y)

## Hypothesis space: 1d signatures at some give time 
# lambda t: np.array([np.ravel(hk.signature([t])) for hk in HKS]) ## for range of t
# lambda t: np.array([np.ravel(hk.signature([hk.timepoints[i]])) for hk in HKS]) ## for i in [0,n-1]

def random_hks_transformer(HK, t_min: float, t_max: float, random_state = None):
  """Generates a random weak learner using the heat kernel signature. 
  """
  from pbsig.linalg import logsample
  # assert len(X) == len(HK), "Must pass a list of heat kernels, one fit for each value in X."
  # assert len(X) == len(y), "length of input data 'X' must match length of label vector 'y'."
  # t_min, t_max = HK.time_bounds("informative")
  rng = np.random.RandomState(random_state)
  ## TODO: expand beyond single time parameter using procrustes
  def _fit_transform_hks(X: ArrayLike, y: np.ndarray = None, **kwargs) -> np.ndarray:
    t = logsample(t_min, t_max, num=np.array([rng.uniform()])) # time for diffusion 
    HKS = [HK.param_laplacian("mesh", x.reshape(len(x) // 2, 2), normed=False).fit() for x in X]
    HKS_sigs = np.array([np.ravel(hk.signature([t])) for hk in HKS])
    return HKS_sigs
  return _fit_transform_hks 

from sklearn.preprocessing import FunctionTransformer
hks_transformer = random_hks_transformer(HeatKernel(S), 1.0, 1000.0, random_state = None)
f = FunctionTransformer(hks_transformer, validate=True)
# f.fit_transform(X[:5,:]).shape

HK_learner = barycenter_classifier("HKS")
wl = HK_learner(metric=signal_dist) # weak learner

pipeline = Pipeline([('hks', f), ('bary', wl)])

pipeline.fit(X, y)
pipeline.predict(X)

## Final estimator is a barycentric classifier
## TODO: expand beyond single time parameter using procrustes
# HKS_learner = barycenter_classifier("HKS")
# wl = HKS_learner(metric=signal_dist) # weak learner
# wl.fit(HK_sigs, y)

random_hks_wl(HKS, y, 10, 1000).score(X, y)



## Convert a deterministic estimator/classifier into a randomized one over some hypothesis space
class HypothesisSpace(BaseEstimator):
  """A _hypothesis space_ is a space of estimators that can be sampled from.
  
  It takes as input: 
    1. An unsupervised Vectorizer (Transformer) with support for randomized vectorization (.fit_transform(X, random_state = ...))
    2. A supervised Estimator (BaseEstimator) with support for weighted prediction or classification (.fit(X, y, sample_weight, ...))

  Given these two as input, an estimator a parameterized that support sklearn's base estimator (https://scikit-learn.org/stable/developers/develop.html) and random_state (https://scikit-learn.org/stable/glossary.html#term-random_state) API guidelines.

  .fit() should satify (https://scikit-learn.org/stable/glossary.html#term-fit)

  The estimator tag of this is set to non_deterministic (https://scikit-learn.org/stable/developers/develop.html#estimator-tags)
  """
  def __init__(estimator: BaseEstimator, vectorizer: BaseEstimator, random_state = None):
    self.estimator = estimator        ## supervised estimator 
    self.vectorizer = vectorizer      ## transformer with support for a random state
    self.random_state = random_state  ## set the random_state

  def fit(X: Sized, y: np.ndarray, sample_weight = None):
    from copy import deepcopy
    assert isinstance(y, np.ndarray) and all(y >= 0), "y must be non-negative integers classes"
    sample_weight = np.ones(len(X))/len(X) if sample_weight is None else sample_weight
    assert np.isclose(np.sum(sample_weight), 1.0, atol=1e-8), f"Sample weight must be a distribution ({np.sum(sample_weight):8f} != 1 )."
    rng = np.random.RandomState(self.random_state)
    V = self.vectorizer.fit_transform(X, random_state = rng)
    self.estimator_ = self.estimator.fit(V, y, sample_weight)
    return self

  def decision_function(X: Sized):
    V = self.vectorizer.fit_transform(X, random_state = self.random_state)
    return self.estimator_.decision_function(V)

  def predict_prob(X: Sized):
    V = self.vectorizer.fit_transform(X, random_state = self.random_state)
    return self.estimator_.predict_proba(V)

  def predict(X: Sized):
    V = self.vectorizer.fit_transform(X, random_state = self.random_state)
    return self.estimator_.predict(V)

    timepoint = self.hspace.rvs(size=1)
    HK_sigs = np.array([np.ravel(hk.signature([timepoint])) for hk in HKS])
    
    ## 
    self.estimator.fit(HK_sigs)


    while not(hasattr(self, "bins_")) or (False if not(ensure_better) else self.score(X, y) <= 0.50):
      self.bins_ = self.bins if isinstance(self.bins, np.ndarray) else np.sort(rng.uniform(low=0, high=1.0, size=self.bins+1))
      self.classes_, y = np.unique(y, return_inverse=True) # TODO: import from cache
      self.n_classes_ = len(self.classes_)                 # TODO: import from cache
      
      ## Re-weight the sample weights to be a distribution on each class
      class_weights, norm_weight = {}, deepcopy(sample_weight)
      for cl in self.classes_: 
        class_weights[cl] = np.sum(norm_weight[y == cl])
        norm_weight[y == cl] = norm_weight[y == cl] / class_weights[cl] if class_weights[cl] > 0 else norm_weight[y == cl]


## Requirements: 

from sklearn.ensemble import AdaBoostClassifier
A_skl = AdaBoostClassifier(ShellsClassifier(bins=15), n_estimators=15, learning_rate=0.01, algorithm="SAMME")
```


```{python}
from pbsig.classifiers import barycenter_classifier
# barycenter_2d = lambda x, w: (w[:,np.newaxis] * x.reshape(len(x) // 2, 2)).mean(axis=0) if w is not None else x.reshape(len(x) // 2, 2).mean(axis=0)

hks = HK.clone().param_laplacian("mesh", x.reshape(NV, 2), normed=False).fit().signature()

HKS_learner = barycenter_classifier("HKS", center=barycenter_2d)
wl = HKS_learner(metric=signal_dist) # weak learner

2
wl.fit(X, y)
weak_learner.score(X_train, y_train)


shape_keys = [('turtle', i) for i in range(1, 4)] + [('bird', i) for i in range(1,4)]
HK = HeatKernel(S)
HKS = [HK.clone().param_laplacian("mesh", X_data[k], normed=False).fit() for k in shape_keys]

## Figure out some global time bounds 
all_timepoints = np.array([hk.time_bounds("informative") for hk in HKS])
all_signatures = np.array([np.ravel(hk.signature()) for hk in HKS])
t_bnd = np.min(all_timepoints[:,0]), np.max(all_timepoints[:,1])
global_time = geomspace(*t_bnd, 64)

## Look at the signature embeddings of all these guys for comparison
HKS_emb = [cmds(squareform(pdist(hk.signature(global_time))**2)) for hk in HKS]
show(row(*[figure_scatter(emb, width=150, height=150) for emb in HKS_emb]))


## start uniformly sampling in the time interval, see how many 
sigs = np.array(all_sigs)
hks_emb = cmds(squareform(pdist(sigs)**2))

## Why isnt this similar????
s1, s2 = all_sigs[0], all_sigs[3]
np.min([np.linalg.norm(s1 - np.roll(s2, 199*i)) for i in range(32)])

p = figure(width=200, height=200)
p.scatter(*hks_emb.T, color=["red"]*3 + ["blue"]*3)
show(p)

for X_pos in X_data.values():
  HK.param_laplacian(approx="mesh", X=X_pos, normed=True).fit()
  all_timepoints.append(geomspace(*HK.time_bounds("effective", interval=[0.0,1.0]), p=32, reverse=False))

hks = HK.signature(HK.timepoints)


```



```{python}
#| label: PNG plots 
stretch = lambda x: np.where(x < 0.5, 2*x ** 2, 1 - 2*(1-x)**2) ## maps [0,1] -> [0,1] monotonically, stretching extrema more
normalize = lambda x: (x - np.min(x))/(np.max(x) - np.min(x))

## Highlight points with low HKS distance / similar HKS  to a fixed point 
ind = np.argmax(X_pos @ np.array([np.cos((7/4)*np.pi), np.sin((7/4)*np.pi)]))
sdist_to_pt = np.linalg.norm(hks - hks[ind], axis=1)
pt_col = bin_color(sdist_to_pt, "turbo")
pt_col[:,3] = 1.0 - stretch(normalize(sdist_to_pt))

from bokeh.io import export_png
from bokeh.io.export import get_screenshot_as_png
s = figure(width=400, height=400)
s.patch(*X_pos.T, fill_color="gray", fill_alpha=0.10, line_alpha=0.0, hatch_pattern="dashed")
s.scatter(*X_pos.T, line_color=None, fill_color=pt_col, size=25.5)
s.background_fill_alpha = 0
s.xaxis.visible = False
s.yaxis.visible = False
s.toolbar_location = None
s.grid.visible = False
s.background_fill_color = None
s.border_fill_color = None
show(s)

## This works
from PIL import Image
# temp = export_png(s, filename="temp.png")
temp = Image.open(temp)

## So does this - and this works with transparency! But neither screenshot nor export support transparency
temp = get_screenshot_as_png(s, height=400, width=400)
im = np.array(temp.convert("RGBA"))
im = im.view("uint32").reshape(im.shape[:2]) # no idea why this works but uint8 doesnt'

p = figure(x_range=(-10,10), y_range=(-10,10), width=500, height=500)
p.image_rgba(image=[im], x=0, y=0, dw=2, dh=2, anchor="center")
show(p)

# This does not
p = figure(x_range=(-10,10), y_range=(-10,10), width=500, height=500)
p.image_url([temp], x=0, y=0, w=4, h=4, anchor="center")
show(p)

# show(figure_complex(delaunay_complex(X_pos)))


# hks_emb = cmds(squareform(pdist(hks)**2))
# from bokeh.models import Div
# p = figure(width=450, height=450, x_range=(-10,10), y_range=(-10,10))
# # p.scatter(*hks_emb.T, color=bin_color(np.arange(X_pos.shape[0]), "turbo"))
# div_image = Div(text=f"<img src=\"file://{temp}\" alt=\"div_image\">", width=150, height=150)
# # p.image_url(url=['http://pngimg.com/uploads/cat/cat_PNG106.png'], x=0, y=0, w=3, h=3, anchor="center", syncable=False)

# show(div_image)

## TODO: continue this by making small images with voronoi cells and everything to show BoF

# s.scatter(*X_pos[ind].T, color="purple", size=10.5)
show(s)

from more_itertools import * 
def cycle_shift(seq, n) -> list:
  N = len(seq)
  L_ind = [take(n,l) for l in circular_shifts(range(N))]
  L_ind = L_ind[-(n // 2):] + L_ind[:-(n // 2)]
  return L_ind

indices = cycle_shift(range(X_pos.shape[0]), 15)

show(p)
print("hello")
p = figure(width=100, height=100)
p.patch(X_pos[np.array(indices[0]),0], X_pos[np.array(indices[0]),1], alpha=0.80, line_width=1.5)

show(p)

# BagOfFeatures()
```







```{python}
#| label: Visualize HKS on MPEG7
from scipy.sparse import diags
from pbsig.distance import dist

from pbsig.utility import cycle_window
X_pos = X_data[('turtle',1)]
S = cycle_graph(X_pos.shape[0])
triangles = list(cycle_window(range(X_pos.shape[0]), offset=2, w=3))
S.update(triangles)

from pbsig.shape import triangle_areas
from pbsig.linalg import vertex_masses
from pbsig.linalg import timepoint_heuristic

## Formulate the mesh Laplacian the normalize by the mass matrices
from pbsig.linalg import heat_kernel_signature
L_sim = up_laplacian(S, weight=gauss_similarity(S, X_pos), p=0, normed=False)
A_mass = diags(vertex_masses(S, X_pos)) # mass matrix
# timepoints = timepoint_heuristic(32, L_sim, A_mass, locality=(0.95,1.5))
timepoints = timepoint_heuristic(64, L_sim, A_mass, locality=(0.10,0.50)) ## higher seems to be quite smooth! 
hks = heat_kernel_signature(L_sim, A_mass, timepoints=timepoints)

## Plot the MDS embedding of the signatures + their similarities to the top point
hks_emb = cmds(squareform(pdist(hks)**2))
p = figure(width=250, height=250)
p.scatter(*hks_emb.T, color=bin_color(np.arange(X_pos.shape[0]), "turbo"))

ind = np.argmax(X_pos @ np.array([0,1]))
q = figure_complex(S, X_pos, height=250, width=250)
q.scatter(*X_pos.T, color=bin_color(np.linalg.norm(hks - hks[ind], axis=1), "turbo"), size=10.5)
q.scatter(*X_pos[ind].T, color="purple", size=5.5)

ind = np.argmax(X_pos @ np.array([0,-1]))
r = figure_complex(S, X_pos, height=250, width=250)
r.scatter(*X_pos.T, color=bin_color(np.linalg.norm(hks - hks[ind], axis=1), "turbo"), size=10.5)
r.scatter(*X_pos[ind].T, color="purple", size=5.5)

ind = np.argmax(X_pos @ np.array([np.cos((7/4)*np.pi), np.sin((7/4)*np.pi)]))
s = figure_complex(S, X_pos, height=250, width=250)
s.scatter(*X_pos.T, color=bin_color(np.linalg.norm(hks - hks[ind], axis=1), "turbo"), size=10.5)
s.scatter(*X_pos[ind].T, color="purple", size=5.5)

show(row(p,q,r,s))
```

```{python}
#| label: Reproduce HKS trace from graph diffusion distance paper
from pbsig.linalg import *
S1 = cycle_graph(4) # square graph 
S2 = cycle_graph(5) # pentagon graph 
S3 = simplicial_complex([*cycle_window(range(7)), *cycle_window(range(7,14)), [14,0], [14,7]]) # simple dumbbell 
S4 = simplicial_complex([*faces(range(7), 1), *faces(range(7, 14), 1), [14,0], [14,7]])        # complex dumbbell 

normed = True
timepoints = logspaced_timepoints(10, lb=0, ub=2.0)
HK1 = HeatKernel(S1).param_laplacian(normed=normed).param_solver(shift_invert=False).fit()
HK2 = HeatKernel(S2).param_laplacian(normed=normed).param_solver(shift_invert=False).fit()
HK3 = HeatKernel(S3).param_laplacian(normed=normed).param_solver(shift_invert=False).fit()
HK4 = HeatKernel(S4).param_laplacian(normed=normed).param_solver(shift_invert=False).fit()

## The trace is discriminating, but notice that they scale essentially by |V|
## Normalizing doesn't seem to affect the trace almost at all 
p = figure(width=200, height=200, x_axis_type="log")
p.line(timepoints, HK1.trace(timepoints)/4, color="red")
p.line(timepoints, HK2.trace(timepoints)/5, color="green")
p.line(timepoints, HK3.trace(timepoints)/15, color="blue")
p.line(timepoints, HK4.trace(timepoints)/15, color="cyan")
show(p)

## The un-scaled heta content is discriminating, but the vertex-scaled versions are very close in norm
## Normalizing dramatically effects the heat content
# indicator = lambda i,n: np.array([0]*(i) + [1] + [0]*(n-i-1))
# indicator_edge = lambda i,j,n: indicator(i,n) + indicator(j,n)

# [np.ones(15) @ hk @ np.ones(15) for hk in HK4.diffuse(timepoints)]
# [np.ones(4) @ hk @ np.ones(4) for hk in HK1.diffuse(timepoints)]
# HKC = np.array([np.array([indicator(i, 4) @ hk @ indicator(i, 4) for hk in HK1.diffuse(timepoints)]) for i in range(4)])
# HKC = np.array([np.array([indicator_edge(i,j,4) @ hk @ indicator_edge(i,j,4) for hk in HK1.diffuse(timepoints)]) for i,j in faces(S1,1)])
# HKC = np.array([np.array([indicator_edge(i,j,4) @ hk @ indicator_edge(i,j,4) for hk in HK1.diffuse(timepoints)]) for i,j in combinations(range(4),2)])

# HKC = np.array([np.array([indicator(i, 15) @ hk @ indicator(i, 15) for hk in HK4.diffuse(timepoints)]) for i in range(15)])

# hkc = HKC.sum(axis=0)
# np.append(hkc[0], hkc[0] + np.cumsum(np.abs(np.diff(hkc))))

## Heat kernel content -- equivalent to cumulative heat kernel trace differences
## i.e. ~ c + np.cumsum(np.abs(np.diff([tr(HK(t)) for t in T])))
p = figure(x_axis_type="log")
p.line(timepoints, HKC.sum(axis=0))
show(p)

p,q = [figure(width=200, height=200, x_axis_type="log") for i in range(2)]
p.line(timepoints, HK1.content(timepoints), color="red")
p.line(timepoints, HK2.content(timepoints), color="green")
p.line(timepoints, HK3.content(timepoints), color="blue")
p.line(timepoints, HK4.content(timepoints), color="cyan")
q.line(timepoints, HK1.content(timepoints)/4, color="red")
q.line(timepoints, HK2.content(timepoints)/5, color="green")
q.line(timepoints, HK3.content(timepoints)/15, color="blue")
q.line(timepoints, HK4.content(timepoints)/15, color="cyan")
show(row(p,q))

## Heat kernel signature is quite telling, very descriptive
## square has 1 class of vectors, pentagon has 5, simple barbell has 5 and complex barbell has 3
p = figure(width=200, height=200, x_axis_type="log")
for s in HK1.signature(timepoints): p.line(timepoints, s, color="red")
for s in HK2.signature(timepoints): p.line(timepoints, s, color="green")
for s in HK3.signature(timepoints): p.line(timepoints, s, color="blue")  
for s in HK4.signature(timepoints): p.line(timepoints, s, color="cyan")  
show(p)
```


























```{python}
HK2.trace(timepoints)
HK3.trace(timepoints)
HK4.trace(timepoints)

## Graph diffusion distance -- needs pairing of vertices + same sized graphs
# [np.linalg.norm(A1 - A2, "fro")**2 for A1, A2 in zip(HK1.diffuse(), HK2.diffuse())]


HK = HeatKernel(S1)

HK.clone().fit(emb, k=10)


## Can't do it, Object.x eagerly resolves to the value, i.e. obj.x is not a property type, but the value itself, precluding 
## obj.x()...
## Based on: https://stackoverflow.com/questions/17330160/how-does-the-property-decorator-work-in-python
class builder_property(property):
  def __init__(self, fget=None, fset=None, fdel=None, doc=None):
    super().__init__(fget, fset, fdel, doc)
    
  def __call__(self, obj: Any = None, value: Any = None):
    if obj is None: return self
    if value is None: return self.fget(obj)
    if self.fset is None:
      raise AttributeError("can't set attribute")
    self.fset(obj, value)
    return self

class Test():
  def __init__(self):
    self._x = 0

  @builder_property
  def x(self):
    return self._x
  
  @x.setter
  def x(self, value):
    self._x = value



def get_x(self): return self._x
def set_x(self, value): self._x = value
p = BuilderProperty(fget = get_x, fset = set_x)
p.fget(t)
p.fset(t, 2)

p(t)

setattr(p, "__call__", lambda self: 5)

t = Test()
t.x
t.x = 1
t.x()
t

```


```{python}
#| label: HKS simplified 
from pbsig.linalg import HeatKernel
HK = HeatKernel(S)
HK.fit(X_pos)
HK.signature()
HK.trace()

HK.timepoints = 64

HK.fit(X_data[('turtle',2)])
eigsh(HK.laplacian_, k=10, M=HK.mass_matrix_, which="LM", return_eigenvectors=False)

heat_sigs = HK.signature()
hks_emb = cmds(squareform(pdist(heat_sigs)**2))
p = figure(width=200, height=200)
p.scatter(*hks_emb.T, color=bin_color(np.arange(X_pos.shape[0]), "turbo"))
show(p)




# list(HK.diffuse())
# for i, emb in enumerate(X_data.values()):
#   HK.clone().fit(emb, k=10)
#   eigsh(A=HK.laplacian_, M=HK.mass_matrix_, k=10, which="SM", return_eigenvectors=False)
#   solver = PsdSolver(HK.laplacian_, return_eigenvectors=True)
#   solver(A=HK.laplacian_, M=HK.mass_matrix_, k=10, which="LM", sigma=1e-6)

#   self = HK
#   self.laplacian_ = up_laplacian(self.complex, p=0, weight=gauss_similarity(self.complex, emb), normed=False)
#   self.mass_matrix_ = diags(vertex_masses(self.complex, emb)) 
#   solver = eigh_solver(self.laplacian_, laplacian=True)
#   ew, ev = solver(A=self.laplacian_, M=self.mass_matrix_, which="LM", k=10, sigma=1e-6) ## solve Ax = \lambda M x 
#   self.eigvecs_ = ev
#   self.eigvals_ = np.maximum(ew, 0.0)

## Fix the timepoints across all the fittings
HK.timepoints *= 1e-4


heat_kernels = [HK.clone().fit(emb, k=10) for emb in X_data.values()]

## Show the shapes
keys = list(X_data.keys())
key_ind = [keys.index(('turtle',1)), keys.index(('turtle',2)), keys.index(('turtle',3)), keys.index(('bird',1)), keys.index(('bird',2)), keys.index(('bird',3))]
p = row([patch_plot(X_data[keys[sk]], sc) for sk, sc in zip(key_ind, ["red", "red", "red", "blue", "blue", "blue"])])
q = figure(width=250, height=200) #  x_axis_type="log"
for i, c in zip(key_ind, ["red", "red", "blue", "blue"]):
  tp = heat_kernels[i].timepoints
  q.line(np.arange(len(tp)), heat_kernels[i].trace(), color=c)

show(q)

heat_kernels[0].trace()
heat_kernels[1].trace()
heat_kernels[20].trace()
heat_kernels[21].trace()


heat_kernels[0].eigvals_[:6]
heat_kernels[1].eigvals_[:6]
heat_kernels[20].eigvals_[:6]
heat_kernels[21].eigvals_[:6]


[hk.trace() for hk in heat_kernels]
# max(HK.timepoints) / (2.0 / 1e-6)
```


```{python}
#| label: Bag of features

def HKS_emb(S: ComplexLike, emb: ArrayLike, timepoints: int, **kwargs):
  L_sim = up_laplacian(S, weight=gauss_similarity(S, emb), p=0, normed=True)
  A_mass = diags(vertex_masses(S, emb)) # mass matrix
  timepoints = timepoint_heuristic(timepoints, L_sim, A_mass) ## higher seems to be quite smooth! 
  return heat_kernel_signature(L_sim, A_mass, timepoints, **kwargs)

hks_mpeg = [HKS(S, X_pos, 32, k=20) for X_pos in X_data.values()]

from pbsig.dsp import signal_dist
signal_dist(np.ravel(hks_mpeg[0]), np.ravel(hks_mpeg[22]))

p = figure(width=450, height=200)
p.line(np.arange(len(X_pos)*32), np.ravel(hks_mpeg[0]), color="blue")
p.line(np.arange(len(X_pos)*32), np.ravel(hks_mpeg[1]), color="red")
show(p)

from pbsig.classifiers import BagOfFeatures
BOF = BagOfFeatures(np.vstack(hks_mpeg))
bof = BOF.fit(k=24).transform(hks_mpeg)

p = figure(width=200, height=200)
p.scatter(*cmds(squareform(pdist(bof))**2).T, color=bin_color(y, "Category10"))
show(p)

X_data



# hks[0,32:]
# from scipy.sparse.csgraph import floyd_warshall
# A = -(L_sim - diags(L_sim.diagonal()))
# np.max(np.max(floyd_warshall(A), axis=1))
# np.log(n-1)/(np.log(ub+lb)/np.log(ub-lb))
# # import networkx as nx
# # 
# # G = nx.from_scipy_sparse_array(A)

# list(nx.all_pairs_shortest_path_length(G))
```


```{python}
# E_ind = np.array(list(faces(S,1)))
# E_dist = dist(X_pos[E_ind[:,0]], X_pos[E_ind[:,1]], paired=True)
# D1 = boundary_matrix(S, p=1)
# # L = D1 @ diags(np.exp(-E_dist)) @ D1.T
# # L = D1 @ diags(1/(1 + E_dist)) @ D1.T
# L = D1 @ diags(E_dist) @ D1.T
# D = diags(pseudoinverse(np.sqrt(L.diagonal())))
# LN = D @ L @ D
# LN = up_laplacian(S, p=0, normed=True)

# def build_mass_matrix(mesh : trimesh.Trimesh):
#     """Build the sparse diagonal mass matrix for a given mesh

#     Args:
#         mesh (trimesh.Trimesh): Mesh to use.

#     Returns:
#         A sparse diagonal matrix of size (#vertices, #vertices).
#     """
#     areas = np.zeros(shape=(len(mesh.vertices)))
#     for face, area in zip(mesh.faces, mesh.area_faces):
#         areas[face] += area / 3.0

#     return scipy.sparse.diags(areas)



## Compute the heat kernel diffusion for placement of heat at specific vertex weighted by distance
# ind = np.argmin(X_pos @ np.array([0,1]))
# ew, ev = eigsh(LN, k=LN.shape[0]-1, which="SM")
# gen_fig = lambda: figure_complex(S, X_pos, height=250, width=250)
# v_gen = diffuse_heat(ev, ew, logspaced_timepoints(10, ub=0.2), subset=ind)
# show(row(*fig_colors(gen_fig, X_pos, v_gen)))



```


```{python}
## Compute the heat kernel signaure for random diffusion based on distance
from pbsig.linalg import heat_kernel_signature
# H = heat_kernel_signature(LN, timepoints=logspaced_timepoints(10, ub=0.2))
# gen_fig = lambda: figure_complex(S, X_pos, height=200, width=200)
# show(row(*fig_colors(gen_fig, X_pos, H)))

## Why is the heat kernel signature useless?
ew, ev = eigsh(LN, k=LN.shape[1]-1, which="SM")
T = logspaced_timepoints(20, ub=0.2)
hks_T = np.array([np.sum(np.exp(-t * ew) * (ev ** 2), axis=1) for t in T]).T
int_S = diags([1/np.sum(np.exp(-t * ew)) for t in T])
hks_TS = hks_T @ int_S

# hks_T.sum(axis=0) == np.array([np.sum(np.exp(-t * ew)) for t in T])
# hks = ((ev[:, :]**2) @ (ew[:,None] * logspaced_timepoints(20, ub=0.2))).sum(axis=1)

from pbsig.shape import landmarks
l_ind, l_rad = landmarks(X_pos, 10)
l_col = bin_color(np.arange(10), "category10")

p, q = figure_complex(S, X_pos, width=200, height=200, simplex_kwargs={ 0 : {'size' : 4.5}}), figure(width=200, height=200, x_axis_type="log")
p.scatter(*X_pos[l_ind].T, color=l_col, size=10.5)
for i, c in zip(l_ind, l_col): 
  q.line(T, hks_TS[i], line_color=rgb_to_hex(c*255))
show(row(p,q))

#  np.square(eigenvectors).dot(np.diag(np.exp(-time * eigenvals))).sum(axis=1)

## Show distance between scaled HKS's
v_color = bin_color(np.arange(X_pos.shape[0]), "turbo")
hks_dist = np.array([np.sum(((hks_T[i] - hks_T[j])/np.sum(np.exp(-t * ew)))**2)**(1/2) for i,j in combinations(range(X_pos.shape[0]), 2)])
# hks_dist = np.array([np.sum(((hks_TS[i] - hks_TS[j]))**2)**(1/2) for i,j in combinations(range(X_pos.shape[0]), 2)])
hks_emb = cmds(squareform(hks_dist)**2)
p = figure(width=200, height=200)
p.scatter(*hks_emb.T, color=v_color)
q = figure_complex(S, X_pos, width=200, height=200, 
  simplex_kwargs={ 0 : {'size' : 4.5 }}, 
  color=np.arange(X_pos.shape[0]), 
  bin_kwargs={'color_pal':'turbo'}
)
show(row(p,q))

# "Since kt(x, x) decays exponentially as t increases, we need a more appropriate strategy of computing the difference between two signatures"
np.argmin(X @ np.array([0,1]))
```


```{python}
import sys
sys.path.append('/Users/mpiekenbrock/mesh-signatures')
import laplace
import signature
laplace.get_laplace_operator_approximation
# laplace.get_laplace_operator_approximation(mesh, approx="")
X_3d = np.hstack([X_pos, np.zeros((X_pos.shape[0],1))])

import trimesh
from pbsig.utility import cycle_window
triangles = list(cycle_window(range(X_pos.shape[0]), offset=2, w=3))
# S.update(triangles)


mesh = trimesh.Trimesh(vertices=X_3d, faces=np.array(triangles))

triangle_area(X_pos[(0,1,2),:])/3
mesh.area_faces[0]

# laplace.approx_methods()
LM, MM = laplace.get_laplace_operator_approximation(mesh, approx="mesh")

MM.diagonal()[0]

## beltrami := unweighted graph laplacian 
## mesh := weighted graph laplacian built using 1/(4*pi*h**2)*exp(-|d_ij|**2/4h) formulation (prioritizes local distances)
## cotangens := weighted graph laplacian weighed by cotangent angles
## All of them use a mass matrix built summing the areas fo triangles of neighbors
## Beltrami is *nearly* useless as the only thing distinguishing the graph from the sphere is the mass matrix
## cotangents and mesh seem surprisingly identical
extractor = signature.SignatureExtractor(mesh, X_3d.shape[0]-1, approx="mesh") 

# extractor.initialize(mesh, )
# extractor.save("./dragon.npz") # Store eigen spectrum

# Compute 64 dimensional heat features and get time values
# heat_fs, ts = extractor.signatures(64, 'heat', return_x_ticks=True)

heat_sigs = extractor.heat_signatures(64)
hks_emb = cmds(squareform(pdist(heat_sigs)**2))
p = figure(width=200, height=200)
p.scatter(*hks_emb.T, color=bin_color(np.arange(X_pos.shape[0]), "turbo"))
# show(p)

q = figure_complex(S, X_pos, width=200, height=200, 
  simplex_kwargs={ 0 : {'size' : 4.5 }}, 
  color=np.arange(X_pos.shape[0]), 
  bin_kwargs={'color_pal':'turbo'}
)
# ext_ind = np.argsort(heat_sigs[:,-1])[-5:]
# q.scatter(*X_pos[ext_ind].T, color="gray", size=10.5)

r = figure(width=250, height=200)
pt_col = bin_color(np.arange(X_pos.shape[0]), "turbo")
for i, (hs, col) in enumerate(zip(heat_sigs, pt_col)):
  if i % 5 == 0:
    r.line(np.arange(heat_sigs.shape[1]), hs, color=rgb_to_hex(col*255))
show(row(p,q,r))
```


```{python}
# L = up_laplacian(S, normed=True)
D1 = boundary_matrix(S, p=1)

L, deg = up_laplacian(S, normed=False, return_diag=True)
D = dia_array(np.diag(deg))
A = L - D
I = dia_array(np.eye(L.shape[0]))

D2 = dia_array(np.diag(pseudoinverse(np.sqrt(deg))))
eigsh(I - D2 @ A @ D2)[0] # indeed in [0,2]
(I - D2 @ A @ D2).sum(axis=0) # sums to 2!
LN = up_laplacian(S, normed=True, return_diag=False)
LN.sum(axis=0) ## sums to zero!
(D2 @ L @ D2).sum(axis=0) ## sums to zero! 

up_laplacian(S).sum(axis=0)

L = dia_array(np.diag(deg)) - A # graph Laplacian
L.sum(axis=1)


from scipy.sparse import dia_matrix
from scipy.spatial.distance import cdist 
E_ind = np.array(list(faces(S,1)))
E_dist = dist(X_pos[E_ind[:,0]], X_pos[E_ind[:,1]], paired=True)

shape_sigs = []
for k, X_pos in X_data.items():
  L = D1 @ dia_array(np.diag(E_dist)) @ D1.T
  D = diags(pseudoinverse(np.sqrt(L.diagonal())))
  L = D @ L @ D
  hkl_sig = hkt(L, timepoints=25, spectral_ub=2)
  shape_sigs.append(hkl_sig)

shape_sigs = np.array(shape_sigs)
Z_emb = cmds(dist(shape_sigs, paired=True)**2)

p = figure(width=200, height=200)
p.scatter(*Z_emb.T, color=['red' if k[0] == 'turtle' else 'blue' for k in X_data.keys()])
show(p)

## exp maps the range [-37,0] roughly to [machine_eps, 1]
## if eigen bound is [0,2], we want log-spaced map [0,2] -> (-)[0,37]
# minimize_scalar(lambda x: np.abs(np.exp(-x) - 1e-8), bounds=(-37, 0))
```



