---
title: "AdaBoost w/ SPRI on MPEG7 dataset"
format: html
jupyter: python3
editor: 
  render-on-save: true
---

```{python}
#| label: Imports 
import numpy as np
from splex import *
from pbsig import * 
from pbsig.linalg import * 
from pbsig.vis import figure_complex
from bokeh.plotting import figure, show
from bokeh.io import output_notebook
from bokeh.layouts import row, column
output_notebook(verbose=False)
```

```{python}
#| label: Load shape data
from pbsig.datasets import mpeg7
# X_data = mpeg7(simplify=100, which=["bone", "watch", "bird", "bell", "bat", 'fish','flatfish', 'fork','frog', "horse"]) # "default", "all",  
# ["turtle", "watch", "bird", "bone", "bell"]
X_data = mpeg7(simplify=100, which=["bird", "fish"])

## Obtain the typical (X,y) setup 
labels = np.array(list(map(itemgetter(0), X_data.keys())))
classes = np.sort(np.unique(labels))
X_all = np.array([np.ravel(x) for x in X_data.values()])
y_all = np.searchsorted(classes, labels)
```

```{python}
#| label: Visualize the shape data 
def patch_plot(X: ArrayLike, color: str, **kwargs):
  p = figure(width=50, height=50, **kwargs)
  p.patch(X[:,0], X[:,1], alpha=0.40, line_width=2.5, color=color)
  p.toolbar_location = None
  p.axis.visible = False
  p.grid.grid_line_color = None
  p.min_border_left = 0
  p.min_border_right = 0
  p.min_border_top = 0
  p.min_border_bottom = 0
  return p 

p = column(
  row([patch_plot(X_data[('bird',i)], "blue") for i in range(1, 11)]), 
  row([patch_plot(X_data[('bird',i)], "blue") for i in range(11, 21)]), 
  row([patch_plot(X_data[('fish',i)], "red") for i in range(1, 11)]),
  row([patch_plot(X_data[('fish',i)], "red") for i in range(11, 21)])
)
show(p)
```

```{python}
# | label: Amplify the size of the data via interpolation
# Interpolate intra-class shapes to generate more datasets for training 
from pbsig.shape import sample_interp
m_new = 500 
X_aug, y_aug = [], []
for cl in range(len(classes)):
  print(f"Generating {m_new} new data sets for class {classes[cl]}")
  X_cl = X_all[y_all == cl]
  X_cl_new = sample_interp(X_cl, m_new) 
  X_aug.append([np.ravel(x) for x in X_cl_new])
  y_aug.extend(np.repeat(cl, m_new))
X_aug = np.vstack([X_all, np.vstack(X_aug)])
y_aug = np.append(y_all, np.array(y_aug))
```

```{python}
# | label: Training and testing sets
from pbsig.utility import split_train_test
train_ind, test_ind = split_train_test(len(X_aug), split=(0.70, 0.30), labels=y_aug)
X_train, X_test = X_aug[train_ind], X_aug[test_ind]
y_train, y_test = y_aug[train_ind], y_aug[test_ind]
```

```{python}
# | label: Boosting a SHELLS classifier
# Even this can be quite slow due to fit and histogram! 
from pbsig.classifiers import ShellsClassifier

import timeit

timeit.timeit(lambda: ShellsClassifier(bins=15, cache=False).fit(X_train, y_train), number=100)

weak_learner = ShellsClassifier(bins=15, cache=True)
weak_learner.fit(X_train, y_train, ensure_better=True)
weak_learner.score(X_train, y_train)
timeit.timeit(lambda: weak_learner.fit(X_train, y_train), number=100)

import line_profiler
profiler = line_profiler.LineProfiler()
profiler.add_function(weak_learner.fit)
profiler.enable_by_count()
for i in range(100):
  weak_learner.fit(X_train, y_train)
profiler.print_stats(output_unit=1e-3)

weak_learner = ShellsClassifier(bins=15, cache=True)
weak_learner.fit(X_train, y_train)
weak_learner.score(X_train, y_train)
weak_learner.score(X_test, y_test)

from sklearn.ensemble import AdaBoostClassifier
A_skl = AdaBoostClassifier(ShellsClassifier(bins=15), n_estimators=15, learning_rate=0.01, algorithm="SAMME")
A_skl.fit(X_train, y_train)
print(len(A_skl.estimators_))

## Train/test goes up to 85/84% respectively
A_skl.score(X_train, y_train) ## accuracy 
A_skl.score(X_test, y_test) 

print(np.array(list(A_skl.staged_score(X_train, y_train))))
print(np.array(list(A_skl.staged_score(X_test, y_test))))


```




```{python}
# | label: Laplacian classifier 
from pbsig.linalg import up_laplacian
from pbsig.simplicial import cycle_graph
from pbsig.vis import figure_complex
from pbsig.color import bin_color
from bokeh.palettes import Category10

x_pc = X_data[('bird', 1)]
S = simplicial_complex(cycle_graph(len(x_pc)))

L = up_laplacian(S, normed=True)

## Test connectedness of laplacian 
# x = np.random.choice([0,1], size=len(x_pc), replace=True)
normalize = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))
x = np.array([0]*50 + [1]*49)
y = L @ x
z = L @ normalize(x_pc @ np.array([0,1]))

from pbsig.linalg import eigh_solver
ew, ev = eigh_solver(L)(L)

N = card(S,0)
np.sort(np.array([1 - np.cos(2*np.pi*k / N) for k in range(N)]))

D = np.diag(1/np.sqrt(np.arange(N)))
D[0,0] = 0

# np.sort(np.linalg.eigh(D @ L @ D)[0])

# p = figure_complex(S, pos=x_pc)
p,q,r,s = [figure(width=250, height=250) for i in range(4)]
p.scatter(*x_pc.T, color=bin_color(x, "Category10"))
q.scatter(*x_pc.T, color=bin_color(y, "Category10"))
r.scatter(*x_pc.T, color=bin_color(z, "viridis", lb=-1, ub=1))
s.scatter(*x_pc.T, color=bin_color(ev[:,0], "viridis"))
show(row(p, q, r, s))


figs = [figure(width=100, height=100) for j in range(ev.shape[1])]
[fig.scatter(*x_pc.T, color=bin_color(ev[:,j], "viridis")) for j, fig in enumerate(figs)]
show(row(*figs))




```





















```{python}
# | label: Augment the training set via shape interpolation 
# Interpolate intra-class shapes to generate more datasets for training 
from pbsig.shape import sample_interp
X_train_aug, y_train_aug = [], []
for cl in range(len(classes)):
  print(f"Generating new data for class {classes[cl]}")
  X_cl = X_train[y_train == cl]
  to2d = lambda x: x.reshape(len(x)//2, 2)
  pro_dist_cl = np.array([procrustes_dist_curves(to2d(X),to2d(Y)) for (X,Y) in combinations(X_cl, 2)])
  # X_cl_new = sample_interp(X_cl, 80, dist=pro_dist_cl, use_landmarks=False)
  X_cl_new = sample_interp(X_cl, 80, dist=pro_dist_cl, use_landmarks=True)
  X_train_aug.append([np.ravel(x) for x in X_cl_new])
  y_train_aug.extend(np.repeat(cl, 80))
X_train_aug = np.vstack([X_train, np.vstack(X_train_aug)])
y_train_aug = np.append(y_train, np.array(y_train_aug))
```



```{python}
# | label: Procrustes + PCA embedding plots 
from itertools import * 
from pbsig.shape import procrustes_dist_curves
from scipy.spatial.distance import squareform, pdist, cdist
from bokeh.palettes import viridis
from bokeh.layouts import row, column

pt_col = np.array(viridis(len(np.unique(y_train_aug))))[y_train_aug.astype(np.uint)]
p = figure(width=375, height=375)
p.scatter(*pca(X_train_aug).T, color=pt_col)
show(p)

# pro_dist = [procrustes_dist_curves(X,Y) for (X,Y) in combinations(X_data.values(), 2)]
# Z = cmds(squareform(np.array(pro_dist))**2, 2)

# pt_col = np.array(viridis(len(np.unique(y))))[y.astype(np.uint)]
# p, q = figure(width=375, height=375), figure(width=375, height=375)
# # p.scatter(*Z.T, color=pt_col)
# q.scatter(*pca(X).T, color=pt_col)
# show(row(p,q))
```


```{python}
# | label: SHELLS weak learner (classifier)

# from sklearn.utils.estimator_checks import check_estimator
# check_estimator(ShellsClassifier(bins=10, dim=2))
```

```{python}
#| label: Get idea of performance on training alone (TODO: generalization error?)
weak_learner = ShellsClassifier(bins=20, random_state=0)
weak_learner.fit(X_train_aug, y_train_aug)
weak_learner.score(X_train_aug, y_train_aug)

## Try different weighting scheme
w = np.random.uniform(size=len(y_train_aug), low=0, high=1)
w /= np.sum(w)
weak_learner.fit(X_train_aug, y_train_aug, w)
weak_learner.score(X_train_aug, y_train_aug)
```

```{python}
#| label: Mean curve plots 
p = figure(width=400, height=225)
for x, y_label in zip(X_train_aug, y_train_aug):
  p_color = 'red' if y_label == 0 else 'blue'
  p.line(weak_learner.bins_[:-1], weak_learner.shells_signature(x), line_color=p_color, line_alpha=0.05, line_dash="dashed")
p.line(weak_learner.bins_[:-1], weak_learner.mean_curves[0], color='red', line_width=1.5)
p.line(weak_learner.bins_[:-1], weak_learner.mean_curves[1], color='blue', line_width=1.5)
show(p)
```

```{python}
class BinaryAdaBoostClassifier:
  """Binary AdaBoost classifier."""
  def __init__(self, weak_learner: object, random_state = None):
    self.random_state = random_state
    self.weak_learner = weak_learner

  def fit(self, X: ArrayLike, y: ArrayLike, n_estimators: int = 100):
    from copy import deepcopy
    n = X.shape[0]
    classes_, y = np.unique(y, return_inverse=True)
    assert n == len(y) and isinstance(X, np.ndarray) and isinstance(y, np.ndarray), "Invalid (X, y) pair given."
    assert len(classes_) == 2, "AdaBoost is only a binary classifier"
    self.classes_ = classes_
    self.estimators_ = []
    self.coeffs_ = []

    ## Run the iterations
    ## Update formula: https://course.ccs.neu.edu/cs6140sp15/4_boosting/lecture_notes/boosting/boosting.pdf
    ## Also: https://arxiv.org/pdf/2210.07808.pdf
    ## rt = 1.0-et
    ## np.where(yt != y, wt * np.sqrt(rt/et), wt * np.sqrt(et/rt))  ## alternative, equiv weight update
    ## 0.5 * np.log((1+(1-2*et))/(1 - (1-2*et)))                    ## alternative, equiv coefficient calculation
    wt = np.ones(n) / n # uniform coefficient weights 
    for t in range(n_estimators):
      ht = self.weak_learner.fit(X, y, wt)                ## fit weak learner with weighted samples; should be randomized
      yt = ht.predict(X)                                  ## predict using the weak learner
      et = np.sum(wt[yt != y]) / np.sum(wt)               ## error of the current weak learner
      at = 0.5 * np.log((1.0-et)/et) if et > 0 else 1.0   ## voting/learning coefficient
      nt = np.where(yt == y, 1, -1)                       ## signed "mistake dichotomy"
      wt *= np.exp(-at * nt)                              ## weight update  
      wt /= np.sum(wt)                                    ## weight normalization
      self.estimators_.append(deepcopy(ht))
      self.coeffs_.append(at)

  def decision_function(self, X: ArrayLike) -> ArrayLike: 
    """Confidence function."""
    y_pred = np.zeros(len(X), dtype=np.float64)
    sgn_classes = np.array([-1, 1], dtype=np.int16) 
    for a, h in zip(self.coeffs_, self.estimators_):
      p = np.searchsorted(self.classes_, h.predict(X)) # guarenteed to be in [0,1]
      y_pred += a * sgn_classes[p] # maps class @ 0 -> -1 and class @ 1 -> 1
    return y_pred

  def predict_proba(self, X: ArrayLike) -> ArrayLike:
    """Probability prediction function."""
    y_pred = self.decision_function(X)
    m = np.sum(np.abs(self.coeffs_)) # all predictions should be in [-m, m] where m = sum(alpha) = max confidence
    prob = 0.5 + np.where(y_pred < 0, (m - np.abs(y_pred))/(2*m), y_pred/(2*m))
    return np.array([(p, 1-p) if s < 0 else (1-p, p) for p,s in zip(prob, y_pred)])
    # normalize = lambda x: (x - np.min(x))/max_confidence
    # proba = (y_pred + max_confidence)/(2*max_confidence) # should be in [0,1]

  def predict(self, X: ArrayLike) -> ArrayLike:
    return self.classes_[np.where(A.decision_function(X) < 0, 0, 1)]
  
  def margin(self, X: ArrayLike, y: ArrayLike):
    sgn_classes = np.array([-1, 1], dtype=np.int16) 
    yp = sgn_classes[np.searchsorted(self.classes_, y)]
    return self.decision_function(X) * yp

  def staged_decision_function(self, X: ArrayLike) -> Generator:
    y_pred = np.zeros(len(X), dtype=np.float64)
    sgn_classes = np.array([-1, 1], dtype=np.int16) 
    for a, h in zip(self.coeffs_, self.estimators_):
      p = np.searchsorted(self.classes_, h.predict(X)) # guarenteed to be in [0,1]
      y_pred += a * sgn_classes[p] # maps class @ 0 -> -1 and class @ 1 -> 1
      yield deepcopy(y_pred)

  def staged_predict_proba(self, X: ArrayLike) -> Generator:
    m = np.sum(np.abs(self.coeffs_)) # max confidence
    for y_pred in self.staged_decision_function(X):
      prob = 0.5 + np.where(y_pred < 0, (m - np.abs(y_pred))/(2*m), y_pred/(2*m))
      yield np.array([(p, 1-p) if s < 0 else (1-p, p) for p,s in zip(prob, y_pred)])
      
  def staged_predict(self, X: ArrayLike) -> Generator:
    for y_pred in self.staged_decision_function(X):
      yield self.classes_[np.where(y_pred < 0, 0, 1)]

  def staged_margin(self, X: ArrayLike, y: ArrayLike, normalize: bool = False) -> Generator:
    margin = np.zeros(len(y), dtype=np.float64)
    for i, (yt, at) in enumerate(zip(self.staged_predict(X), self.coeffs_)):
      margin += at * np.where(yt == y, 1, -1)
      yield deepcopy(margin/np.sum(np.abs(self.coeffs_[:(i+1)]))) if normalize else deepcopy(margin)

  def score(self, X: ArrayLike, y: ArrayLike) -> float:
    return np.sum(self.predict(X) == y)/len(y)

  def staged_score(self, X: ArrayLike, y: ArrayLike) -> Generator:
    for yt in self.staged_predict(X):
      yield float(np.sum(yt == y)/len(y))
```

```{python}
## Show the accuracy on the training data
A = BinaryAdaBoostClassifier(ShellsClassifier(bins=20))
A.fit(X_train_aug, y_train_aug, n_estimators=50)
# A.predict(X_train_aug)
# A.decision_function(X_train_aug)
A.score(X_train_aug, y_train_aug)

np.array(list(A.staged_score(X_train_aug, y_train_aug)))

list(A.staged_score(X_test, y_test))

margin_train_aug = np.array(list(A.staged_margin(X_train_aug, y_train_aug, normalize=False)))
margin_test = np.array(list(A.staged_margin(X_test, y_test, normalize=True)))

p = figure(width=350, height=200)
# for margin in margin_train_aug:
#   margin_norm /= np.sum(np.abs(margin))
#   # margin = np.sort(margin)
#   p.line(margin, np.cumsum(margin), color='green')
for margin in margin_test:
  # margin /= np.sum(np.abs(margin))
  p.line(np.sort(margin), np.cumsum(np.sort(margin))), color='red')
show(p)

# A.score(X_train, y)
# list(A.staged_decision_function(X))[-1]
# list(A.staged_margin(X, y))[-1]
# np.array(list(A.staged_score(X,y)))
```



```{python}
from sklearn.ensemble import AdaBoostClassifier
A_skl = AdaBoostClassifier(ShellsClassifier(bins=20), n_estimators=40, algorithm="SAMME")
A_skl.fit(X_train_aug, y_train_aug)
# np.sum(A_skl.predict(X_train) == y)/len(y)

print(np.array(list(A_skl.staged_score(X_train_aug, y_train_aug))))
print(np.array(list(A_skl.staged_score(X_test, y_test))))

list(A_skl.staged_predict_proba(X_train))
A_skl.estimator_weights_
# A_skl.estimator_errors_

```

```{python}
#| label: PCA plot of the data
from bokeh.palettes import turbo, viridis
from pbsig.linalg import pca
X_train = np.array([np.ravel(x) for x in X_data.values()])
# X_train = np.array([np.ravel(shells(x, 20)) for x in X_data.values()])
Z_train = pca(X_train)
p = figure(width=375, height=375)
p.scatter(*Z_train.T, color=np.array(viridis(len(np.unique(y))))[y.astype(np.uint)])
show(p)
```

```{python}
#| label: Create a "weak learner"
from pbsig.betti import Sieve
from pbsig.simplicial import cycle_graph
from pbsig.pht import parameterize_dt
X_train = X[('turtle',1)]
K = cycle_graph(len(X_train))
dt_filters = parameterize_dt(X_train, dv = 32)
sieve = Sieve(K, family = dt_filters)
sieve.randomize_pattern(1, lb=0, ub=3, area=(0.05**2, 0.15**2), max_asp=5)
sieve.solver.params['tol'] = 1e-4 
sieve.sift(w=1.0, k=10, progress=False)
np.allclose(np.ravel(sieve.summarize()), 0.0, atol=1e-9)

from bokeh.models import Range1d
p = figure(height=150, width=275)
p.line(np.arange(32), np.ravel(sieve.summarize()))
p.y_range = Range1d(-1, 1)
show(p)
```

```{python}
#| label:  Logistic regression baseline
from pbsig.shape import shells 
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
# X_train = np.array([np.ravel(x) for x in X_data.values()])
X_train = np.array([np.ravel(shells(x,20)) for x in X_data.values()])
model = LogisticRegression(random_state=0, penalty="l2").fit(X_train, y)
model = LogisticRegressionCV(random_state=0, penalty="l2").fit(X_train, y)

## Get accuracy
print(sum(model.predict(X_train) != y))

## 10-fold cross-validation --- generalization accuracy for shells around ~ 25%
## Point cloud data (as-is) w/ 100 points: train/test accuracy averages 86/68%, respectively
## Shells vector (10 shells): train/test accuracy averages 50/37%, respectively
from sklearn.model_selection import cross_validate
cross_validate(model, X_train, y, cv=10, verbose=1, scoring="accuracy", return_train_score=True)

from bokeh.palettes import turbo
Z = cmds(squareform(pdist(X_train)**2))
p = figure(width=375, height=375)
p.scatter(*Z.T, color=np.array(turbo(len(np.unique(y))))[y.astype(np.uint)])
show(p)

from sklearn.manifold import TSNE
z = TSNE(2).fit_transform(X_train)
p = figure(width=375, height=375)
p.scatter(*z.T, color=np.array(turbo(len(np.unique(y))))[y.astype(np.uint)])
show(p)
```

Test the shells classifier in binary label situation




```{python}
X_train = np.array([np.ravel(x) for x in X_data.values()])
weak_learner = ShellsLearner(k=3, d=2, random_state=1)
weak_learner.fit(X_train, y)
# weak_learner.predict_proba(X_train)
print(f"Accuracy: {np.sum(weak_learner.predict(X_train) == y)/len(y):.2f}")
cross_validate(weak_learner, X_train, y, cv=10, verbose=1, scoring="accuracy", return_train_score=True)

## TODO: find a case where two classes severely overlap and logistci struggles. 
## Logistic is already a power-house with OvR enabled, need to re-create the multi-class functionality from scratch here 
from sklearn.ensemble import AdaBoostClassifier
ensemble_logistic = AdaBoostClassifier(LogisticRegression(penalty="l2"), n_estimators=20, algorithm="SAMME.R", random_state=1234)
ensemble_logistic.fit(X_train, y)
cross_validate(ensemble_logistic, X_train, y, cv=10, verbose=1, scoring="accuracy", return_train_score=True)

np.array(list(ensemble_logistic.staged_score(X_train, y)))

ensemble = AdaBoostClassifier(ShellsLearner(k=10, d=2), n_estimators=200, algorithm="SAMME.R", random_state=1234)
ensemble.fit(X_train, y)

[est.n_classes_ for est in ensemble.estimators_]



boosted_scores = np.array(list(ensemble.staged_score(X_train, y)))
p = figure()
p.line(np.arange(len(boosted_scores)), boosted_scores)
show(p)


res = list(ensemble.staged_predict(X_train))

## Fit the classifier

print(ensemble.estimator_weights_)
print(ensemble.estimator_errors_)
print(f"Accuracy: {np.sum(ensemble.predict(X_train) == y)/len(y):.2f}")
cross_validate(ensemble, X_train, y, cv=10, verbose=1, scoring="accuracy", return_train_score=True)


est1 = ensemble.estimators_[1]
(len(y) - np.sum(est1.predict(X_train) != y))/len(y)
# np.histogram(ensemble.estimator_weights_)

# random_stateint, RandomState instance or None, default=None
# Controls the random seed given at each estimator at each boosting iteration. Thus, it is only used when estimator exposes a random_state. Pass an int for reproducible output across multiple function calls. See Glossary.

```


```{python}
#| label: Natural gradient boosting 
from ngboost import NGBClassifier
from ngboost.distns import k_categorical, Bernoulli
from ngboost.scores import LogScore, CRPScore

X_train = np.array([np.ravel(x) for x in X_data.values()])

## Need a regressor as a base learner to use ngboost ~ 
## See: https://github.com/stanfordmlgroup/ngboost/issues/225
weak_learner = ShellsLearner(k=3, d=2, random_state=1)
classifier = NGBClassifier(Base=weak_learner, Dist=k_categorical(len(np.unique(y))), verbose=False)
_ = classifier.fit(X_train, y)


```





Benchmarking 
```{python}
def _bench():
  ## For a given feature, create a reference mean curve for each class, phase aligned
  from pbsig.dsp import phase_align
  from pbsig.utility import progressbar
  mean_curves = {}
  sieve.randomize_pattern(1, lb=0, ub=3, area=(0.05**2, 0.15**2), max_asp=5)
  i = 0
  for x_pc, y_label in progressbar(zip(X.values(), y), len(y)):
    sieve.family = parameterize_dt(x_pc, dv = 16)
    sieve.sift(w=1, progress=False)
    feature_summary = np.ravel(sieve.summarize())
    if y_label in mean_curves: 
      mean_curves[y_label] += phase_align(feature_summary, mean_curves[y_label])
    else: 
      mean_curves[y_label] = feature_summary
    if i < 20: 
      i += 1
    else: 
      break
  mean_curves = { cl : ts / sum(y == cl) for cl, ts in mean_curves.items() }

sieve.solver = PsdSolver(sieve.laplacian, solver='lanczos', ncv=5, tolerance=1e-1, k=5, maxiter=30, return_stats=True, return_eigenvectors=True, return_unconverged=True)
res = sieve.solver(sieve.laplacian)
sieve.solver(sieve.laplacian, v0=res[1], return_eigenvectors=True)

sieve.solver(sieve.laplacian)

sieve.solver = PsdSolver(sieve.laplacian, solver='lanczos', ncv=5, tolerance=1e-1, k=5, maxiter=30, return_stats=False, return_eigenvectors=False, return_unconverged=True)
from line_profiler import LineProfiler
profiler = LineProfiler()
profiler.add_function(_bench)
profiler.add_function(sieve.sift)
profiler.add_function(sieve.project)
profiler.add_function(sieve.solver.solver)
profiler.add_function(sieve.solver.__call__)
profiler.enable_by_count()
_bench()
profiler.print_stats(output_unit=1e-3)
# from pbsig.dsp import phase_align2
# s1 = np.roll(s2, 15) + np.random.uniform(size=len(s2), low=-1e-4, high=1e-4)
# p = figure(width=250, height=175)
# p.line(np.arange(len(s1)), s1, color='orange')
# p.line(np.arange(len(s1)), phase_align2(s1,s2), color='red')
# p.line(np.arange(len(s2)), s2, color='blue')
# show(p)

```



```{python}
from sklearn.ensemble import AdaBoostRegressor




class SieveWeakClassifier:
  def __init__(sieve, curves: dict):
    self.curves = curves
    self.sift_params = {}

  def predict_proba(X: ArrayLike):
    assert len(X) == sieve.laplacian.nv, "Number of vertices must match"
    sieve.family = parameterize_dt(X, dv = 32)
    sieve.sift(**self.sift_params)
    curve = np.ravel(sieve.summarize())
    curve - 



import timeit
timeit.timeit(lambda: sieve.sift(w=1.0, k=55, tol=1), number=5)


## TODO: 
# from scipy.sparse.linalg.eigen import arpack
# Import _SymmetricArpackParams from arpack and see how to control the behavior from there\
from line_profiler import LineProfiler
profiler = LineProfiler()
profiler.add_function(sieve.sift)
profiler.add_function(sieve.project)
profiler.add_function(sieve.solver.__call__)
profiler.add_function(sieve.solver.solver)
profiler.enable_by_count()
sieve.sift(w=1.0, k=55, tol=1)
profiler.print_stats(output_unit=1e-3)
```
